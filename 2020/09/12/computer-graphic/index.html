

<!DOCTYPE html>
<html lang="" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/Blog/img/favicon.png">
  <link rel="icon" href="/Blog/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Daolin">
  <meta name="keywords" content="">
  
    <meta name="description" content="图像学     基本构成Elements of image formation Object Exists independent of image formation and viewer - defined in its onw object space Formed by geometric primitives, (e.g. triangles)   Viewer Forms the">
<meta property="og:type" content="article">
<meta property="og:title" content="图像学">
<meta property="og:url" content="https://daolinzhou.github.io/Blog/2020/09/12/computer-graphic/index.html">
<meta property="og:site_name" content="Daolin&#39;s Repository">
<meta property="og:description" content="图像学     基本构成Elements of image formation Object Exists independent of image formation and viewer - defined in its onw object space Formed by geometric primitives, (e.g. triangles)   Viewer Forms the">
<meta property="og:locale">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/com_graphic/g_3.PNG">
<meta property="article:published_time" content="2020-09-12T19:51:57.000Z">
<meta property="article:modified_time" content="2022-01-02T05:18:51.468Z">
<meta property="article:author" content="Daolin">
<meta property="article:tag" content="python3">
<meta property="article:tag" content="opencv">
<meta property="article:tag" content="图像学">
<meta property="article:tag" content="WebGl">
<meta property="article:tag" content="javascript">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daolinzhou.github.io/Blog/intro/com_graphic/g_3.PNG">
  
  
  <title>图像学 - Daolin&#39;s Repository</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/Blog/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"daolinzhou.github.io","root":"/Blog/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/Blog/local-search.xml"};
  </script>
  <script  src="/Blog/js/utils.js" ></script>
  <script  src="/Blog/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/Blog/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/">
                <i class="iconfont icon-home-fill"></i>
                ホーム
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                アーカイブ
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                カテゴリー
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                タグ
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/about/">
                <i class="iconfont icon-user-fill"></i>
                本ブログ情報　
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/Blog/intro/red_Kodori.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="图像学">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-09-12 12:51" pubdate>
        September 12, 2020 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      77k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      642 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">图像学</h1>
            
            <div class="markdown-body">
              <span id="more"></span>

<h1 id="图像学"><a href="#图像学" class="headerlink" title="图像学"></a>图像学</h1><br>

<p><img src="/Blog/Blog/intro/com_graphic/g.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h2 id="基本构成"><a href="#基本构成" class="headerlink" title="基本构成"></a>基本构成</h2><h3 id="Elements-of-image-formation"><a href="#Elements-of-image-formation" class="headerlink" title="Elements of image formation"></a>Elements of image formation</h3><ul>
<li>Object<ul>
<li>Exists independent of image formation and viewer - defined in its onw <strong>object space</strong></li>
<li>Formed by geometric primitives, (e.g. triangles)</li>
</ul>
</li>
<li>Viewer<ul>
<li>Forms the image of the object via a <strong>projection</strong></li>
<li>Image is produced in 2D</li>
<li>Objects transformed from object to <strong>image space</strong></li>
</ul>
</li>
</ul>
<br>

<h3 id="光与颜色"><a href="#光与颜色" class="headerlink" title="光与颜色"></a>光与颜色</h3><p>如果没有光, 那么就看不到任何东西</p>
<p>人们是如何看见东西的?</p>
<ol>
<li>光照到物体上, <strong>反射</strong>到人眼 (Light reflected)</li>
<li>光直接从光源<strong>传播</strong>到人眼 (Light transmitted)</li>
</ol>
<br>

<p>什么是反射的颜色?</p>
<p>光与物体都有颜色.</p>
<p>当光找到一个有颜色的物体, 一些颜色被反射(reflected), 一些颜色被吸收(absorbed)</p>
<br>

<p>计算机中使用RGB来描述一个颜色</p>
<p><img src="/Blog/Blog/intro/com_graphic/g_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="Ray-tracing"><a href="#Ray-tracing" class="headerlink" title="Ray tracing"></a>Ray tracing</h2><p><img src="/Blog/Blog/intro/com_graphic/g_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>Directly derived from the <strong>synthetic camera</strong> model</p>
<ul>
<li>Follow rays of light from light sources</li>
<li>Determine which rays enter the lens of the camera through image window</li>
<li>Compute color of projection</li>
</ul>
<p>简而言之就是模拟现实中光的反射, 判断哪些光进入摄像头, 之后计算颜色</p>
<p>然而这个方法并不好. 毕竟能进入摄像头的光线只有一部分, 然而我们却要计算所有的光线. 做了很多无用功.</p>
<br>

<p>所以一个很自然的想法就是将上面的步骤反过来</p>
<p>Cast one ray per pixel from the eye and shoot it into the scene.</p>
<blockquote>
<p>不是从光源发射一道ray到眼睛, 而是从眼睛发射一道ray到光源</p>
</blockquote>
<p>假设从眼睛中发射出一道光线, 照到物体上. 经过反射直到达到光源, 我们就能看到物体的颜色.</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/g_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>A point on an object may be illuminated(照亮) by</p>
<ul>
<li>Light source directly - through <strong>shadow ray</strong></li>
<li>Light reflected off an object - through <strong>reflected ray</strong></li>
<li>Light transmitted through a transparent object through <strong>refracted ray</strong></li>
</ul>
<br>

<h3 id="Ray-tracing-算法"><a href="#Ray-tracing-算法" class="headerlink" title="Ray tracing 算法"></a>Ray tracing 算法</h3><figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> each pixel on screen:<br>    <br>    determine ray <span class="hljs-keyword">from</span> eye through pixel<br>    <br>    <span class="hljs-keyword">if</span> ray shoots into infinity:<br>        <span class="hljs-keyword">return</span> a background color<br>    <br>    <span class="hljs-keyword">if</span> ray shoots into light source:<br>        <span class="hljs-keyword">return</span> light color appropiately<br>    <br>    <span class="hljs-comment"># most expensive step</span><br>    find closest intersection of ray <span class="hljs-keyword">with</span> an <span class="hljs-built_in">object</span><br>    cast off **shadow ray** (toward light sources)<br>    <br>    <span class="hljs-keyword">if</span> shadow ray hits a light source:<br>        compute light contribution(<span class="hljs-number">1</span>) according to some illumination model<br>    <br>    cast reflected(<span class="hljs-number">2</span>) <span class="hljs-keyword">and</span> refracted ray(<span class="hljs-number">3</span>), **recursively** to calculate pixel color contributions<br>    <br>    <span class="hljs-keyword">return</span> pixel color <span class="hljs-keyword">as</span> a <span class="hljs-built_in">sum</span> of (<span class="hljs-number">1</span>)-(<span class="hljs-number">3</span>) after some absorption<br></code></pre></td></tr></table></figure>

<br>

<h3 id="Ray-tracing-的好处与坏处"><a href="#Ray-tracing-的好处与坏处" class="headerlink" title="Ray tracing 的好处与坏处"></a>Ray tracing 的好处与坏处</h3><p>好处:</p>
<ul>
<li>近乎遵循物理光流</li>
<li>看起来更真实. can model light-object interactions and <strong>inter-surface reflections and refractions</strong></li>
</ul>
<p>坏处:</p>
<ul>
<li>Expensive, 耗费高, 光的合成以及反射的递归都消耗性能</li>
<li>Inadequate for modeling <strong>non-reflective</strong> (dull-looking) objects</li>
</ul>
<br>

<br>

<h2 id="z-buffer"><a href="#z-buffer" class="headerlink" title="z-buffer"></a>z-buffer</h2><ul>
<li>在 image space 中工作, 和 ray tracing 相似</li>
<li><strong>Per-polygon</strong> operations vs. per-pixel for ray tracing</li>
<li>Simple and often accelerated with hardware (<strong>fase</strong>)</li>
<li>Works regardless of the order in which the polygons are processed — no need to sort them back to front</li>
<li>A <strong>visibility algorithm</strong> and not designed to compute colors</li>
</ul>
<br>

<h3 id="z-buffer-算法"><a href="#z-buffer-算法" class="headerlink" title="z-buffer 算法"></a>z-buffer 算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> each polygon <span class="hljs-keyword">in</span> the scene<br>    project its vertices onto viewing(image) plane<br>    <span class="hljs-keyword">for</span> each pixel inside the polygon formed on viewing plane<br>        determine point on polygon corresponding to this pixel get **depth value** <span class="hljs-keyword">for</span> this pixel (distance <span class="hljs-keyword">from</span> point to plane)<br>        <span class="hljs-keyword">if</span> depth value &lt; stored depth value <span class="hljs-keyword">for</span> the pixel<br>            compute pixel color based on some illumination model<br>            update pixel color <span class="hljs-keyword">in</span> frame buffer<br>            update depth value <span class="hljs-keyword">in</span> **depth buffer** (z-buffer)<br>        end <span class="hljs-keyword">if</span><br></code></pre></td></tr></table></figure>

<p>简单来说就是前面的物体会遮挡住后面的物体, 所以计算每个物体的距离, 来判断一个点展示什么颜色.</p>
<br>

<br>

<h2 id="Frame-Buffer-FB"><a href="#Frame-Buffer-FB" class="headerlink" title="Frame Buffer(FB)"></a>Frame Buffer(FB)</h2><p>存储每个像素的信息(颜色).</p>
<p>但是 <strong>color buffer</strong> 只是众多buffer中的一个, 例如 **depth&#x2F;z-buffer **, accumulation buffer…</p>
<p><strong>Depth</strong> of a frame buffer: number of bits per pixel</p>
<ul>
<li>For color representation, 1bit &#x3D;&gt; 2 colors, 8 bit &#x3D;&gt; $2^8$colors, 24 bits&#x3D;&gt; true color(16 million colors)</li>
</ul>
<br>

<br>

<h2 id="Graphics-Pipeline-图形渲染管道"><a href="#Graphics-Pipeline-图形渲染管道" class="headerlink" title="Graphics Pipeline(图形渲染管道)"></a>Graphics Pipeline(图形渲染管道)</h2><p><img src="/Blog/Blog/intro/com_graphic/pipeline.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h3 id="顶点着色器-Vertex-shader"><a href="#顶点着色器-Vertex-shader" class="headerlink" title="顶点着色器(Vertex shader)"></a>顶点着色器(Vertex shader)</h3><p><img src="/Blog/Blog/intro/com_graphic/pipeline_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h3 id="图元组装"><a href="#图元组装" class="headerlink" title="图元组装"></a>图元组装</h3><p>将位置和索引组装成图元数据</p>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h3 id="栅格化"><a href="#栅格化" class="headerlink" title="栅格化"></a>栅格化</h3><p>将图元数据转化成一个一个像素的数据, 每个数据(像素)称作一个片段(Fragment)</p>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>对于这些 Fragment 我们还会进行一些操作</p>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="片段着色器-Fragment-Shader"><a href="#片段着色器-Fragment-Shader" class="headerlink" title="片段着色器(Fragment Shader)"></a>片段着色器(Fragment Shader)</h3><p>计算图元每个像素的颜色</p>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>每个像素都有深度, 这个信息存储在深度缓冲区中</p>
<br>

<h3 id="帧缓冲区-Frame-Buffer"><a href="#帧缓冲区-Frame-Buffer" class="headerlink" title="帧缓冲区(Frame Buffer)"></a>帧缓冲区(Frame Buffer)</h3><p>我们改变帧缓冲区中的数据, 显示器就拿去显示给用户</p>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h2 id="OpenGL-历史"><a href="#OpenGL-历史" class="headerlink" title="OpenGL 历史"></a>OpenGL 历史</h2><p>旧的(3.x 之前)OpenGL 主要是一个状态机</p>
<p>它的函数主要有2种类型:</p>
<ul>
<li>Primitive generating</li>
<li>Can cause output to be produced if primitive is visible</li>
<li>How vertices are processed and how primitives appear are controlled by their states</li>
<li>State changing or querying<ul>
<li>Transformation functions</li>
<li>Attribute functions</li>
</ul>
</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>新的OpenGL 从 immediate display mode 变为 retained mode</p>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>program 将准备要渲染的数据, 以array的形式存储他们. 之后把这些数据传给shaders, 由shader进行渲染</p>
<p>Model OpenGL is Shader-based:</p>
<ul>
<li>no default shaders</li>
<li>Each application must provide both a <strong>vertex shader</strong> and a <strong>fragment shader</strong> . 2 files</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/pipeline_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="WebGL-基础"><a href="#WebGL-基础" class="headerlink" title="WebGL 基础"></a>WebGL 基础</h2><h3 id="传递数据给GLSL–Buffer管理"><a href="#传递数据给GLSL–Buffer管理" class="headerlink" title="传递数据给GLSL–Buffer管理"></a>传递数据给GLSL–Buffer管理</h3><h4 id="OpenGL着色器语言-GLSL"><a href="#OpenGL着色器语言-GLSL" class="headerlink" title="OpenGL着色器语言(GLSL)"></a>OpenGL着色器语言(GLSL)</h4><p>计算顶点位置(Vertex Shader)</p>
<ul>
<li>平移, 旋转, 缩放…</li>
<li>投影</li>
</ul>
<br>

<p>为每个像素上色(Fragment Shader)</p>
<ul>
<li>颜色</li>
<li>材质</li>
<li>光照</li>
</ul>
<br>

<h4 id="传递方式"><a href="#传递方式" class="headerlink" title="传递方式"></a>传递方式</h4><ul>
<li>Uniforms – 直接传递 (以全局变量的方式)</li>
<li>Attributes – 通过Buffer (顶点的位置, 顶点的颜色都叫做顶点的属性)</li>
</ul>
<br>

<h4 id="传递uniform"><a href="#传递uniform" class="headerlink" title="传递uniform"></a>传递uniform</h4><p>uniform通常比较小, 可以直接传递</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>1234 指的是向量的维度</p>
<p>ui 指的是 unsigned int, f 指的是float</p>
<p>v 值得是传递的 data 是否是 vector</p>
<br>

<p>也可以传递矩阵</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>第二个参数false固定是false, 它是一个预留参数, 代表矩阵是否需要转置. 然而底层没有实现, 所以固定为false</p>
<br>

<h4 id="传递Attribute"><a href="#传递Attribute" class="headerlink" title="传递Attribute"></a>传递Attribute</h4><p>attribute比较大, 所以要用缓冲区Buffer</p>
<p>webgl中用于存储数据</p>
<p>类型:</p>
<ul>
<li>Vertex Buffer</li>
<li>Index Buffer</li>
<li>Frame Buffer(用于驱动显示器, 通常用户不需要直接用)</li>
</ul>
<p>一个正方形有8个顶点, 然而有6个面, 每个面由两个三角形构成, 就是12个三角形, 每个三角形有3个顶点, 所以一个正方体由36的顶点成.</p>
<p>然而我们只给出这些值是不够的, 我们要告诉它哪三个顶点组成一个三角形</p>
<br>

<h4 id="缓冲区相关操作"><a href="#缓冲区相关操作" class="headerlink" title="缓冲区相关操作"></a>缓冲区相关操作</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs javascript">gl.<span class="hljs-title function_">createBuffer</span>()  <span class="hljs-comment">// 创建缓冲区</span><br><br>gl.<span class="hljs-title function_">bindBuffer</span>(target, buffer)  <span class="hljs-comment">// 绑定缓冲区的类型(Vertex Buffer / Index Buffer)</span><br><br><span class="hljs-comment">// 向缓冲区注入数据, 并设置数据如何使用(提示gl这些数据是否会经常变换以让gl采取不同策略)</span><br>gl.<span class="hljs-title function_">bufferData</span>(target, data, useage)  <br></code></pre></td></tr></table></figure>

<p>Buffer 的 target &#x2F; usage</p>
<ul>
<li>target (表述buffer被绑定在哪里)<ul>
<li><code>gl.ARRAY_BUFFER</code> 用来存顶点属性</li>
<li><code>gl.ELEMENT_ARRAY_BUFFER</code> 用来存索引</li>
</ul>
</li>
<li>usage (提示webgl数据将如何使用)<ul>
<li><code>gl.STATIC_DRAW</code> 数据通常不会发生变化, webgl只加载一次</li>
<li><code>gl.DYNAMIC_DRAW </code> 数据经常发生变化 webgl会经常检测数据变化了没有</li>
</ul>
</li>
</ul>
<br>

<p>着色器语言中大体也有三种类型:</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>buffer要bind两次, 第二次是为了切换回当前buffer(gl内部有各种状态)</p>
<p>enableVertexAttribarray 是启用这个属性, 可以在任何时候开启开关. 如果buffer开启太多也可以在合理的地方关闭</p>
<br>

<br>

<h3 id="世界坐标与剪裁空间"><a href="#世界坐标与剪裁空间" class="headerlink" title="世界坐标与剪裁空间"></a>世界坐标与剪裁空间</h3><p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>程序员脑海中的概念: 我们的空间是一个150 : 100的区域. 这个150和100没有单位, 或者说单位不固定. 我们可以称为世界坐标</p>
<p>而机器是由想租组成, 例如高1080个像素</p>
<p>然而图形渲染引擎不认这些东西, 它说所有坐标必须在$[-1, 1]$之间, 这个空间叫剪裁空间</p>
<br>

<p>我们要把世界坐标转化成剪裁空间的坐标</p>
<p>$x’, y’$代表剪裁空间的坐标, $x,y$ 代表世界坐标.</p>
<p>如果我们用矩阵的思想思考, 这可以分为两个矩阵: 一个缩放矩阵, 一个平移矩阵</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>注意, 我们思考时, y是从下到上的, 然而图形学中y是从上到下.</p>
<p>所以对于y要取负</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>进行转置后就是这样</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>然而这也是问题所在, 旋转操作会基于(0, 0)点进行旋转, 也就是左上角.</p>
<p>一个解决办法就是先平移到原点, 再选择, 再移动回去</p>
<br>

<br>

<h3 id="投影"><a href="#投影" class="headerlink" title="投影"></a>投影</h3><h4 id="多视图投影"><a href="#多视图投影" class="headerlink" title="多视图投影"></a>多视图投影</h4><p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>多视图投影每个点移动的距离都是一样的.</p>
<h4 id="透视投影"><a href="#透视投影" class="headerlink" title="透视投影"></a>透视投影</h4><p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>模拟人的眼睛</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="坐标系"><a href="#坐标系" class="headerlink" title="坐标系"></a>坐标系</h3><p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="透视的推导"><a href="#透视的推导" class="headerlink" title="透视的推导"></a>透视的推导</h4><p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>y方向</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>x方向</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>由于x方向和y方向的比值是确定的, 所以<br>$$<br>\frac {\tan(\frac {fovx} 2)}{\tan(\frac {fovy}2)}&#x3D;\frac f{f’} &#x3D; aspect &#x3D; \frac {width}{height}<br>$$<br><br></p>
<p>z方向</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>z方向</p>
<p>我们通常不会用这样一种方法</p>
<p>因为虽然眼睛看到的效果是近大远小, 但是眼睛看到的不是成比例的减小</p>
<p>人的视觉: 越近越重要, 越远越忽略</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们希望它是一个双曲线</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>将m和c带回原式中</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>因此我们得到这样的几个式子:</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>问题是怎么将他们形成矩阵</p>
<p>但是这里有一个问题, x’, y’都乘以了一个$\frac 1{-z}$, 而z是一个变量</p>
<p>于是我们用另一组坐标表达, 让<br>$$<br>x’’&#x3D;\frac {x’}{-z}, y’’&#x3D;\frac {y’}{-z},  z’’&#x3D;\frac {z’}{-z}<br>$$<br>这样:</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>最后再把x’’, y’’, z’’ 变回 x’, y’, z’</p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/webgl_basic_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>矩阵中最后一样的-1是关键, 因为最后行点乘坐标得到的新坐标, 当新坐标进行reduce时, 会除以向量中第四个元素, 这就相当于把x’’, y’’, z’’ 变回 x’, y’, z’</p>
<p>$aspect &#x3D; width&#x2F;height$</p>
<br>

<br>

<h4 id="世界坐标系与观察坐标系"><a href="#世界坐标系与观察坐标系" class="headerlink" title="世界坐标系与观察坐标系"></a>世界坐标系与观察坐标系</h4><p><img src="/Blog/Blog/intro/com_graphic/world_c.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/world_c_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>就是平移加旋转, 首先平移让原点重合, 再旋转让x轴重合, 再旋转让y轴重合, 最后旋转让z轴重合</p>
<p><img src="/Blog/Blog/intro/com_graphic/world_c_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="GLSL-基本语法"><a href="#GLSL-基本语法" class="headerlink" title="GLSL 基本语法"></a>GLSL 基本语法</h3><h4 id="数据类型和基本操作"><a href="#数据类型和基本操作" class="headerlink" title="数据类型和基本操作"></a>数据类型和基本操作</h4><h5 id="基础类型"><a href="#基础类型" class="headerlink" title="基础类型"></a>基础类型</h5><ul>
<li>void</li>
<li>bool</li>
<li>int</li>
<li>float</li>
</ul>
<h4 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h4><ul>
<li>vec2 2d单精度浮点向量</li>
<li>vec3 3d单精度浮点向量</li>
<li>vec4 4d单精度浮点向量</li>
<li>bvec2 2d布尔向量</li>
<li>bvec3 3d布尔向量</li>
<li>bvec4 4d布尔向量</li>
<li>ivec2 2d整数向量</li>
<li>ivec3 3d整数向量</li>
<li>ivec4 4d整数向量</li>
</ul>
<h4 id="向量操作"><a href="#向量操作" class="headerlink" title="向量操作"></a>向量操作</h4><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-type">vec2</span> a = <span class="hljs-type">vec2</span>(<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>);<br><span class="hljs-type">vec2</span> b = <span class="hljs-type">vec2</span>(<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>);<br><span class="hljs-type">vec2</span> c = a + b;<br><span class="hljs-comment">// c = (4.0, 6.0)</span><br></code></pre></td></tr></table></figure>

<p>可以通过<code>xyzw</code>获取向量中的值</p>
<figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-type">vec4</span> a = <span class="hljs-type">vec4</span>(<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>);<br><span class="hljs-type">vec3</span> b = a.xyz;<br><span class="hljs-comment">// b = (1.0, 2.0, 3.0)</span><br>    <br><span class="hljs-type">vec2</span> c = a.xw;<br><span class="hljs-comment">// c = (1.0, 4.0)</span><br><br><span class="hljs-type">vec4</span> d = a.xyxx;<br><span class="hljs-comment">// d = (1.0, 2.0, 1.0, 1.0)</span><br></code></pre></td></tr></table></figure>

<p><code>rgba</code> 和 <code>xyzw</code> 拥有同样的能力</p>
<figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-type">vec4</span> a = <span class="hljs-type">vec4</span>(<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>);<br><br><span class="hljs-type">vec4</span> d = a.rgrr;<br><span class="hljs-comment">// d = (1.0, 2.0, 1.0, 1.0)</span><br></code></pre></td></tr></table></figure>

<p><code>stpq</code> 也有这个能力 (纹理向量)</p>
<figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-type">vec4</span> a = <span class="hljs-type">vec4</span>(<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>);<br><br><span class="hljs-type">vec4</span> d = a.stss;<br><span class="hljs-comment">// d = (1.0, 2.0, 1.0, 1.0)</span><br></code></pre></td></tr></table></figure>

<br>

<h4 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h4><ul>
<li>mat2  $2\times 2$ 矩阵</li>
<li>mat3  $3\times 3$ 矩阵</li>
<li>mat4  $4\times 4$ 矩阵</li>
<li>matnxm  m行n列的矩阵</li>
</ul>
<p>初始化:</p>
<figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-type">mat3</span> a;<br>a[<span class="hljs-number">0</span>] = <span class="hljs-type">vec3</span>(<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>);<br>a[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] = <span class="hljs-number">10.0</span>;<br>a[<span class="hljs-number">2</span>].yzx = <span class="hljs-type">vec3</span>(<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>)<br></code></pre></td></tr></table></figure>

<p>矩阵构造器:</p>
<figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-type">mat3</span> b = <span class="hljs-type">mat3</span>(<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>, <span class="hljs-number">7.0</span>, <span class="hljs-number">8.0</span>, <span class="hljs-number">9.0</span>)<br><br><span class="hljs-type">mat2</span>(<span class="hljs-type">float</span>, <span class="hljs-type">float</span>, <span class="hljs-type">float</span>, <span class="hljs-type">float</span>);<br><span class="hljs-type">mat3</span>(<span class="hljs-type">vec3</span>, <span class="hljs-type">vec3</span>, <span class="hljs-type">vec3</span>);<br><span class="hljs-type">mat3</span>(<br>    <span class="hljs-type">vec2</span>, <span class="hljs-type">float</span>,<br>    <span class="hljs-type">vec2</span>, <span class="hljs-type">float</span>,<br>    <span class="hljs-type">vec2</span>, <span class="hljs-type">float</span><br>);<br></code></pre></td></tr></table></figure>

<br>

<h4 id="外部类型"><a href="#外部类型" class="headerlink" title="外部类型"></a>外部类型</h4><p>有一些外部定义的类型会创建对象的引用</p>
<ul>
<li>Sampler 每个sampler代表了一种材质</li>
<li>Image 代表一张图片</li>
</ul>
<h4 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h4><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-keyword">uniform</span> <span class="hljs-type">float</span> arr[<span class="hljs-number">8</span>];<br><span class="hljs-keyword">uniform</span> <span class="hljs-type">vec3</span> arr[<span class="hljs-number">5</span>][<span class="hljs-number">2</span>];<br></code></pre></td></tr></table></figure>

<p>uniform 是存储限定符, 它告诉webgl怎么存储这个类型</p>
<p>uniform是所有着色器执行之前, 这个值是已知的.</p>
<h4 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h4><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs glsl">struct Light<br>&#123;<br>    <span class="hljs-type">vec3</span> eyePosOrDir;<br>    <span class="hljs-type">bool</span> isDirectional;<br>    <span class="hljs-type">vec3</span> intensity;<br>    <span class="hljs-type">float</span> attenuation;<br>&#125; foo;<br></code></pre></td></tr></table></figure>

<p>上面代码创建了一个结构体类型Light, 并且定义了一个变量foo数据结构体类型Light.</p>
<figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs glsl">struct Data&#123;<br>    <span class="hljs-type">float</span> first;<br>    <span class="hljs-type">vec2</span> second;<br>&#125;;<br><br>Data dataValue = Data(<span class="hljs-number">1.4</span>, <span class="hljs-type">vec2</span>(<span class="hljs-number">16.0</span>, <span class="hljs-number">22.5</span>))<br></code></pre></td></tr></table></figure>

<br>

<h4 id="类型严格"><a href="#类型严格" class="headerlink" title="类型严格"></a>类型严格</h4><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-type">int</span> a = <span class="hljs-number">3.0</span>;<br><span class="hljs-type">float</span> a = <span class="hljs-number">2</span>;<br></code></pre></td></tr></table></figure>

<p>整型不能赋值给浮点型, 浮点型也不能赋值给整型</p>
<br>

<p>操作符和控制语句都和c一样</p>
<p>if, while, for</p>
<br>

<h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs glsl"><span class="hljs-type">void</span> MyFunction(<span class="hljs-keyword">in</span> <span class="hljs-type">float</span> inputValue, <span class="hljs-keyword">out</span> <span class="hljs-type">int</span> outputValue, <span class="hljs-keyword">inout</span> <span class="hljs-type">float</span> inAndOutValue);<br></code></pre></td></tr></table></figure>

<ul>
<li>in 输入(会将外部遍历值拷贝进函数)</li>
<li>out 输出(调用完成会将值拷贝到外部遍历)</li>
<li>inout(拷贝两次)</li>
</ul>
<br>

<p>不支持函数递归</p>
<br>

<h4 id="存储限定符"><a href="#存储限定符" class="headerlink" title="存储限定符"></a>存储限定符</h4><p>存储限定符(storage qualifiers), 用来表述遍历如何被glsl存储</p>
<br>

<h5 id="uniform"><a href="#uniform" class="headerlink" title="uniform"></a>uniform</h5><p>一种全局着色器变量, 用于从程序向着色器传递参数, uniform的意思就是在所有着色器调用中值保持一致</p>
<h5 id="attribute"><a href="#attribute" class="headerlink" title="attribute"></a>attribute</h5><p>仅仅可以在顶点着色器中使用, 用来存储顶点相关的数据</p>
<h5 id="varying"><a href="#varying" class="headerlink" title="varying"></a>varying</h5><p>在顶点着色器中定义, 然后传递给片段着色器的变量.</p>
<br>

<br>

<h2 id="纹理"><a href="#纹理" class="headerlink" title="纹理"></a>纹理</h2><h3 id="纹理贴图-Texture-Mappiing"><a href="#纹理贴图-Texture-Mappiing" class="headerlink" title="纹理贴图 (Texture Mappiing)"></a>纹理贴图 (Texture Mappiing)</h3><p><img src="/Blog/Blog/intro/com_graphic/texture.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="mipmap"><a href="#mipmap" class="headerlink" title="mipmap"></a>mipmap</h3><p><img src="/Blog/Blog/intro/com_graphic/texture_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs javascript">gl.<span class="hljs-title function_">texImage2D</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,  <span class="hljs-comment">// bind point</span><br>    <span class="hljs-number">0</span>,  <span class="hljs-comment">// level of detail</span><br>    gl.<span class="hljs-property">RGBA</span>,    <span class="hljs-comment">// internal format</span><br>    gl.<span class="hljs-property">RGBA</span>,    <span class="hljs-comment">// format</span><br>    gl.<span class="hljs-property">UNSIGNED_BYTE</span>,   <span class="hljs-comment">// image 数据类型</span><br>    image<br>)<br></code></pre></td></tr></table></figure>

<p>第二个参数level of detail就是设置level, 如果level为0, 就说明用原图. <code>(256x256)</code></p>
<p>如果level&#x3D;1, 就把宽高缩减一半 <code>(128x128)</code></p>
<br>

<h4 id="gl-texParameteri"><a href="#gl-texParameteri" class="headerlink" title="gl.texParameteri()"></a>gl.texParameteri()</h4><p><code>gl.texParameteri()</code> 是用来设置贴图的参数</p>
<br>

<p>如果texture的坐标正好契合要贴图位置的坐标</p>
<p>如果用 <code>TEXTURE_MIN_FILTER</code></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs javascript">gl.<span class="hljs-title function_">texParameteri</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,<br>    gl.<span class="hljs-property">TEXTURE_MIN_FILTER</span>,<br>    gl.<span class="hljs-property">NEAREST</span><br>);<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/texture_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/texture03.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这个图片非常小, 只有2x2&#x3D;4个像素</p>
<br>

<p>如果用 <code>TEXTURE_MAG_FILTER</code></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs javascript">gl.<span class="hljs-title function_">texParameteri</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,<br>    gl.<span class="hljs-property">TEXTURE_MAG_FILTER</span>,<br>    gl.<span class="hljs-property">NEAREST</span><br>);<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/texture_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>LINEAR 会根据周围像素进行加权blur</p>
<p>无论 <code>TEXTURE_MAG_FILTER</code> 还是 <code>TEXTURE_MIN_FILTER</code> 得到的都是这样的图</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs javascript">gl.<span class="hljs-title function_">texParameteri</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,<br>    gl.<span class="hljs-property">TEXTURE_MAG_FILTER</span>,<br>    gl.<span class="hljs-property">LINEAR</span><br>);<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/texture_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果不契合, 默认会进行repeat</p>
<p><img src="/Blog/Blog/intro/com_graphic/texture_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs javascript">gl.<span class="hljs-title function_">texParameteri</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,<br>    gl.<span class="hljs-property">TEXTURE_WRAP_T</span>,<br>    gl.<span class="hljs-property">CLAMP_TO_EDGE</span><br>);<br>gl.<span class="hljs-title function_">texParameteri</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,<br>    gl.<span class="hljs-property">TEXTURE_MAG_FILTER</span>,<br>    gl.<span class="hljs-property">NEAREST</span><br>);<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/texture_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><code>TEXTURE_WRAP</code>是指当超出纹理范围时, 它会停留在最后一个像素</p>
<p>STPQ 四种方向, 但是<code>TEXTURE_WRAP_S</code>或者<code>TEXTURE_WRAP_T</code>两种可能 </p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs javascript">gl.<span class="hljs-title function_">texParameteri</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,<br>    gl.<span class="hljs-property">TEXTURE_WRAP_S</span>,<br>    gl.<span class="hljs-property">CLAMP_TO_EDGE</span><br>);<br>gl.<span class="hljs-title function_">texParameteri</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,<br>    gl.<span class="hljs-property">TEXTURE_WRAP_T</span>,<br>    gl.<span class="hljs-property">CLAMP_TO_EDGE</span><br>);<br>gl.<span class="hljs-title function_">texParameteri</span>(<br>    gl.<span class="hljs-property">TEXTURE_2D</span>,<br>    gl.<span class="hljs-property">TEXTURE_MAG_FILTER</span>,<br>    gl.<span class="hljs-property">NEAREST</span><br>);<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/texture_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="光照现象"><a href="#光照现象" class="headerlink" title="光照现象"></a>光照现象</h2><p>光在传播中会有反射, 折射, 吸收, 散射等现象.</p>
<p>从能量角度看, 光有辐射现象.</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>更复杂一些还有折射</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>再复杂一些, 可能有漫反射以及材质的问题</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="一个简单的模型"><a href="#一个简单的模型" class="headerlink" title="一个简单的模型"></a>一个简单的模型</h3><p>如果一个点没有光照, 那么它就乘以0, <code>#000000</code>是黑色.</p>
<p>如果光照最强, 就显示原本的颜色</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果light &#x3D; 1就是直射的效果, light&#x3D;0.5就是斜着照射的效果, light&#x3D;0 就是夹角大于90度(照到背面)的效果.</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/light_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>n是法向量, l是光照的反方向. 如果他们都是单位向量.</p>
<p>nl点乘的结果就是$\cos\theta$  这符合物理规律, 当夹角为0时, 光照为1. 就是直射, 法向量和光照反方向重合.</p>
<p>而$\theta$ 如果在 $[0, \pi&#x2F;2]$之间 那么$\cos\theta$就是一个0到1的值</p>
<br>

<br>

<h3 id="点光源的简单模型"><a href="#点光源的简单模型" class="headerlink" title="点光源的简单模型"></a>点光源的简单模型</h3><p><img src="/Blog/Blog/intro/com_graphic/light_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>点光源要告诉模型光源的位置, 以及眼睛的位置.</p>
<p>光照打在一个片段上. 这个片段的颜色如何决定? 有两个因素</p>
<ol>
<li>光源和平面的夹角</li>
<li>眼睛的位置, 如果反射直接射入眼睛, 此时感受到的光最亮(大多数情况不是镜面反射, 而是有一定偏移, 这就是中间方向)</li>
</ol>
<p>中间方向其实是点到camera和点到光源两个向量的加和再归一.</p>
<p>光源和眼睛有一个中间方向, 而光源和法向量之间有一个入射角. </p>
<p>当中间方向等于法向量, 入射角等于反射角, 眼睛在反射角的线上, 此时亮度最亮.</p>
<br>

<h4 id="数学模型"><a href="#数学模型" class="headerlink" title="数学模型"></a>数学模型</h4><p><img src="/Blog/Blog/intro/com_graphic/light_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>对比区别</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="图像学中-shape-的表示"><a href="#图像学中-shape-的表示" class="headerlink" title="图像学中 shape 的表示"></a>图像学中 shape 的表示</h2><p><img src="/Blog/Blog/intro/com_graphic/shape.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>Mesh 和 Polygons 的组合是最经常使用的.</p>
<br>

<h3 id="Polygon-多边形"><a href="#Polygon-多边形" class="headerlink" title="Polygon 多边形"></a>Polygon 多边形</h3><p>polygon 分为 simple 和 non-simple:</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>像(a)这种是没有交叉边的就是simple, 反之像(b)那样有交叉边的</p>
<br>

<p>同时 polygon 还可以分为 Convex 和 Concave </p>
<p>对于多边形内<strong>任意</strong>两点的连线都在多边形内就是convex, 反之就是not convex(Concave)</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Polygonal-meshes"><a href="#Polygonal-meshes" class="headerlink" title="Polygonal meshes"></a>Polygonal meshes</h3><p>A <strong>piece-wise flat surface</strong> formed by a set of polygons pasted along their edges.</p>
<p>三角形mesh是最常用的:</p>
<ol>
<li>三角形总是flat, simple, convex</li>
<li>WebGL只支持三角形</li>
<li>硬件加速</li>
<li>所有的多边形都可以分解为一组三角形**(triangulated)**</li>
</ol>
<p><img src="/Blog/Blog/intro/com_graphic/shape_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h2 id="线的表示"><a href="#线的表示" class="headerlink" title="线的表示"></a>线的表示</h2><p>线有很多表示方法:</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/shape_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>如果我们想画一条线, 我们希望:</p>
<ol>
<li>线过两个顶点</li>
<li>尽可能直</li>
<li>亮度均匀</li>
<li>高性能高效率</li>
</ol>
<br>

<h3 id="Naive-algorithm"><a href="#Naive-algorithm" class="headerlink" title="Naive algorithm"></a>Naive algorithm</h3><p>基本思想: 每次向x前进一个像素, 同时更新y.</p>
<p>伪代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> each pixel x compute<br>    y = <span class="hljs-built_in">round</span>(m*x + n)<br></code></pre></td></tr></table></figure>

<p>然而这段代码效率低下, 每当 x 增加时mx都会导致一个浮点乘法. 浮点乘法非常耗时</p>
<p>因此我们可以优化这段代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span>(x = x0 + <span class="hljs-number">1</span>; x &lt; x1; x++)<br>    y += m<br>    write_pixel(x, <span class="hljs-built_in">round</span>(y))<br></code></pre></td></tr></table></figure>

<p>把乘法转换为加法. 每次只执行浮点加法即可.</p>
<br>

<p>然而这样还是有一些问题</p>
<ol>
<li>依然需要进行浮点加法</li>
<li>如果slop太大, 则会出现不连续情况</li>
</ol>
<p><img src="/Blog/Blog/intro/com_graphic/shape_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果slop太大, y+m 可能就会跳过一些像素点, 例如m&#x3D;2, 此时y+&#x3D;2就跳过了一个像素</p>
<br>

<h3 id="Bresenham’s-algorithm"><a href="#Bresenham’s-algorithm" class="headerlink" title="Bresenham’s algorithm"></a>Bresenham’s algorithm</h3><p>目前效率最高的画线的算法, 也是目前标准的画线算法</p>
<p>也被叫做 <strong>midpoint algorithm</strong>, 这个算法不需要浮点数和乘法, 只需要整数和加法</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>由此也可以看到这个算法最小化的 midpoint error</p>
<br>

<p>如果slop大于1, 可以取倒数, 画线再进行翻转</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如何高效判断一个midpoint是在line的上面还是下面?</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_11.PNG" srcset="/Blog/img/loading.gif" lazyload><br>$$<br>F(x, y)&#x3D;(x-x_0)dy-(y-y_0)dx<br>$$<br>举个例子:</p>
<p>如果$d&#x3D;2F(x,y)$, 之所以乘以2是因为midpoint是在像素中间, 我们不希望有浮点运算, 所以把0.5乘以2变为1.</p>
<p>最开始第一个点是 $d&#x3D;2F(x_0+1, y_0+1&#x2F;2) &#x3D; 2dy-dx$, 以此类推</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>伪代码: 假设x0&lt;x1 同时slope在0,1 之间</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span> <span class="hljs-title function_">drawline</span><span class="hljs-params">(x0, y0, x1, y1)</span>&#123;<br>    <span class="hljs-type">int</span> dx, dy, d, incE, incNE, x, y;<br>    dx = x1 - x0;<br>    dy = y1 - y0;<br>    d = <span class="hljs-number">2</span> * dy - dx;       <span class="hljs-comment">// 初始时, d=2F(x-+1, y+1/2)</span><br>    incE = <span class="hljs-number">2</span> * dy;         <span class="hljs-comment">// 如果走E, x += 1</span><br>    incNE = <span class="hljs-number">2</span> * (dy - dx); <span class="hljs-comment">// 如果走NE, x+=1, y+=1</span><br>    y = y0;<br>    <br>    <span class="hljs-keyword">for</span>(x = x0; x &lt;= x1; x++)&#123;<br>        write_pixel(x, y);<br>        <span class="hljs-keyword">if</span>(d &gt; <span class="hljs-number">0</span>)&#123;<br>            d = d + incNE;<br>            y = y + <span class="hljs-number">1</span>;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            d = d+incE;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs routeros">如果想画一条(0,1)(6,4)的线<br>dx = 6<br>dy = 3<br>d= 2<span class="hljs-number">*3</span> - 6 = 0<br>incE = 6<br>incNE = -6<br><br>Loop <span class="hljs-keyword">from</span> 0 <span class="hljs-keyword">to</span> 6<br> * <span class="hljs-attribute">x</span>=0: write_pixel(0, 1), d&lt;=0: d = 0 + incE  = 6<br> * <span class="hljs-attribute">x</span>=1: write_pixel(1, 1), d&gt; 0: d = 6 + incNE = 0, <span class="hljs-attribute">y</span>=1+1=2<br> * <span class="hljs-attribute">x</span>=2: write_pixel(2, 2), d&lt;=0: d = 0 + incE  = 6<br> * <span class="hljs-attribute">x</span>=3: write_pixel(3, 2), d&gt; 0: d = 6 + incNE = 0, <span class="hljs-attribute">y</span>=2+1=3<br> * <span class="hljs-attribute">x</span>=4: write_pixel(4, 3), d&lt;=0: d = 0 + incE  = 6<br> * <span class="hljs-attribute">x</span>=5: write_pixel(5, 3), d&gt; 0: d = 6 + incNE = 0, <span class="hljs-attribute">y</span>=3+1=4<br> * <span class="hljs-attribute">x</span>=6: write_pixel(6, 4), d&lt;=0: d = 0 + incE  = 6<br></code></pre></td></tr></table></figure>

<br>

<br>

<h3 id="OpenGL-diamond-exit-rule"><a href="#OpenGL-diamond-exit-rule" class="headerlink" title="OpenGL diamond exit rule"></a>OpenGL diamond exit rule</h3><p><img src="/Blog/Blog/intro/com_graphic/shape_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="Polygon-fill"><a href="#Polygon-fill" class="headerlink" title="Polygon fill"></a>Polygon fill</h2><p>如何为多边形填充颜色? 一个最基本的思想就是 odd-even test</p>
<p><strong>Scanline polygon rendering:</strong> once inside, draw pixels along scanline, until outside.</p>
<br>

<p>基本思路:</p>
<ul>
<li><p>Shoot any ray from infinity (edge of screen) and count from zero</p>
</li>
<li><p>Increment count any time a polygon edge is intersected</p>
</li>
<li><p>At <strong>even</strong> count, point is outside. At <strong>odd</strong> count, point is inside.</p>
</li>
</ul>
<p>然而对于 non-simple polygon 效果不好:</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>而且如果顶点在scan-line上, add-even test 会变得混乱</p>
<ul>
<li>If joining edges on same side:<br>count twice – (a)</li>
<li>Otherwise: shorten edge by<br>one scan-line – (b)</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/shape_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Winding-number-test"><a href="#Winding-number-test" class="headerlink" title="Winding number test"></a>Winding number test</h3><p>有一个更加复杂的算法: Winding number test</p>
<p>我们为沿某一个方向遍历多边形的边(一定是可以遍历的, 因为是多边形). 并为每个边赋予一个标记up&#x2F;down</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>然而non-simple polygon是多种多样的, 这个算法也不是完美的</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>另外一个思想是:</p>
<p>如果我们在多边形外有一点, 这一点和多边形内任意一点相连接. 如果经过多边形的任意一条边, 就说明这个点在多边形内部. 反之在多边形外部</p>
<p><img src="/Blog/Blog/intro/com_graphic/shape_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>然而这个算法更加难以实现</p>
<br>

<br>

<h2 id="rotation-3d"><a href="#rotation-3d" class="headerlink" title="rotation 3d"></a>rotation 3d</h2><p>沿x轴旋转其实可以想象成将图像沿x轴拍平, 再旋转. 其余轴同理 </p>
<p><img src="/Blog/Blog/intro/com_graphic/rotation_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/rotation.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>





<h2 id="投影-1"><a href="#投影-1" class="headerlink" title="投影"></a>投影</h2><p><img src="/Blog/Blog/intro/com_graphic/proj.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h3 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h3><p><img src="/Blog/Blog/intro/com_graphic/proj_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h3 id="斜投影"><a href="#斜投影" class="headerlink" title="斜投影"></a>斜投影</h3><p><img src="/Blog/Blog/intro/com_graphic/proj_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/oblique_proj.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h3 id="透视投影-1"><a href="#透视投影-1" class="headerlink" title="透视投影"></a>透视投影</h3><p>透视投影分为3种, one point, two point, three point.</p>
<p><img src="/Blog/Blog/intro/com_graphic/proj_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>它们是根据vanishing point 区分的. 一个vanishing point, 两个vanishing point, 3个vanishing point.</p>
<h4 id="vanishing-point"><a href="#vanishing-point" class="headerlink" title="vanishing point"></a>vanishing point</h4><p><img src="/Blog/Blog/intro/com_graphic/proj_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="Visibility-and-Clipping"><a href="#Visibility-and-Clipping" class="headerlink" title="Visibility and Clipping"></a>Visibility and Clipping</h2><p><img src="/Blog/Blog/intro/com_graphic/visibility.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>Human perception expects both <strong>Visibility and Shading</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>Visibility 是图形学中最基本的一个问题.</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Visibility-algorithms"><a href="#Visibility-algorithms" class="headerlink" title="Visibility algorithms"></a>Visibility algorithms</h3><p><img src="/Blog/Blog/intro/com_graphic/visibility_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>回顾ray tracing算法</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>回顾z-buffer算法</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Object-space-visibility"><a href="#Object-space-visibility" class="headerlink" title="Object space visibility"></a>Object space visibility</h4><p><img src="/Blog/Blog/intro/com_graphic/visibility_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h4 id="depth-sort"><a href="#depth-sort" class="headerlink" title="depth sort"></a>depth sort</h4><p>要确定一组 object 的 visibility(depth) 的顺序</p>
<p>在从后向前(<strong>back-to-front</strong>)进行渲染</p>
<br>

<p>然而在现实中, 物体的深度可能是交错的.</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>此时就可能要对物体进行一些拆分来区分不同的深度</p>
<br>

<h4 id="BSP"><a href="#BSP" class="headerlink" title="BSP"></a>BSP</h4><p>另一种算法叫做BSP(Binary space partitioning) of object space</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>例如通过A把space分成两个sub space. B所在的就叫front, E所在的就是back</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>假设现在有A-E, 5个polygon</p>
<p>我们先选择A作为基准</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>可以看到 C 被 cut 为 C1, C2</p>
<p>之后再以B为基准</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>此时就创建了一个树的左半部分, 同理得到这棵树的右半部分</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>显然这是一个递归算法</p>
<p>在得到这个数之后, 就要对它进行遍历(traversal)</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>同样也是从后往前draw</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h4 id="Clipping"><a href="#Clipping" class="headerlink" title="Clipping"></a>Clipping</h4><h5 id="Line-Clipping"><a href="#Line-Clipping" class="headerlink" title="Line Clipping"></a>Line Clipping</h5><p>Cohen-Sutherland 算法</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>对于一个线的两个端点进行判断. 并为它们一个赋一个4 bit的值(TBRL). 之后通过位运算判断是否应该画出这条线.</p>
<p>例如如果端点是在可视区域的左上角, 那么就是T&#x3D;1, L&#x3D;1既是top又是left.</p>
<p>对应到 TBRL 就是 <code>1001</code>. </p>
<p>如果两个端点都是<code>0000</code>, 说明两个端点都在可视范围内, 应该画出这条线, 此时叫做 Trivial accept.</p>
<p>&#x2F;&#x2F; TODO: Cohen-Sutherland 算法</p>
<br>

<h5 id="Polygon-clipping"><a href="#Polygon-clipping" class="headerlink" title="Polygon clipping"></a>Polygon clipping</h5><p>Sutherland-Hodgeman算法</p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/visibility_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h3 id="Local-Illumination-and-Shading-照明与阴影"><a href="#Local-Illumination-and-Shading-照明与阴影" class="headerlink" title="Local Illumination and Shading(照明与阴影)"></a>Local Illumination and Shading(照明与阴影)</h3><p>光照和阴影使得图形看起来更加真实</p>
<br>

<p>要设置照明模型, 有一些东西一定要设置: light source, material properties, location of viewer, surface orientation…</p>
<br>

<p><strong>Illumination model(IM)</strong>: determine the <strong>color of a point</strong> on a surface by simulating light-object(s) interaction.</p>
<ul>
<li>Local IM: illuminates <strong>isolated surface points</strong> through <strong>direct light</strong> sources. 光直接从光源射出</li>
<li>Global IM: illumination based on <strong>light transmissions between all surfaces</strong> in the environment.  通过反射的光</li>
</ul>
<p>Shading model: <strong>colors whole scene</strong> after applying an IM at a set of point samples. eg. <strong>an interpolation</strong></p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/light.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>Phong reflection model for local illumination: the color at<br>a point on a surface due to one light source:<br>$$<br>I &#x3D; ambient + diffuse + specular<br>$$</p>
<ul>
<li>Not concerned with translucent surfaces</li>
<li>Efficient enough to be interactive</li>
</ul>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/light_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>注意 l, n, r 都是单位向量</p>
<br>

<p>如果要在webgl中使用light, user需要指定一切参数</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Ambient-light-component"><a href="#Ambient-light-component" class="headerlink" title="Ambient light component"></a>Ambient light component</h4><p>Models <strong>general brightness</strong> due to light source</p>
<p>一个比较粗糙的方法.<br>$$<br>I_a &#x3D; k_aL_a,\ \ \ \ 0\le k_a\le 1<br>$$</p>
<ul>
<li>$L_a$: ambient light energy (property of light source)</li>
<li>$k_a$: ambient reflection coefficient (a material property)</li>
</ul>
<p>$k_a$ of ambient reflected; $1-k_a$ absorbed</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="光线的衰减"><a href="#光线的衰减" class="headerlink" title="光线的衰减"></a>光线的衰减</h4><p>光的强度应该根据<strong>点光源</strong>到平面的距离($d_L$)而减弱.</p>
<p>光的强度应该和$d_L^2$成反比</p>
<p>需要一个attenuation(衰减) 参数 $f_{att}$</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>红色的曲线是分母这个多项式函数的绘制图</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>light source</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h3 id="Shading-of-surface"><a href="#Shading-of-surface" class="headerlink" title="Shading of surface"></a>Shading of surface</h3><h4 id="Shading-of-flat-polygons"><a href="#Shading-of-flat-polygons" class="headerlink" title="Shading of flat polygons"></a>Shading of flat polygons</h4><p><img src="/Blog/Blog/intro/com_graphic/light_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_26.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/light_27.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Gouraud-Shadiing"><a href="#Gouraud-Shadiing" class="headerlink" title="Gouraud Shadiing"></a>Gouraud Shadiing</h4><p><img src="/Blog/Blog/intro/com_graphic/light_28.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>norm 是通过周围的norm计算得出的</p>
<br>

<h4 id="Color-interpolation"><a href="#Color-interpolation" class="headerlink" title="Color interpolation"></a>Color interpolation</h4><p><img src="/Blog/Blog/intro/com_graphic/light_29.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>linear是从一点到另外一点颜色渐变. </p>
<p>Bilinear是使用两次linear, 现用一次linear求出$l_a, l_b$ 在用一次linear 通过 $l_a, l_b$ 求出 $l_p$</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/light_30.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Phone-Shading"><a href="#Phone-Shading" class="headerlink" title="Phone Shading"></a>Phone Shading</h4><p>也叫做 accurate Shading</p>
<p>对每一个点都进行shading</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_31.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>对于边界, shading是比较敏感的, 可以看到有明显的顶点</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_32.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>解决的办法就是在边界进行更细致的划分.</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_33.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>当进行bilinear shading时, 颜色会取决于方向, 如果选择方向, 那么一个点shading的颜色就可能不一样, 例如</p>
<p><img src="/Blog/Blog/intro/com_graphic/light_34.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>左图p是ABD三个点进行加权平均的颜色, 而旋转90度后同样的点p则是取决于ABC三个点</p>
<p>解决方法是使用三角形.</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/light_35.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>可以看到顶点和面的normal vector 不一致, 解决方法也是细分</p>
<br>

<br>

<h3 id="Global-Illumination"><a href="#Global-Illumination" class="headerlink" title="Global Illumination"></a>Global Illumination</h3><p><strong>LIM(Local Illumination) 的局限</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>GIM 尝试 illuminate the <strong>whole scene dependently</strong></p>
<p>Local models 依然会被使用</p>
<p>Global Illumination会合并scene中所有的光. 尤其是反射光</p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>暴力方式:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> <span class="hljs-built_in">all</span> light sources<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">all</span> light rays <span class="hljs-keyword">from</span> a light source, <span class="hljs-keyword">in</span> <span class="hljs-built_in">all</span> directions<br>        trace ray until it hits screen <span class="hljs-keyword">or</span> “leaves the room” <span class="hljs-keyword">or</span> has decayed beyond recognition<br></code></pre></td></tr></table></figure>

<br>

<p>有两个公认的比较好的算法</p>
<ul>
<li>Ray tracing: discretize the image plane</li>
<li>Radiosity: discretize the scene objects&#x2F;environment</li>
</ul>
<p>二者都简化了rendering equation</p>
<br>

<p>Ray tracing 是 <strong>view-dependent</strong></p>
<p>traces light rays in <strong>reversed</strong> direction</p>
<p>是一个混合模型(hybrid model): Local IM也同时被使用</p>
<p>For each surface point hit, combine contribution of direct illumination(LIM) and reflection + refraction.</p>
<br>

<h4 id="Visible-surface-ray-tracing"><a href="#Visible-surface-ray-tracing" class="headerlink" title="Visible surface ray tracing"></a>Visible surface ray tracing</h4><p><img src="/Blog/Blog/intro/com_graphic/gim_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>从每个像素中射出一个ray: </p>
<ul>
<li><p>如果 ray 碰到了光源, 就返回光的颜色.</p>
</li>
<li><p>如果 ray 射向无尽(infinity), 返回背景颜色</p>
</li>
<li><p>如果 ray 碰到了物体表面(surface), 使用 Phong LIM.</p>
</li>
</ul>
<p>This basically performs visibility</p>
<p>Otherwise the same as Phong Shading</p>
<br>

<h4 id="add-shadow-rays"><a href="#add-shadow-rays" class="headerlink" title="add shadow rays"></a>add shadow rays</h4><p><img src="/Blog/Blog/intro/com_graphic/gim_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="add-reflections"><a href="#add-reflections" class="headerlink" title="add reflections"></a>add reflections</h4><p><img src="/Blog/Blog/intro/com_graphic/gim_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="add-transmissions"><a href="#add-transmissions" class="headerlink" title="add transmissions"></a>add transmissions</h4><p><img src="/Blog/Blog/intro/com_graphic/gim_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/gim_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>伪代码:</p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
 <br>

<h4 id="ray-tracing中的漫反射"><a href="#ray-tracing中的漫反射" class="headerlink" title="ray tracing中的漫反射"></a>ray tracing中的漫反射</h4><p>Current ray tracer only accounts for a single reflected ray</p>
<ul>
<li>Only perfect mirror reflection contributes to pixel color</li>
</ul>
<p>Diffuse relector:</p>
<ul>
<li>light from all directions should contribute</li>
</ul>
<p>Current ray tracer only suited for a shiny environment.</p>
<br>

<p>什么时候停止递归?</p>
<ul>
<li>当ray traveled long enough(energy decay)</li>
<li>当ray 离开一个enclosing environment</li>
</ul>
<p>More efficient ray-object intersection</p>
<ul>
<li>Use of <strong>bounding volumes</strong></li>
<li>Use of BSP trees or other <strong>spatial subdivisions</strong></li>
</ul>
<p><strong>Super-sampling</strong> a pixel. 例如 &gt; 1 rays per pixel</p>
<p><strong>Stochastically generate reflected rays</strong> for better illumination of diffuse objects: <strong>path tracing</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h4 id="Rendering-equation"><a href="#Rendering-equation" class="headerlink" title="Rendering equation"></a>Rendering equation</h4><p>光可以看作是能量.</p>
<p>假设在一个 <strong>closed environment</strong></p>
<p>有无限数量的rays 进行absorption, emission, and reflection.</p>
<p>We reach a steady state, an <strong>equilibrium</strong>, which we compute from the <strong>rendering equation</strong></p>
<p>Ray tracing: 是一种简化 rendering equation 的方法</p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>$\epsilon$ 是self emission.  integral 那部分是指所有反射光的加和</p>
<p>g 是occlusion, 如果$x’和x$ 之间有一个物体, 此时occlusion就是0, 没有光线从x’ 到达 x</p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>为了求解 I 这个公式, 我们需要不同的模型:</p>
<ol>
<li>A model of the emitted light $\epsilon$ (property of surface)</li>
<li>A model for the BRDF $\rho$ (property of surface)</li>
<li>Visibility information g (determined by geometry)</li>
</ol>
<br>

<p>然而这个equation 很复杂</p>
<ul>
<li>no analytic solution in general</li>
<li>practical algorithms perform lots of simplification</li>
</ul>
<p>如果我们能解出I, 那么我们就知道每个点的光的强度</p>
<ul>
<li>a color can be assigned based on view</li>
</ul>
<br>

<h4 id="Radiosity"><a href="#Radiosity" class="headerlink" title="Radiosity"></a>Radiosity</h4><p>Ray tracing 是 view dependent 的算法</p>
<ul>
<li>Consider $I(x’, x)$ only for<ul>
<li>x(a pixel location) on the image plane</li>
<li>x’ along ray from the eye through pixel x</li>
</ul>
</li>
<li>Consider x’’ only from perfect reflections and refractions</li>
</ul>
<br>

<p>Radiosity 则是 view independent 的算法</p>
<ul>
<li>Discretize the scene into small <strong>patches</strong>, to be flat shaded</li>
<li>Each patch is assumed to be <strong>perfectly diffuse</strong></li>
<li>Project patches onto image plane using computed colors - this is ok for diffuse surface(look the same from all views)</li>
</ul>
<br>

<h5 id="Light-source"><a href="#Light-source" class="headerlink" title="Light source"></a>Light source</h5><p>In ray tracing, 我们假设使用点光源, only one shadow ray to consider per light source</p>
<p>In radiosity, light sources modeled as patches. Can have <strong>area light sources</strong> – more realistic</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/gim_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h5 id="Form-factors"><a href="#Form-factors" class="headerlink" title="Form factors"></a>Form factors</h5><p><img src="/Blog/Blog/intro/com_graphic/gim_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><strong>compute</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/gim_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="Texture"><a href="#Texture" class="headerlink" title="Texture"></a>Texture</h2><p><img src="/Blog/Blog/intro/com_graphic/buffer.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>Texture 可以是 1D, 2D, 3D(solid texture).</p>
<p>2D textures 通常是以图片的形式给出, 是最常用的.</p>
<br>

<p>问题1: texture mapping 是 map 一个2D的texture 到一个任意形状的平面.</p>
<p>问题2: 如何基于texture coordinates给屏幕上的像素上色</p>
<br>

<h4 id="Texture-synthesis"><a href="#Texture-synthesis" class="headerlink" title="Texture synthesis"></a>Texture synthesis</h4><p><img src="/Blog/Blog/intro/com_graphic/buffer_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>webgl 只支持2D texture. opengl 支持3D texture</p>
<p>texture mapping 的基本步骤:</p>
<ol>
<li>获得texture image 并放入 GPU 的 texture memory</li>
<li>计算物体表面和texture image 之间的 mapping. 得到 result &#x3D; each surface point has a texture coordinate.</li>
<li>Assign texture coordinates to each fragment(pixel)</li>
<li>为每个fragment计算texture.</li>
</ol>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/buffer_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>point sampling 容易产生 aliasing:</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<p><img src="/Blog/Blog/intro/com_graphic/buffer_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>Approaches covered</p>
<ul>
<li>Linear mapping to texture parameterizable surfaces</li>
<li>“Two-step” method to texture close to canonical shapes</li>
<li>General mesh parameterization</li>
</ul>
<br>

<h4 id="Linear-Mapping"><a href="#Linear-Mapping" class="headerlink" title="Linear Mapping"></a>Linear Mapping</h4><p><img src="/Blog/Blog/intro/com_graphic/buffer_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="A-Two-step-approach"><a href="#A-Two-step-approach" class="headerlink" title="A Two-step approach"></a>A Two-step approach</h4><p><img src="/Blog/Blog/intro/com_graphic/buffer_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>先把图片贴到简单的3D图形上, 再让原本的3D(里面)从简单3D图形(外面)对应的点获取值</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Mesh-Parameterization"><a href="#Mesh-Parameterization" class="headerlink" title="Mesh Parameterization"></a>Mesh Parameterization</h4><p><img src="/Blog/Blog/intro/com_graphic/buffer_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Interpolation"><a href="#Interpolation" class="headerlink" title="Interpolation"></a>Interpolation</h4><p><img src="/Blog/Blog/intro/com_graphic/buffer_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h4 id="Pixel-color"><a href="#Pixel-color" class="headerlink" title="Pixel color"></a>Pixel color</h4><p><img src="/Blog/Blog/intro/com_graphic/buffer_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/buffer_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_26.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_27.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如何使用mipmap?</p>
<p>To rasterize a pixel P in screen space</p>
<ol>
<li><p>Determine how much (size) the pixel P covers the<br>original (level 0) texture map</p>
</li>
<li><p>Find the mipmap level i at which a single texel<br>covers about the same area at level 0 as P</p>
</li>
<li><p>Do a linear filtering using the mipmap at level i to<br>obtain a color for pixel P</p>
</li>
</ol>
<p>如果 size of coverage by pixel P lies between level i and i+1 of the mipmaps.</p>
<ol>
<li>Compute color of P at both mipmap levels: ci at<br>level i and ci+1 at level i+1</li>
<li>Linearly interpolate between ci and ci+1 based on<br>size of coverage and “size” of texels at mipmap<br>level i and i+1</li>
</ol>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_28.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_29.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="SAT"><a href="#SAT" class="headerlink" title="SAT"></a>SAT</h4><p>Summed area table</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_30.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>右图中每个元素等于左图对应位置元素左上角区域的加和</p>
<br>

<h4 id="Accumulation-Buffer-A-Buffer"><a href="#Accumulation-Buffer-A-Buffer" class="headerlink" title="Accumulation Buffer (A-Buffer)"></a>Accumulation Buffer (A-Buffer)</h4><ul>
<li>Same resolution as the screen</li>
<li>Part of the frame buffer, along with depth buffer,<br>front and back color buffer, etc.</li>
<li>Has more depth (can store more bits) than color<br>buffer and typically used as temporary storage<ul>
<li>Useful when accumulation&#x2F;summing pixel values</li>
<li>Do this in A-buffer to avoid clamping</li>
</ul>
</li>
</ul>
<p>使用SAT</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_31.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_32.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Bump-mapping"><a href="#Bump-mapping" class="headerlink" title="Bump mapping"></a>Bump mapping</h4><p><img src="/Blog/Blog/intro/com_graphic/buffer_33.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_34.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>Bump mapping 是费时的, 因为我们要shading at each point.</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_35.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Environmental-mapping"><a href="#Environmental-mapping" class="headerlink" title="Environmental mapping"></a>Environmental mapping</h4><p>Goal: simulate highly reflective objects without doing ray tracing<br>Idea:</p>
<ul>
<li>Map the environment onto an intermediate object (e.g.,<br>a sphere or a cube) – resulting map is called the<br>environment map</li>
<li>Then, depending on the direction of view, follow the<br>direction of reflection to pick up color of shiny objects<br>from the environment map – poor man’s ray tracing</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_36.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_37.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_38.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>只进行一次反射, 只再光到达environment时bound.</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_39.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h4 id="image-Compositing"><a href="#image-Compositing" class="headerlink" title="image Compositing"></a>image Compositing</h4><p>通过对 pixel 加权得到新的 pixel 值</p>
<p>可以用于处理 Aliasing</p>
<ul>
<li><p>Discrete (image processing style) techniques<br>used to generate certain effects</p>
<ul>
<li>e.g., fog, translucency, motion blur, etc.</li>
</ul>
</li>
<li><p>Frequent use of the accumulation buffer</p>
</li>
</ul>
<p>一个 pixel 可能在两个 polygon 之间</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_40.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="alpha-channel"><a href="#alpha-channel" class="headerlink" title="alpha channel"></a>alpha channel</h4><p>为了要处理这种情况, 就需要alpha channel.</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_41.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>alpha channel 存储权值</p>
<br>

<p>然而即使一个 pixel 可能在两个 polygon 之间, 那也有很多种情况.</p>
<p>理想状况是把所有的可能都考虑到, 例如下图<strong>应该</strong>生成不同的pixel</p>
<p>但是情况太多, 无法全部考虑</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_42.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们会假设 coverage 是随机的</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_43.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_44.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>使用 aliasing 来对 line 进行 Anti-aliasing</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_45.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_46.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_47.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>alpha channel 还有很多其他的用法.</p>
<p>可以当作透明度…</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_48.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>可以用作雾</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_49.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_50.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>alpha blending 经常使用 accumulation buffer</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_51.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>Multisampling and rendering</p>
<ul>
<li>Use A-buffer to store jittered versions of a scene</li>
<li>Combine them with original rendering of the<br>scene: multi-sampling or multi-rendering</li>
<li>Applications for motion blur and anti-aliasing</li>
</ul>
<p>Multisampling 可以用于 anti-aliasing </p>
<ul>
<li>Perturb views slightly to get multiple sampling of a scene and combine renderings in the A-buffer</li>
<li>This is referred to as the jittering approach</li>
<li>The new samples should be very close to the original samples</li>
<li>Blend the color buffer (original rendering) and result in the A-buffer to reduce aliasing artifacts</li>
</ul>
<br>

<p>Multisampling 可以用于 motion blur</p>
<p><img src="/Blog/Blog/intro/com_graphic/buffer_52.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h4 id="Tex-coordinates"><a href="#Tex-coordinates" class="headerlink" title="Tex-coordinates"></a>Tex-coordinates</h4><p><img src="/Blog/Blog/intro/com_graphic/tex_coor_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tex_coor_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="Smooth-Curves-and-Surfaces"><a href="#Smooth-Curves-and-Surfaces" class="headerlink" title="Smooth Curves and Surfaces"></a>Smooth Curves and Surfaces</h2><p>Although smooth curves and surfaces are converted to triangle meshes when <strong>rendered</strong>, they still provide the means for conceptual modeling</p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>在computer graphic中, curves 和 surfaces 通过 <strong>polynomials</strong> 表示</p>
<ul>
<li><strong>Approximation power</strong>: Can approximate any continuous<br>function to any accuracy (Weierstrass’s Theorem)</li>
<li>Can offer <strong>local control</strong> for shape design through the use of<br>piecewise polynomials</li>
<li>All <strong>derivatives and integrals</strong> are available (infinitely smooth)<br>and easy to compute</li>
<li><strong>Compact representation</strong></li>
<li><strong>Efficient evaluation</strong> – e.g., Horner’s rule</li>
</ul>
<br>

<h3 id="Horner’s-rule"><a href="#Horner’s-rule" class="headerlink" title="Horner’s rule"></a>Horner’s rule</h3><p><img src="/Blog/Blog/intro/com_graphic/curve_surf_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Fairness-vs-Smoothness"><a href="#Fairness-vs-Smoothness" class="headerlink" title="Fairness vs Smoothness"></a>Fairness vs Smoothness</h3><ul>
<li><p>Smoothness of curves and surfaces:</p>
<ul>
<li>Local property: often achieved by design</li>
<li>Related to existence and continuity of various derivative</li>
<li>$f(x) &#x3D; 3x^{100}-9x^2…$ is infinitely smooth, but is it “visually pleasing?”</li>
</ul>
</li>
<li><p>Fairness (often appears in CAGD literature)</p>
<ul>
<li><strong>Global property</strong>: often achieved by some form of <strong>energy minimization</strong></li>
<li>Related to the “energy” of a curve or surface</li>
<li>$f(x) &#x3D; 3x^{100}-9x^2…$ has high bending energy, and is not very visually pleasing.</li>
</ul>
</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Curve-design"><a href="#Curve-design" class="headerlink" title="Curve design"></a>Curve design</h3><p><img src="/Blog/Blog/intro/com_graphic/curve_surf_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h4 id="Hermite-curves"><a href="#Hermite-curves" class="headerlink" title="Hermite curves"></a>Hermite curves</h4><p><img src="/Blog/Blog/intro/com_graphic/curve_surf_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>$[t^3, t^2, t, 1]’&#x3D;[3t^2,2t, 1, 0]$ 所以$x’(0)&#x3D;[0,0,1,0]$</p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Cubic-Bezier-curve"><a href="#Cubic-Bezier-curve" class="headerlink" title="Cubic Bézier curve"></a>Cubic Bézier curve</h4><p><img src="/Blog/Blog/intro/com_graphic/curve_surf_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_26.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Cubic-B-Spline-curves"><a href="#Cubic-B-Spline-curves" class="headerlink" title="Cubic B-Spline curves"></a>Cubic B-Spline curves</h4><p><img src="/Blog/Blog/intro/com_graphic/curve_surf_27.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_28.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_29.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_30.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Tensor-product-surfaces"><a href="#Tensor-product-surfaces" class="headerlink" title="Tensor-product surfaces"></a>Tensor-product surfaces</h4><p><img src="/Blog/Blog/intro/com_graphic/curve_surf_31.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_32.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="TP-cubic-Bezier-patch"><a href="#TP-cubic-Bezier-patch" class="headerlink" title="TP cubic Bézier patch"></a>TP cubic Bézier patch</h4><p><img src="/Blog/Blog/intro/com_graphic/curve_surf_33.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_34.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="TP-cubic-B-spline-surfaces"><a href="#TP-cubic-B-spline-surfaces" class="headerlink" title="TP cubic B-spline surfaces"></a>TP cubic B-spline surfaces</h4><p><img src="/Blog/Blog/intro/com_graphic/curve_surf_35.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>Smoothness of Bézier surface</p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_36.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_37.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_38.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_39.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_40.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_41.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_42.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><strong>Advantages</strong></p>
<ul>
<li><strong>Efficient to compute&#x2F;render</strong> with simple algorithms –<br>weighted averages within a local neighborhood</li>
<li><strong>Provable smoothness</strong> if well designed</li>
<li>Can model surfaces with arbitrary topology (same<br>topology as control mesh) with relative ease<ul>
<li>One difficulty with objects composed of many patches – always<br>need to ensure continuity, e.g., in animation</li>
<li>A subdivision surface is a <strong>single piece and seamless</strong></li>
</ul>
</li>
<li>Flexible <strong>local control</strong> of surface features</li>
<li>Natural <strong>level-of-detail</strong> (hierarchical) representation</li>
</ul>
<br>

<p><strong>Subdivision schemes</strong></p>
<ul>
<li>Two aspects:<ul>
<li><strong>Topological rule</strong>: where to insert new vertices?<br>Are old vertices kept?</li>
<li><strong>Geometrical rule:</strong> spatial location of the new<br>vertices – typically given by a subdivision matrix</li>
</ul>
</li>
<li>The scheme should be designed with<ul>
<li>Provable smoothness in the limit</li>
<li>Local control</li>
<li>Computational efficiency</li>
<li>Convex hull property, etc.</li>
</ul>
</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_43.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_44.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_45.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_46.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h4 id="Subdivision-surface-vs-mesh"><a href="#Subdivision-surface-vs-mesh" class="headerlink" title="Subdivision surface vs. mesh"></a>Subdivision surface vs. mesh</h4><ul>
<li>Subdivision surfaces are <strong>smooth limit surfaces</strong></li>
<li>But in practice, e.g., rendering, only a few subdivisions<br>are needed to produced a <strong>mesh</strong> that is dense enough</li>
<li><strong>Polygonal meshes</strong>: a more general geometric model</li>
<li>Does not have to result from subdivision – <strong>irregular connectivity</strong> vs. <strong>subdivision connectivity</strong></li>
<li>Typically obtained from discretization of math representation or reconstruction out of a <strong>point cloud</strong></li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_47.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_48.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_49.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_50.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_51.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_52.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_53.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_54.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_55.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_56.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_57.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_58.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_59.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/curve_surf_60.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>





















<br>

<h1 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h1><h2 id="opencv-的一些简单的使用方法"><a href="#opencv-的一些简单的使用方法" class="headerlink" title="opencv 的一些简单的使用方法"></a>opencv 的一些简单的使用方法</h2><h3 id="读取视频-播放视频"><a href="#读取视频-播放视频" class="headerlink" title="读取视频, 播放视频"></a>读取视频, 播放视频</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_read_video</span>():<br>    <span class="hljs-comment"># 读取视频文件</span><br>    <span class="hljs-comment"># video = cv2.VideoCapture(&quot;resources/bobo.mp4&quot;)</span><br>    video = cv2.VideoCapture(<span class="hljs-number">0</span>) <span class="hljs-comment"># 将使用摄像头作为流</span><br><br>    <span class="hljs-comment"># video.set(3, 640)# 设置宽度640</span><br>    <span class="hljs-comment"># video.set(4, 480)# 设置高度</span><br>    <span class="hljs-comment"># video.set(10, 100)# 亮度</span><br><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        success, img = video.read()<br>        cv2.imshow(<span class="hljs-string">&quot;video test&quot;</span>, img)<br>        <span class="hljs-comment"># 如果输入q则退出</span><br>        <span class="hljs-keyword">if</span> cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span> == <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;q&#x27;</span>):<br>            <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure>

<h3 id="读取图片-写入图片-resize-图片"><a href="#读取图片-写入图片-resize-图片" class="headerlink" title="读取图片, 写入图片, resize 图片"></a>读取图片, 写入图片, resize 图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">img = cv2.imread(<span class="hljs-string">&quot;resources/street.jpg&quot;</span>) <br>imgResize = cv2.resize(img, (<span class="hljs-number">300</span>, <span class="hljs-number">200</span>)) <span class="hljs-comment"># 宽300 高200</span><br>cv2.imwrite(<span class="hljs-string">&quot;resources/resized_street.jpg&quot;</span>, imgResize)<br></code></pre></td></tr></table></figure>

<p>如果我们print(imgResize)</p>
<p>我们将得到一个 200x300x3 的 numpy array</p>
<p>显然200x300代表图的高和宽, 3代表3种颜色.</p>
<p>opencv读取图片时, <strong>使用BGR方式读取</strong> 而不是RGB</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">filter</span> = np.zeros((<span class="hljs-number">200</span>, <span class="hljs-number">300</span>, <span class="hljs-number">2</span>))<br>imgB = imgResize[:, :, :]<br>imgB[:, :, <span class="hljs-number">1</span>:<span class="hljs-number">3</span>] = <span class="hljs-built_in">filter</span>	<span class="hljs-comment"># 把red green部分设为0 </span><br>cv2.imshow(<span class="hljs-string">&quot;B&quot;</span>, imgB)<br></code></pre></td></tr></table></figure>

<br>

<h3 id="裁剪图片"><a href="#裁剪图片" class="headerlink" title="裁剪图片"></a>裁剪图片</h3><p>既然我们直到img使用np.array存储, 自然就可以使用numpy的方式截取数组中的部分元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">img = cv2.imread(<span class="hljs-string">&quot;resources/street.jpg&quot;</span>)<br>imgResize = cv2.resize(img, (<span class="hljs-number">300</span>, <span class="hljs-number">200</span>))<br><span class="hljs-comment"># 截取高和宽 (先截取高在截取宽)</span><br>imgCropped = imgResize[<span class="hljs-number">0</span>:<span class="hljs-number">100</span>, <span class="hljs-number">0</span>:<span class="hljs-number">150</span>]<br>cv2.imshow(<span class="hljs-string">&quot;cropped&quot;</span>, imgCropped)<br></code></pre></td></tr></table></figure>

<br>

<h3 id="im2double"><a href="#im2double" class="headerlink" title="im2double"></a>im2double</h3><p>matlab中有一个im2double函数. 它的作用是让读取的图片用double的形式存储. </p>
<p>正常imread一个图片后每个像素最大值是255. 使用im2double就可以让它突破最大值.</p>
<p>然而在opencv中没有这个函数. 因此我们可以自己实现一下.</p>
<p>这个函数的本质是将矩阵进行<a href="https://daolinzhou.github.io/Blog/2020/04/20/machine-learning-2/#%E6%9C%80%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96-normalization-normalize">最值归一化</a>, 将矩阵中的每个值转化为一个0-1之间的小数.</p>
<p>所以我们可以这样实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">im2double</span>(<span class="hljs-params">img: np.ndarray</span>):<br>    minN = np.<span class="hljs-built_in">min</span>(img)<br>    maxN = np.<span class="hljs-built_in">max</span>(img)<br>    out1 = (img - minN) / (maxN - minN)<br>    <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">im2double</span>(<span class="hljs-params">img: np.ndarray</span>):<br>    out = cv2.normalize(img.astype(<span class="hljs-string">&#x27;float&#x27;</span>), <span class="hljs-literal">None</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, cv2.NORM_MINMAX)<br>    <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>

<br>

<br>

<h2 id="颜色"><a href="#颜色" class="headerlink" title="颜色"></a>颜色</h2><p>对于一张灰色的图片, 如果我们限制灰色的等级. 例如我们现在只有5种灰色</p>
<p><img src="/Blog/Blog/intro/com_graphic/black.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>左边是原图, 右边是限制灰度的图.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) <span class="hljs-comment"># 转化为灰色</span><br>imgGray_d = im2double(imgGray)	<span class="hljs-comment"># 用double表示</span><br>imgGray_d2 = np.array(imgGray_d*, dtype=<span class="hljs-string">&#x27;int&#x27;</span>) / <span class="hljs-number">5</span> <span class="hljs-comment"># 让每一个double只有5种可能. (乘以5, 转int, 再除以5)</span><br></code></pre></td></tr></table></figure>

<br>

<p>电脑显示颜色的方式:</p>
<p><img src="/Blog/Blog/intro/com_graphic/colors.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="RGB"><a href="#RGB" class="headerlink" title="RGB"></a>RGB</h3><p>RGB是最常用的颜色显示方式, 有3个通道(R, G, B)</p>
<p><img src="/Blog/Blog/intro/com_graphic/rgb.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Y-Cb-Cr"><a href="#Y-Cb-Cr" class="headerlink" title="Y Cb Cr"></a>Y Cb Cr</h3><p>这个方式会把大部分的信息存储到elimination channel(white channel)</p>
<p>再把颜色信息存储到剩余两个channel中</p>
<p><img src="/Blog/Blog/intro/com_graphic/ycbcr.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>早期电视就是使用这个方式, 一个黑白channel就存储大部分信息</p>
<br>

<h3 id="YUV"><a href="#YUV" class="headerlink" title="YUV"></a>YUV</h3><p>类似的还有YUV</p>
<p>it is better to represent color as we see them</p>
<p><img src="/Blog/Blog/intro/com_graphic/Barn-yuv.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="HSV"><a href="#HSV" class="headerlink" title="HSV"></a>HSV</h3><p>Hue channel 存储我们想要表达的颜色</p>
<p>Saturation 表示 how “deep” that color is.</p>
<p>value 表示亮度(brightness)</p>
<p>如图所示, 当value非常小的时候(所有颜色都是黑色),  hue channel就失去了它的意义</p>
<p><img src="/Blog/Blog/intro/com_graphic/hsv.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">img = cv2.imread(<span class="hljs-string">&quot;../resources/street.jpg&quot;</span>)<br>img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)<br></code></pre></td></tr></table></figure>

<br>

<h3 id="LAB"><a href="#LAB" class="headerlink" title="LAB"></a>LAB</h3><p>lab的这个用途非常酷, 以后学 TODO<a target="_blank" rel="noopener" href="https://www.mathworks.com/help/images/color-based-segmentation-using-the-l-a-b-color-space.html">使用lab进行基于颜色的分割</a></p>
<br>

<h3 id="RGBA"><a href="#RGBA" class="headerlink" title="RGBA"></a>RGBA</h3><p>RGBA是由RGB拓展出来的, 在原本RGB三个channel之上又添加一个alpha channel 代表透明度</p>
<p>不是所有格式都支持rbga, jpg只有3条channel, 但是png支持它</p>
<br>

<p>alpha &#x3D;&#x3D; 0就代表透明, alpha &#x3D;&#x3D; 1就代表不透明</p>
<p>加入我们想把两张图片叠加起来.</p>
<p>这是第一张图, 左边是图片, 右边是alpha层</p>
<p><img src="/Blog/Blog/intro/com_graphic/rgba_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>第二张图</p>
<p><img src="/Blog/Blog/intro/com_graphic/rgba_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>结果图</p>
<p><img src="/Blog/Blog/intro/com_graphic/rgba_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>自然我们希望在 alpha&#x3D;0 的时候显示第二张图, alpha&#x3D;1 的时候显示第一张图.</p>
<p>假设第一张图为 $f_0(x)$, 第二张图为$f_1(x)$, 结果图为$g(x)$, x代表像素的位置<br>$$<br>g(x) &#x3D; \alpha f_0(x) + (1-\alpha) f_1(x)<br>$$<br>当 alpha &#x3D;&#x3D; 0 时$g(x) &#x3D; f_1(x)$, 当 alpha&#x3D;&#x3D;1 时$g(x)&#x3D;f_0(x)$</p>
<p>一种从bgr转为bgra的方法 (opencv默认用bgr而不是rgb)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">img_bgra = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)<br>img_alpha = np.random.rand(height, width) &gt; <span class="hljs-number">0.5</span><br>img_bgra[:, :, <span class="hljs-number">3</span>] = img_alpha	<span class="hljs-comment"># 修改第4个channel</span><br></code></pre></td></tr></table></figure>

<br>

<br>

<br>

<p>为了方便展示图片, 我实现了一个简易的imshow, 可以让图片并排并列, 成网格状摆放</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">imshow</span>(<span class="hljs-params">name: <span class="hljs-built_in">str</span>, imgs: <span class="hljs-built_in">list</span>, path=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-comment"># 如果是0维升维</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(imgs, np.ndarray):<br>        imgs = [imgs]<br>    <span class="hljs-comment"># 如果是一维, 则进行升维</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(imgs[<span class="hljs-number">0</span>], np.ndarray):<br>        imgs = [imgs]<br><br>    height = <span class="hljs-built_in">len</span>(imgs)<br>    combin = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(height):<br>        <span class="hljs-comment"># channel 数必须一致</span><br>        combin.append(np.concatenate(imgs[i], axis=<span class="hljs-number">1</span>))<br>    result = np.concatenate(combin, axis=<span class="hljs-number">0</span>)<br>    cv2.imshow(name, result)<br></code></pre></td></tr></table></figure>

<br>

<br>

<h2 id="图像过滤-Image-Filtering"><a href="#图像过滤-Image-Filtering" class="headerlink" title="图像过滤 Image Filtering"></a>图像过滤 Image Filtering</h2><p>kernel也可以看作时filter</p>
<h3 id="box-kernel"><a href="#box-kernel" class="headerlink" title="box kernel"></a>box kernel</h3><p>box kernel 也叫做 box filter, 因为如果我们 plot 这个kernel, 它看起来像box.</p>
<p>可以参考box blur</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    a = np.zeros(<span class="hljs-number">10</span>)<br>    a[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>] = <span class="hljs-number">1</span><br>    <span class="hljs-built_in">print</span>(a)<br><br>    kernel = [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br>    afilt = a.copy()	<span class="hljs-comment"># 记录使用核之后的图的样子</span><br>    <span class="hljs-comment"># 使用核</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">9</span>):<br>        afilt[i] = <span class="hljs-built_in">sum</span>(a[i-<span class="hljs-number">1</span>:i+<span class="hljs-number">2</span>] * kernel)<br>    <span class="hljs-built_in">print</span>(afilt)<br><br>    plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>), a, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br>    plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>), afilt, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure>

<p>output:</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-string">[0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]</span><br><span class="hljs-string">[0. 0. 1. 2. 3. 2. 1. 0. 0. 0.]</span><br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/box_kernel.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>新的数组 afilt[i] 位置的元素, 是原本 a[i-1], a[i], a[i+1] 这三个位置的和</p>
<p>换句话说, afilt 第i个位置的<strong>信息</strong>, 33%来自于a[i-1], 33%来自于a[i],33%来自于a[i+1]</p>
<br>

<p>然而原本 a 的最大值为1, 但与kernel运算后, afilt的最大值为3. 因为虽然 afilt[i] 只有 33% 来自与 a[i-1], 但是它包含了a[i-1]的所有信息.</p>
<p>afilt[i]的信息肯定大于a[i-1], a[i], a[i+1]</p>
<p>如果我们对图像进行这样的操作是非常不好的. 这会使得图形某一部分信息非常多, 某一部分信息非常少. 换句话说, 某些部分非常亮, 某些部分非常暗.</p>
<br>

<p>我们希望整个图的信息量不变</p>
<p>所以我们要对kernel进行<strong>标准化</strong>, 使得kernel的sum为1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">kernel = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>kernel = kernel / <span class="hljs-built_in">sum</span>(kernel)<br></code></pre></td></tr></table></figure>

<p>把之前kernel的定义改为上面两句话.</p>
<p>output:</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-string">[0. 0. 0.    1.    1. 1.    0.    0. 0. 0.]</span><br><span class="hljs-string">[0. 0. 0.333 0.666 1. 0.666 0.333 0. 0. 0.]</span><br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/kernel_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>三者的比较(蓝色是原图, 红色是使用第一种kernel, 绿色是归一化后kernel)</p>
<p><img src="/Blog/Blog/intro/com_graphic/kernel_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>假设这是一个图像, 那么使用这个归一化后的kernel在2-6这个区间就会变得平滑一些(模糊一些), 因为灰度是逐渐变化的而不是突然从0变为1.</p>
<br>

<h3 id="互相关-cross-correlation"><a href="#互相关-cross-correlation" class="headerlink" title="互相关 cross-correlation"></a>互相关 cross-correlation</h3><p>上面我们进行的步骤叫做cross-correlation<br>$$<br>G[i, j]&#x3D;\sum_{u&#x3D;-k}^k\sum_{v&#x3D;-k}^kH[u, v]F[i+u, j+v]<br>$$<br>显然H是核, F是我们要处理的图形, G是得到的结果. 为了简化公式通常写作:<br>$$<br>G &#x3D; H \otimes F<br>$$<br>上面的例子中k就是1, v 就是从 -1 到 1 进行变化 (由于只有一维所以u&#x3D;0)</p>
<p><img src="/Blog/Blog/intro/com_graphic/cross-correlation.gif" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="卷积-Convolution"><a href="#卷积-Convolution" class="headerlink" title="卷积 Convolution"></a>卷积 Convolution</h3><p>然而当我们讨论<strong>image filtering</strong>, 我们将使用一个类似的, 但又不同的operation: 卷积<br>$$<br>G[i, j]&#x3D;\sum_{u&#x3D;-k}^k\sum_{v&#x3D;-k}^kH[u, v]F[i-u, j-v]<br>$$<br>同样简写为<br>$$<br>G &#x3D; H \star F<br>$$<br>其实这只是相当于反转kernel, 之后作cross-correlation</p>
<p>然而, 在数学的意义上, we gain a lot from using <strong>convolution</strong> instead of cross-correlation.</p>
<p>数学意义上更好操作</p>
<p><img src="/Blog/Blog/intro/com_graphic/coverlution_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>卷积支持乘法交换律, 乘法结合律.</p>
<p>这意味着如果我们向用两个filter对一张图进行过滤</p>
<p>我们可以先convolve the filter, 再进行image filtering. 这样就节约很多计算. 这点correlation就做不到</p>
<br>

<p>Distributes over addition 说, 如果你想用filter过滤两个图片, 之后再将它们相加(通常在预处理, 计算机视觉中经常使用). 我们可以现将两张图相加在进行一次 filtering 就可以了</p>
<br>

<blockquote>
<p>convolution in image-domain is equivalent to the multiplication in  frequency domain </p>
<p>卷积的另一个重要操作后序会讲, 在特殊的domain(image domain)进行卷积 等价于 在 frequency domain </p>
</blockquote>
<br>

<br>

<h3 id="Linear-Shift-invariant-Filter"><a href="#Linear-Shift-invariant-Filter" class="headerlink" title="Linear Shift-invariant Filter"></a>Linear Shift-invariant Filter</h3><ul>
<li>About modifying pixels based on <strong>neighborhood</strong>. Local methods simplest.(根据周围的像素计算出当前像素)</li>
<li>Linear means linear combination of neighbors. Linear methods simplest.</li>
<li>Shift-invariant means doing the same for each pixel. Same for all is simplest.</li>
</ul>
<blockquote>
<p>也有很多Shift-variant filters. </p>
</blockquote>
<br>

<p>L is a linear map&#x2F;transform if<br>$$<br>L[\alpha l_1+\beta l_2]&#x3D;\alpha L[l_1]+\beta L[l_2]<br>$$<br><a href="https://daolinzhou.github.io/Blog/2020/01/24/play-with-linear-algebra-3/#%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2">线性变换</a></p>
<br>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 卷积</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convolution</span>(<span class="hljs-params">img, x, y, kernel</span>):<br>    max_h, max_w = img.shape<br>    height, width = kernel.shape<br>    width = (width - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>    height = (height - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>    res = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(-width, width + <span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(-height, height + <span class="hljs-number">1</span>):<br>            new_x, new_y = x - i, y - j<br>            p = <span class="hljs-number">0</span>	<span class="hljs-comment"># 如果超出边界, 默认为 0</span><br>            <span class="hljs-comment"># check range</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-number">0</span> &lt;= new_x &lt; max_w <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= new_y &lt; max_h:<br>                p = img[new_y][new_x]<br>            res += p * kernel[j + height][i + width]<br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-comment"># 只处理 img 是一层的情况</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">try_filter</span>(<span class="hljs-params">img, kernel</span>):<br>    height, width = img.shape<br>    res = img.copy()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(height):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(width):<br>            res[i][j] = convolution(img, j, i, kernel)<br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/resized_street.jpg&quot;</span>)<br>    boxkern = np.ones((<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>    boxkern = boxkern / np.<span class="hljs-built_in">sum</span>(boxkern)<br>    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br>    <span class="hljs-comment"># 这一行是使用cv2的卷积操作</span><br>    <span class="hljs-comment"># new_img = cv2.filter2D(img_gray, -1, boxkern)</span><br>    new_img = try_filter(img_gray, boxkern) <span class="hljs-comment"># 使用自己写的卷积</span><br>    cv2.imshow(<span class="hljs-string">&quot;box kernel&quot;</span>, new_img)<br></code></pre></td></tr></table></figure>

<p>既然知道了卷积的原理, 就可以尝试进行实现.</p>
<p>但是这里没有用任何优化的方法. 仅仅是根据上面的卷积公式进行实现.</p>
<blockquote>
<p>注释代码是使用cv2的卷积, 这个操作更快. 但操作结果得到的效果图和我自己写的try_filter是一模一样的</p>
<p>所以就不放filter2D的效果图了</p>
<p>尝试实现只是作为兴趣, 看代码实现是否真的和理论一样</p>
</blockquote>
<p>效果图:</p>
<p><img src="/Blog/Blog/intro/com_graphic/box_filter.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>自然左边就是原图, 右边是过滤后的图</p>
<p>很明显图片是模糊的. 因为每一个pixel都和周围的pixel中和了一下. </p>
<br>

<br>

<p>当然如果使用filter2D, 不将图片转为灰色也是可以的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/resized_street.jpg&quot;</span>)<br>    boxkern = np.ones((<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>    boxkern = boxkern / np.<span class="hljs-built_in">sum</span>(boxkern)<br>    new_img = cv2.filter2D(img, -<span class="hljs-number">1</span>, boxkern)<br>    imshow(<span class="hljs-string">&quot;box kernel&quot;</span>, new_img)<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/box_filter_color.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h3 id="高斯核-gauss-kernel"><a href="#高斯核-gauss-kernel" class="headerlink" title="高斯核 gauss kernel"></a>高斯核 gauss kernel</h3><p>高斯核常用于降噪</p>
<p>高斯核与高斯分布 (<a href="https://daolinzhou.github.io/Blog/2020/04/04/stat-203/">正态分布</a>) 密切相关. 举一个例子:</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs dns">[[<span class="hljs-number">0.002 0</span>.<span class="hljs-number">013</span> <span class="hljs-number">0.021 0</span>.<span class="hljs-number">013</span> <span class="hljs-number">0</span>.<span class="hljs-number">002</span>]<br> [<span class="hljs-number">0.013 0</span>.<span class="hljs-number">059</span> <span class="hljs-number">0.098 0</span>.<span class="hljs-number">059</span> <span class="hljs-number">0</span>.<span class="hljs-number">013</span>]<br> [<span class="hljs-number">0.021 0</span>.<span class="hljs-number">098 0.162</span> <span class="hljs-number">0.098 0</span>.<span class="hljs-number">021</span>]<br> [<span class="hljs-number">0.013 0</span>.<span class="hljs-number">059</span> <span class="hljs-number">0.098 0</span>.<span class="hljs-number">059</span> <span class="hljs-number">0</span>.<span class="hljs-number">013</span>]<br> [<span class="hljs-number">0.002 0</span>.<span class="hljs-number">013</span> <span class="hljs-number">0.021 0</span>.<span class="hljs-number">013</span> <span class="hljs-number">0</span>.<span class="hljs-number">002</span>]]<br></code></pre></td></tr></table></figure>

<p>这就是一个5x5的高斯核(方差为1).  可以看到, 越靠近中心(2, 2), 值就越高.</p>
<p>距离中心越远, 值就越低.</p>
<p>这个矩阵的每个维度都服从正态分布. 更一般的高斯核可视化地看一下</p>
<p><img src="/Blog/Blog/intro/com_graphic/gauss_kernel.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<blockquote>
<p>其实我认为可以将高斯核理解为一个服从正态分布的二维矩阵</p>
</blockquote>
<p>如果说box kernel是<strong>均衡</strong>地从周围的像素中提取信息中和. 那么高斯核就是有倾向性地中和.</p>
<p>上面的矩阵和约为1. 同样每一个元素可以理解为”占比”</p>
<br>

<p>以上面的高斯核为例</p>
<p>如果我们使用高斯核对图片过滤某一点. 新的图中这一点 16% 的信息来自于原图的这一点, 9%的信息分别来自上下左右四个相邻点. 依次递减</p>
<p>这也是非常合理的. 因为一个点距离原点(我们要过滤的点)越远, 我们自然就认为它对原点影响越小. </p>
<p>反之离得越近, 就说明影响越大. 自然占比就越高.</p>
<blockquote>
<p>box kernel 是无论多远, 占比都是一样的. 因为box kernel中每一个元素都是一样的. </p>
</blockquote>
<blockquote>
<p>高斯核支持很多操作</p>
<p><strong>可以用一个高斯核对另一个高斯核进行卷积, 得到一个新的高斯核</strong></p>
</blockquote>
<br>

<h4 id="openCV中使用高斯核"><a href="#openCV中使用高斯核" class="headerlink" title="openCV中使用高斯核"></a>openCV中使用高斯核</h4><p>openCV中没有直接生成二维高斯核的函数, 但我们可以通过两个一维的高斯核相乘得到一个二维高斯核</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">kernel_factor = cv2.getGaussianKernel(<span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br>gauss_kernel = kernel_factor.dot(kernel_factor.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure>

<p>kernel_factor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">[[<span class="hljs-number">0.05448868</span>]<br> [<span class="hljs-number">0.24420134</span>]<br> [<span class="hljs-number">0.40261995</span>]<br> [<span class="hljs-number">0.24420134</span>]<br> [<span class="hljs-number">0.05448868</span>]]<br></code></pre></td></tr></table></figure>

<p>这个kernelfactor是一个5x1的高斯核, 同样里中心越近值就越高. 且所有元素的总和为1.</p>
<p>gausskernel的第一行, 或者第一列的和对应kernelfactor中的第一位元素.(可以用线性代数证明)</p>
<br>

<h3 id="对比-box-kernel-和-gauss-kernel的效果"><a href="#对比-box-kernel-和-gauss-kernel的效果" class="headerlink" title="对比 box kernel 和 gauss kernel的效果"></a>对比 box kernel 和 gauss kernel的效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_kernels</span>():<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/resized_street.jpg&quot;</span>)<br>	<br>    <span class="hljs-comment"># 定义高斯核</span><br>    kernel_factor = cv2.getGaussianKernel(<span class="hljs-number">41</span>, <span class="hljs-number">8</span>)<br>    gauss_kernel = kernel_factor.dot(kernel_factor.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br>    <br>    <span class="hljs-comment"># 定义box核</span><br>    box_kernel = np.ones((<span class="hljs-number">41</span>, <span class="hljs-number">41</span>))<br>    box_kernel = box_kernel / np.<span class="hljs-built_in">sum</span>(box_kernel)<br>    <br>    <span class="hljs-comment"># 使用核</span><br>    new_img_gauss = cv2.filter2D(img, -<span class="hljs-number">1</span>, gauss_kernel)<br>    new_img_box = cv2.filter2D(img, -<span class="hljs-number">1</span>, box_kernel)<br>   	<br>    <span class="hljs-comment"># 这个函数是我自己写的, 让两张图并排打印</span><br>    imshow(<span class="hljs-string">&quot;compare kernel&quot;</span>, [new_img_box, new_img_gauss])<br></code></pre></td></tr></table></figure>

<p>原图 </p>
<p><img src="/Blog/Blog/intro/com_graphic/resized_street.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>使用kernel后的图 (左边用box kernel, 右边用 gauss kernel)</p>
<p><img src="/Blog/Blog/intro/com_graphic/comparekernel.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>显然使用高斯核的 blur image(右图) 看起来更加自然</p>
<br>

<p>使用高斯核的时候要注意它的尺寸. 例如下图的高斯核, 它丢失了某些信息(四周没有收敛到0)</p>
<p><img src="/Blog/Blog/intro/com_graphic/gauss_kernel_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>理想情况下, 我们希望高斯核能表达整个(或者大部分) 钟型shape. 下图就勉强可以.</p>
<p><img src="/Blog/Blog/intro/com_graphic/gauss_kernel_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><strong>记得<a href="https://daolinzhou.github.io/Blog/2020/04/04/stat-203/#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E7%89%B9%E5%BE%81">正态分布的 3 sigma 法则</a></strong>, 我们设置的size, 最好要和设置的sigma匹配</p>
<br>

<p>根据这个3 sigma法则, 调整sigma就可以调整整个图片的模糊程度</p>
<p>如果我们希望更多的信息提取自身(以及周围, sigma就设置小一点, 图片更清晰) 反之把sigma设置大一些, 图片更模糊</p>
<p><img src="/Blog/Blog/intro/com_graphic/change_sigma.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这4张图都是使用51x51的高斯核, 只是改变他们的sigma值就得到了不同的模糊程度的图片</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">左上</th>
<th align="center">右上</th>
<th align="center">左下</th>
<th align="center">右下</th>
</tr>
</thead>
<tbody><tr>
<td align="center">sigma值</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">4</td>
</tr>
</tbody></table>
<br>

<h3 id="separable-kernel"><a href="#separable-kernel" class="headerlink" title="separable kernel"></a>separable kernel</h3><p>有一些kernel叫做separable kernel. 高斯核就是其中之一</p>
<p>这种kernel允许使用一个一维的kernel, 而不是二维kernel进行过滤.</p>
<p>思想就是先让这个kernel沿着x轴进行过滤, 得到一个临时图, 再让kernel沿着临时图的y轴过滤. 得到的效果和使用2维kernel一样</p>
<p>举个例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/resized_street.jpg&quot;</span>)<br>    <br>    <span class="hljs-comment"># 得到一个 一维的41个元素, 标准差维8的高斯核</span><br>    kernel_factor = cv2.getGaussianKernel(<span class="hljs-number">41</span>, <span class="hljs-number">8</span>)	<br>    <span class="hljs-comment"># 得到一个二维(41, 41), 标准差为8个高斯核</span><br>    gauss_kernel = kernel_factor.dot(kernel_factor.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br><br>    <span class="hljs-comment"># 使用二维高斯核过滤</span><br>    new_img_gauss_1 = cv2.filter2D(img, -<span class="hljs-number">1</span>, gauss_kernel)<br><br>    <span class="hljs-comment"># 使用一维高斯核沿轴分别过滤两次</span><br>    temp_img = cv2.filter2D(img, -<span class="hljs-number">1</span>, kernel_factor)<br>    new_img_gauss_2 = cv2.filter2D(temp_img, -<span class="hljs-number">1</span>, kernel_factor.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br><br>    imshow(<span class="hljs-string">&quot;compare kernel&quot;</span>, [[img, temp_img], [new_img_gauss_1, new_img_gauss_2]])<br></code></pre></td></tr></table></figure>

<br>

<p><img src="/Blog/Blog/intro/com_graphic/gauss_kernel_3.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>左上是原图, 右上是使用一维高斯核过滤得到的临时图</p>
<p>左下是使用二维高斯核的结果, 右下是使用一维高斯核的结果</p>
<p>可以看到, 左下和右下两张图是一样的.</p>
<br>

<br>

<p>而且使用一维核比使用二维核要快. (filter变得越大, 差距就越明显)</p>
<p>卷积可以看作是向量的点乘. (把两个矩阵reshape成一维, 其中一个reverse, 之后点乘)</p>
<ul>
<li><p>那么使用 (5, 5) 的<strong>二维核</strong>对每个pixel进行的卷积的操作就是两个<strong>25维的向量</strong>进行点乘 (进行25次对应元素相乘再相加的操作)</p>
</li>
<li><p>而使用(1, 5)的<strong>一维核</strong>对每个pixel进行的卷积的操作是: 进行两次, 两个<strong>5维的向量</strong>点乘 (进行10次对应元素相乘再相加的操作)</p>
</li>
</ul>
<p>因此我觉得使用一维高斯核进行过滤才是opencv推荐的做法.</p>
<br>

<br>

<h3 id="Shift-filter"><a href="#Shift-filter" class="headerlink" title="Shift filter"></a>Shift filter</h3><p>假设我们使用这样一个kernel, 41x41的kernel, 只有[2, 2]位置的元素为1, 其余元素为0, 那么此时对图片进行过滤会得到什么?</p>
<p><img src="/Blog/Blog/intro/com_graphic/kernel_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/resized_street.jpg&quot;</span>)<br><br>    kernel = np.zeros((<span class="hljs-number">41</span>, <span class="hljs-number">41</span>))<br>    kernel[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>] = <span class="hljs-number">1</span><br><br>    img_kernel = cv2.filter2D(img, -<span class="hljs-number">1</span>, kernel)<br><br>    <span class="hljs-comment"># 对两张图截取同一片区域</span><br>    img_part_1 = img[<span class="hljs-number">100</span>:<span class="hljs-number">500</span>, <span class="hljs-number">150</span>:<span class="hljs-number">350</span>, :]<br>    img_part_2 = img_kernel[<span class="hljs-number">100</span>:<span class="hljs-number">500</span>, <span class="hljs-number">150</span>:<span class="hljs-number">350</span>, :]<br><br>    imshow(<span class="hljs-string">&quot;test kernel&quot;</span>, [img_part_1, img_part_2])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/shift.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>左边是原图的截图, 右边是使用kernel后的截图</p>
<p>很明显, 使用kernel后图片有了水平方向的右移. (虽然不同容易看出来, 但其实也有垂直方向的下移)</p>
<p>因为我们的核是(41, 41), 只有[2, 2]是1. 举个例子, 如果我们计算(21, 21)位置的像素, 就会让[1:41, 1:41]这个区域和核进行卷积</p>
<p>由于其余位置都是0, 所以我们应该得到的是 原图 (39, 39) 位置的元素</p>
<p>然而<strong>filter2D</strong>实际上做的不是卷积, 而是correlation. 所以实际上得到的原图的位置(2, 2)的元素</p>
<br>

<p>新图中(20, 20)的元素是原图(2, 2)的元素, 同理(21, 21)的元素是原图(3, 3)的元素… …</p>
<p>相当于对原图进行了一次平移, 向下平移18个pixel, 向右平移18个pixel.</p>
<br>

<br>

<h3 id="Median-Filter"><a href="#Median-Filter" class="headerlink" title="Median Filter"></a>Median Filter</h3><p>Median Filter 运用中位数作为结果.</p>
<p>这不是一个linear filter, 进行的操作不是线性操作</p>
<p>opencv中用medianBlur函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>res = cv2.medianBlur(img, <span class="hljs-number">7</span>)<br>imshow(<span class="hljs-string">&quot;median filter&quot;</span>, [img, res])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/medianFilter.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>可以看到, 颜色集中的部分更清晰. 扑克牌上的红桃, 黑桃等</p>
<br>

<p>median filter的一个作用就是去除 salt and pepper noise (椒盐噪声)</p>
<blockquote>
<p>什么是椒盐噪声</p>
<p><img src="/Blog/Blog/intro/com_graphic/sp_noise.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>就是随机创建一些纯白或纯黑的像素点. 看起来像椒盐一样.</p>
<p>改进了一下网上的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sp_noise</span>(<span class="hljs-params">img, prob</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    添加椒盐噪声</span><br><span class="hljs-string">    prob:噪声比例</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    output = img.copy()<br>    prob /= <span class="hljs-number">2</span>   <span class="hljs-comment"># 一半概率生成黑色</span><br>    thres = <span class="hljs-number">1</span> - prob  <span class="hljs-comment"># 一半概率生成白色</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(img.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(img.shape[<span class="hljs-number">1</span>]):<br>            rdn = np.random.random()<br>            <span class="hljs-keyword">if</span> rdn &lt; prob:<br>                output[i][j] = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">elif</span> rdn &gt; thres:<br>                output[i][j] = <span class="hljs-number">255</span><br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
</blockquote>
<p>我们先创建一个noise image</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>noi_img = sp_noise(img, <span class="hljs-number">0.1</span>)<br>cv.imshow(<span class="hljs-string">&quot;sp_noise&quot;</span>, noi_img)<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/sp_noise_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们要尝试对右边这张图进行降噪.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>    noi_img = sp_noise(img, <span class="hljs-number">0.1</span>)<br>    <br>    img_median = cv2.medianBlur(noi_img, <span class="hljs-number">3</span>)<br>    imshow(<span class="hljs-string">&quot;sp&quot;</span>, [img, noi_img, img_median])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/denoise.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>最右边就是降噪后的图. </p>
<p>这里使用了一个思想: <strong>中位数受极值影响的效果最小</strong>.</p>
<p>而无论是纯黑还是纯白, 他们都是极值. 我们取中位数, 自然就是降噪.</p>
<br>

<br>

<p>当我们对一个pixel进行filtering. 我们是根据它相邻的pixel进行计算的. 然而不是所有的像素都有相邻像素. 例如在边缘或角落的像素是没有某些相邻像素的.</p>
<p><img src="/Blog/Blog/intro/com_graphic/cross-correlation.gif" srcset="/Blog/img/loading.gif" lazyload></p>
<p>例如上图就是通过填充0, 来代替相邻像素.</p>
<br>

<br>

<h3 id="腐蚀和膨胀"><a href="#腐蚀和膨胀" class="headerlink" title="腐蚀和膨胀"></a>腐蚀和膨胀</h3><p>erode(腐蚀) 是寻找在neighborhood中的最小值作为填充</p>
<p>dilate(膨胀) 做的是相反的操作, 寻找在neighborhood中的最大值作为填充</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>	<br>    <span class="hljs-comment"># 将在5x5的区域内寻找最大或最小值</span><br>    kernel = np.ones((<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><br>    erode_img = cv2.erode(img, kernel)<br>    dilate_img = cv2.dilate(img, kernel)<br>    imshow(<span class="hljs-string">&quot;erode and dilate&quot;</span>, [img, erode_img, dilate_img])<br></code></pre></td></tr></table></figure>

<p> <img src="/Blog/Blog/intro/com_graphic/erode_and_dilate.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>分别是上面三张分别是: 原图, 腐蚀图, 膨胀图</p>
<br>

<h4 id="开运算闭运算"><a href="#开运算闭运算" class="headerlink" title="开运算闭运算"></a>开运算闭运算</h4><p>matlab中有两个函数 imopen, imclose, </p>
<ul>
<li>imopen 等同与 dilate(erode(img)), 先腐蚀在膨胀</li>
<li>imclose 等同于 erode(dilate(img)), 先膨胀在腐蚀</li>
</ul>
<p>opencv中也有同样的操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>():<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br><br>    kernel = np.ones((<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><br>    <span class="hljs-comment"># 自己实现开运算闭运算</span><br>    erode_img = cv2.erode(img, kernel)<br>    dilate_img = cv2.dilate(img, kernel)<br>    ed = cv2.erode(dilate_img, kernel)<br>    de = cv2.dilate(erode_img, kernel)<br>    imshow(<span class="hljs-string">&quot;de-ed&quot;</span>, [ed, de])<br><br>    <span class="hljs-comment"># 使用opencv提供的函数</span><br>    close_img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)<br>    open_img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)<br>    imshow(<span class="hljs-string">&quot;close-open&quot;</span>, [close_img, open_img])<br></code></pre></td></tr></table></figure>

<p>两次打印的图是一样的</p>
<p>为什么叫close操作? 因为它允许close the holes inside the image</p>
<p>开运算属于形态学图像处理，先腐蚀后膨胀。作用：可以使边界平滑，消除细小的尖刺，断开窄小的连接,保持面积大小不变</p>
<ul>
<li>开运算数学上是先腐蚀后膨胀的结果，开运算的结果为完全删除了不能包含结构元素的对象区域，平滑了对象的轮廓，断开了狭窄的连接，去掉了细小的突出部分。</li>
<li>闭运算在数学上是先膨胀再腐蚀的结果，闭运算的结果也是会平滑对象的轮廓，但是与开运算不同的是，闭运算一般会将狭窄的缺口连接起来形成细长的弯口，并填充比结构元素小的洞。</li>
</ul>
<br>

<p>举个例子</p>
<p>这里有三张图</p>
<p><img src="/Blog/Blog/intro/com_graphic/j.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/dotj.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/dotinj.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p>第二张图和第三张图有一些噪音, 如何去除它们.</p>
<br>

<p>我们可以把白色看作是”峰”, 黑色看作是”洞”, 白色是1, 黑色是0, 把图像想象成一个function</p>
<p>对于第二张图, 噪音再图像外部, 我们可以先进行腐蚀, 将这些小的”峰”腐蚀掉, 再用膨胀来复原</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dotj = cv2.imread(<span class="hljs-string">&quot;dotj.png&quot;</span>)<br>    result = cv2.morphologyEx(dotj, cv2.MORPH_OPEN, np.ones((<span class="hljs-number">7</span>, <span class="hljs-number">7</span>)))<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/compare_dotj.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>对于第三张图, 噪音是在图像内部, 我们可以先用膨胀去除这些”洞”, 再用腐蚀来将他还原, 即使用闭运算来关闭这些洞</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dotinj = cv2.imread(<span class="hljs-string">&quot;dotinj.png&quot;</span>)<br>    result = cv2.morphologyEx(dotinj, cv2.MORPH_CLOSE, np.ones((<span class="hljs-number">7</span>, <span class="hljs-number">7</span>)))<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/compare_dotinj.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="形态学梯度"><a href="#形态学梯度" class="headerlink" title="形态学梯度"></a>形态学梯度</h3><p>$$<br>\text{梯度&#x3D;原图-腐蚀}<br>$$</p>
<p>和图像本身的梯度没有关系</p>
<p>腐蚀会让图片内容缩小一圈, 通过原图减去腐蚀得到的就是图像的轮廓(边缘)</p>
<p><img src="/Blog/Blog/intro/com_graphic/gradient.png" srcset="/Blog/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    j = cv2.imread(<span class="hljs-string">&quot;j.png&quot;</span>)<br>    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>    gradient = cv2.morphologyEx(j, cv2.MORPH_GRADIENT, kernel)<br><br>    <span class="hljs-comment"># gradient = j - cv2.morphologyEx(j, cv2.MORPH_ERODE, kernel)</span><br>    imwrite(<span class="hljs-string">&quot;gradient.png&quot;</span>, [j, gradient])<br></code></pre></td></tr></table></figure>

<br>

<h3 id="顶帽运算"><a href="#顶帽运算" class="headerlink" title="顶帽运算"></a>顶帽运算</h3><p>$$<br>\text{顶帽&#x3D;原图-开运算}<br>$$</p>
<p>开运算是先腐蚀再膨胀, 可以消除掉图像外面的噪点(“峰”)</p>
<p>原图减去开运算的结果得到的就是这些噪点</p>
<p><img src="/Blog/Blog/intro/com_graphic/dotj.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/tophat.png" srcset="/Blog/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dotj = cv2.imread(<span class="hljs-string">&quot;dotj.png&quot;</span>)<br>    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))<br>    tophat = cv2.morphologyEx(dotj, cv2.MORPH_TOPHAT, kernel)<br></code></pre></td></tr></table></figure>

<br>

<h3 id="黑帽运算"><a href="#黑帽运算" class="headerlink" title="黑帽运算"></a>黑帽运算</h3><p>$$<br>\text{黑帽&#x3D;原图-闭运算}<br>$$</p>
<p><img src="/Blog/Blog/intro/com_graphic/dotinj.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/blackhat.png" srcset="/Blog/img/loading.gif" lazyload></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dotinj = cv2<span class="hljs-selector-class">.imread</span>(<span class="hljs-string">&quot;dotinj.png&quot;</span>)<br>    kernel = cv2<span class="hljs-selector-class">.getStructuringElement</span>(cv2<span class="hljs-selector-class">.MORPH_RECT</span>, (<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))<br>    blackhat = cv2<span class="hljs-selector-class">.morphologyEx</span>(dotinj, cv2<span class="hljs-selector-class">.MORPH_BLACKHAT</span>, kernel)<br>    <span class="hljs-built_in">imwrite</span>(<span class="hljs-string">&quot;blackhat.png&quot;</span>, blackhat)<br></code></pre></td></tr></table></figure>







<br>

<h2 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h2><h3 id="索伯算子-Sobel-operator"><a href="#索伯算子-Sobel-operator" class="headerlink" title="索伯算子 Sobel operator"></a>索伯算子 Sobel operator</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/diandian.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br><br>    <span class="hljs-comment"># 索伯算子</span><br>    sob = np.array([[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                    [-<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],<br>                    [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])<br><br>    gradx = cv2.filter2D(img, -<span class="hljs-number">1</span>, sob)<br>    grady = cv2.filter2D(img, -<span class="hljs-number">1</span>, sob.T)<br><br>    imshow(<span class="hljs-string">&quot;grad&quot;</span>, [gradx, grady])<br></code></pre></td></tr></table></figure>

<p>第一次用索伯算子检测的是x轴的边缘, 第二次用索伯算子的转置检测的是y轴的边缘.</p>
<p><img src="/Blog/Blog/intro/com_graphic/diandian.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/sob.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>左边的图沿着x轴检测边(检测到竖向的边), 右边的图沿y轴检测边(检测到水平的边)</p>
<p>一个很显然的疑惑是: 为什么这个核能做到这个事情.</p>
<blockquote>
<p>我给出我个人的理解:</p>
<p>这个核是沿中心”对称”的, 例如[-2, 0, 2]. 当进行filtering时, 如果两边的差距很小, 那么sum的结果就很小</p>
<p>例如[3, 4, 3] * [-2, 0, 2] &#x3D; -6+6 &#x3D; 0, 此时这个点就不会被考虑是一条边.</p>
<p>而当两边差距较大时, [2, 5, 10] * [-2, 0, 2] &#x3D; 16, 计算的值就越大, 就会被考率作为一条边</p>
<p>而竖直方向是 不作判断的(都是0), 因此如果向进行竖直的边缘检测, 需要将这个矩阵转置一下. 这样就不考虑水平方向的边了</p>
<p>同理, 可以检测处于对角线上的边</p>
</blockquote>
<br>

<br>

<p>Sobel算子既可以检测x轴上的梯度, 也可以检测y轴上的梯度.</p>
<p>然而通常我们是不关注边是哪个轴的. 我们只关注是否是一条边</p>
<p>我们可以把 gradient of image 想象成是一个向量, 有两个元素分别代表x方向的gradient, y方向的gradient</p>
<p>通过计算它的模来看梯度的大小</p>
<p><img src="/Blog/Blog/intro/com_graphic/grad.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/diandian.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br><br>    <span class="hljs-comment"># 索伯算子</span><br>    sob = np.array([[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                    [-<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],<br>                    [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])<br><br>    gradx = np.<span class="hljs-built_in">abs</span>(cv2.filter2D(img, -<span class="hljs-number">1</span>, sob))<br>    grady = np.<span class="hljs-built_in">abs</span>(cv2.filter2D(img, -<span class="hljs-number">1</span>, sob.T))<br>    gradx = im2double(gradx)<br>    grady = im2double(grady)<br>    grad = np.sqrt(gradx ** <span class="hljs-number">2</span> + grady ** <span class="hljs-number">2</span>)	<span class="hljs-comment"># 如果不转double会溢出</span><br><br>    imshow(<span class="hljs-string">&quot;grad&quot;</span>, [[img, grad], [gradx, grady]])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/compare_grad.jpg" srcset="/Blog/img/loading.gif" lazyload></p>
<p>对应如下:</p>
<table>
<thead>
<tr>
<th align="center">原图</th>
<th align="center">x-y轴检测图</th>
</tr>
</thead>
<tbody><tr>
<td align="center">x轴检测图</td>
<td align="center">y轴检测图</td>
</tr>
</tbody></table>
<p><img src="/Blog/Blog/intro/com_graphic/soble.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>然而我们依然可以看到一些细小的噪音</p>
<p><img src="/Blog/Blog/intro/com_graphic/grad_noise.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p>此时, 我们就可以用高斯核进行降噪</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/diandian.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br><br>    gausk = cv2.getGaussianKernel(<span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br>    tmp = cv2.filter2D(img, -<span class="hljs-number">1</span>, gausk)<br>    smoothed = cv2.filter2D(tmp, -<span class="hljs-number">1</span>, gausk.T)<br><br>    <span class="hljs-comment"># 索伯算子</span><br>    sob = np.array([[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                    [-<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],<br>                    [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])<br><br>    <span class="hljs-comment"># 求不使用高斯核的梯度</span><br>    gradx = cv2.filter2D(img, -<span class="hljs-number">1</span>, sob)<br>    grady = cv2.filter2D(img, -<span class="hljs-number">1</span>, sob.T)<br>    <span class="hljs-comment"># 因为filter2D后的结果是0-255, 这里一定要转换一下, 防止下一句溢出</span><br>    gradx, grady = im2double(gradx), im2double(grady)<br>    grad = np.sqrt(gradx ** <span class="hljs-number">2</span> + grady ** <span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># 求使用高斯核的梯度, 同理在filter2D后要转换一下</span><br>    smoothed_grad = np.sqrt(im2double(cv2.filter2D(smoothed, -<span class="hljs-number">1</span>, sob))**<span class="hljs-number">2</span> +<br>                            im2double(cv2.filter2D(smoothed, -<span class="hljs-number">1</span>, sob.T))**<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># 剪裁</span><br>    cropped_grad = grad[<span class="hljs-number">100</span>:<span class="hljs-number">300</span>, <span class="hljs-number">300</span>:<span class="hljs-number">450</span>]<br>    cropped_smoothed = smoothed_grad[<span class="hljs-number">100</span>:<span class="hljs-number">300</span>, <span class="hljs-number">300</span>:<span class="hljs-number">450</span>]<br>    imshow(<span class="hljs-string">&quot;compare&quot;</span>, [cropped_grad, cropped_smoothed])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/de_grad_noise.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p>使用高斯核先进行降噪, 再用检测边的效果是比较好的.</p>
<br>

<p>然而正如之前卷积所描述的. 我们可以先让两个kernel先进行filtering再对图形进行filtering.</p>
<blockquote>
<p>转载于 <a target="_blank" rel="noopener" href="https://www.itdaan.com/tw/82bd90b53df938d528946299317b6bee">https://www.itdaan.com/tw/82bd90b53df938d528946299317b6bee</a></p>
<h3 id="MATLAB-filter2-和-conv2-函数说明"><a href="#MATLAB-filter2-和-conv2-函数说明" class="headerlink" title="MATLAB filter2 和 conv2 函数说明"></a>MATLAB filter2 和 conv2 函数说明</h3><p>　　在 MATLAB 中，filter2 函数实现二维数字滤波器．conv2 函数实现二维卷积．</p>
<p>　　filter2(H, X, mode) 等价于 conv2(X, rot90(H,2), mode). 其中，H 表示有理传递函数的系数(Coefficients of rational transfer function)，即常说的滤波器核或卷积核．X 表示输入数据．mode 表示卷积或滤波所采用的模式，其决定了返回数据的长度，以及结果数据的不同部分．共有三种模式：’full’, ‘same’, ‘valid’</p>
<p>　　注：MATLAB 中，B &#x3D; rot90(A, k) 将数组 A 逆时针旋转 k*90 度，其中 k 为整数．</p>
<h3 id="Python-中的等价函数"><a href="#Python-中的等价函数" class="headerlink" title="Python 中的等价函数"></a>Python 中的等价函数</h3><p>　　在 Python 中，MATLAB filter2 的等价函数是 scipy.signal.correlate2d；MATLAB conv2 的等价函数是 scipy.signal.convolve2d．</p>
<p>　　具体来说，filter2(H, X, mode) 等价于 scipy.signal.correlate2d(X, H, mode)；conv2(X, H, mode) 等价于 scipy.signal.convolve2d(X, H, mode)．</p>
<p>　　注意，filter2(MATLAB) 与 scipy.signal.correlate2d的输入参数 H, X 的顺序是不一致的．</p>
<p>　　如果二维滤波或卷积结果采用常用的 ‘same’ 模式，还可以使用 Python 中的 cv2.filter2D (OpenCV模块), scipy.ndimage.correlate 和 scipy.ndimage.convolve 替代．对于二维滤波（’same’模式）的具体用法为</p>
<p>　　cv2.filter2D(X, -1, H, borderType&#x3D;cv2.BORDER_CONSTANT)<br>　　scipy.ndimage.correlate(X, H, mode&#x3D;’constant’, cval&#x3D;0.0)<br>　　scipy.ndimage.convolve(X, np.rot90(H,2), mode&#x3D;’constant’, cval&#x3D;0.0)</p>
</blockquote>
<br>

<blockquote>
<p>转载于 <a target="_blank" rel="noopener" href="https://blog.csdn.net/u010830004/article/details/78627031">https://blog.csdn.net/u010830004/article/details/78627031</a></p>
<p> matlab中的conv2是用于对二维数据进行卷积运算，有三个参数可供选择</p>
<p>full模式(默认)</p>
<p><img src="/Blog/Blog/intro/com_graphic/full.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>same模式</p>
<p><img src="/Blog/Blog/intro/com_graphic/same.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>valid模式</p>
<p><img src="/Blog/Blog/intro/com_graphic/valid.png" srcset="/Blog/img/loading.gif" lazyload></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/diandian.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br><br>    gausk = cv2.getGaussianKernel(<span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br>    gausk = gausk.dot(gausk.T)<br><br>    <span class="hljs-comment"># 索伯算子</span><br>    sob = np.array([[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                    [-<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],<br>                    [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])<br><br>    new_filter_1 = convolve2d(gausk, sob)<br>    res_1 = cv2.filter2D(img, -<span class="hljs-number">1</span>, new_filter_1)<br>    <br>    new_filter_2 = convolve2d(gausk, sob.T)<br>    res_2 = cv2.filter2D(img, -<span class="hljs-number">1</span>, new_filter_2)<br><br>    res = np.sqrt(im2double(res_1)**<span class="hljs-number">2</span> + im2double(res_2)**<span class="hljs-number">2</span>)<br><br>    imshow(<span class="hljs-string">&quot;filter&quot;</span>, res)<br></code></pre></td></tr></table></figure>

<br>

<p><img src="/Blog/Blog/intro/com_graphic/smooth_gradient.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p>然而实际上在这个例子中是有点鸡肋的. 这是因为我们的索伯算子要运行两次, 分别对x轴和y轴, 之后取模. 而取模不是卷积运算.</p>
<p>只有两张图都求出来后才进行求模</p>
<br>

<p>可视化一下高斯核和索伯算子的结果</p>
<p>下面是一个50x50, sigma&#x3D;5的高斯核和3x3的索伯算子的运算结果, 这个叫做<strong>derivative-of-Gaussian (DoG) kernel</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/gauss_sob.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Canny-Edge-Detection"><a href="#Canny-Edge-Detection" class="headerlink" title="Canny Edge Detection"></a>Canny Edge Detection</h3><p>Canny 边缘检测有许多策略从一个gradient image中准确地连接edge pixel</p>
<p>首先, 使用它同样用高斯核进行filtering, 之后再用一个gradient kernel. 到此为止都和上面的步骤一样.</p>
<p>之后, Canny 边缘检测会进行Non Maximum Suppression(NMS 非极大值抑制算法)</p>
<br>

<p>假设我们得到的gradient image是这样</p>
<p><img src="/Blog/Blog/intro/com_graphic/canny_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>对于其中的每一个像素, 我们要判断它的梯度的方向(通过索伯算子的公式)</p>
<p><img src="/Blog/Blog/intro/com_graphic/canny_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>得到一个类似这样的图</p>
<p><img src="/Blog/Blog/intro/com_graphic/canny_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>首先要进行NMS(非极大值抑制), 换句话说, 我们只关注local max的像素点(通过对比相邻的像素点判断local max), 其余点都被判定为<strong>不是边</strong>, 然而即使是local max也有不同</p>
<p><img src="/Blog/Blog/intro/com_graphic/localmax.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>此时我们要设置两个阈值, 一个用来判断一个像素(local max)一定是一条边, 另一个用来判断一个像素可能是一条边</p>
<p><img src="/Blog/Blog/intro/com_graphic/canny_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>此时得到了一个图, 标记着每一个像素点的属性(一定是一条边, 可能是一条边, 不是边)</p>
<p><img src="/Blog/Blog/intro/com_graphic/canny_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果一个像素可能是一条边, 同时它周围有像素<strong>一定是边</strong>. 那么就把他们连接起来(把可能是边的点判定为边)</p>
<p><img src="/Blog/Blog/intro/com_graphic/canny_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs haxe"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<span class="hljs-type"></span><br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br>	<br>	<span class="hljs-meta"># 100 和 200 分别是两个 threshold</span><br>    <span class="hljs-keyword">new</span><span class="hljs-type">_img</span> = cv2.Canny(img, <span class="hljs-number">100</span>, <span class="hljs-number">200</span>)<br>    imshow(<span class="hljs-string">&quot;canny&quot;</span>, [img, <span class="hljs-keyword">new</span><span class="hljs-type">_img</span>])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/canny_card.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h3 id="拉普拉斯operator"><a href="#拉普拉斯operator" class="headerlink" title="拉普拉斯operator"></a>拉普拉斯operator</h3><p>使用拉普拉斯operator, filtering一次就可以得到所有gradient和对应的direction</p>
<p><img src="/Blog/Blog/intro/com_graphic/lapop.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>拉普拉斯operator能检测 zero crossing 和 derivative of the image </p>
<br>

<p>如果只检测first derivative, 我们就需要检测derivative is high or not. 就像Canny Edge Detection 中做的那样. 之后还有做 Non Maximum Suppression(NMS 非极大值抑制算法) 去除模糊的edge</p>
<p>但让我们关注于 zero crossing, 我们可以准确地找到边应该出现的地方</p>
<p>这就是为什么要使用拉普拉斯operator</p>
<br>

<p>但是这也会产生很多noise edges, 甚至比使用 threshold 更 noise</p>
<p>因为我们标记了second derivative中<strong>所有的zero crossing</strong> </p>
<p>所以当<strong>直接</strong>使用拉普拉斯operator时, 即使是图片中微小的变化, 也会创造出一些edge information</p>
<br>

<p>所以和canny edge detection 一样, 我们先用gauss filtering一次, 之后再用拉普拉斯operator检测边</p>
<p>这是我们又可以现用拉普拉斯先filtering高斯核, 得到一个新的filter, 在对image进行filtering 来加快速度.</p>
<br>

<p>首先先看一下直接使用拉普拉斯核的结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br>    <br>    lapkern = np.array([[<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>                        [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, -<span class="hljs-number">1</span>],<br>                        [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])<br>    res = cv2.filter2D(img, -<span class="hljs-number">1</span>, lapkern)<br>    imshow(<span class="hljs-string">&quot;lap&quot;</span>, [img, res])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/lap.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p>可以看到它很很多噪音</p>
<p><img src="/Blog/Blog/intro/com_graphic/lap_noise.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<p>在使用拉普拉斯核过滤高斯核之前, 先看一下它长什么样子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lapkern = np.array([[<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>                    [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, -<span class="hljs-number">1</span>],<br>                    [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure>

<p>这是一个100x100, sigma&#x3D;10的高斯核和拉普拉斯核进行卷积的结果</p>
<p><img src="/Blog/Blog/intro/com_graphic/gauss-laplas.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>这是5x5, sigma&#x3D;1的高斯核和拉普拉斯核卷积的结果</p>
<p><img src="/Blog/Blog/intro/com_graphic/gauss-laplas_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br>    <br>    lapkern = np.array([[<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>                        [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, -<span class="hljs-number">1</span>],<br>                        [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])<br>    gauss = cv2.getGaussianKernel(<span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br><br>    kernel = convolve2d(lapkern, gauss)<br>    kernel = convolve2d(kernel, gauss.T)<br><br>    img_filtered = cv2.filter2D(img, -<span class="hljs-number">1</span>, kernel)<br>    <span class="hljs-comment"># 由于太暗了, 所以放大5倍</span><br>    imshow(<span class="hljs-string">&quot;gauss-lap&quot;</span>, [img, <span class="hljs-number">5</span>*img_filtered])<br>    imwrite(<span class="hljs-string">&quot;gauss-lap.png&quot;</span>, [img, <span class="hljs-number">5</span>*img_filtered])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/gauss-lap.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p>和直接使用拉普拉斯相比效果好了很多</p>
<p><img src="/Blog/Blog/intro/com_graphic/lap.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<br>

<h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><p>计算机视觉(Computer Vision)的很多问题都可以转换成一个函数的定义:</p>
<p>我们希望找到一个function, 输入一个图形, 返回我们需要的结果<br>$$<br>F(image) &#x3D; result<br>$$<br>比如说:</p>
<p>把一个图片传入function中, 返回是猫还是狗</p>
<p>或者传入一个图像, 返回它的 alpha map</p>
<br>

<p>这是一个非常复杂的function, 它接收一个非常大的input(图形的大小). 将这么多的数据转化成一个更高层次的输出</p>
<p>我们将创建一系列小的function, 而不是创建一个这么”大”的function</p>
<p>$$<br>F(image) &#x3D; f_1(f_2(f_3…(image)))<br>$$<br>我们希望这一系列小的function能计算出这个大F function的结果</p>
<br>

<p>这些小的函数都有哪些可能</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>首先肯定要有一些卷积核对图片进行过滤, 然而我们不应该手动指定核中的元素, 而应该交给神经网络自己生成</p>
<p>如果我们有很多卷积核, 这就是<strong>卷积神经网络</strong>, 学习每个 kernel 的 weight.</p>
<br>

<p>Convolution 的操作和 Cross-Correlation 的操作非常相似</p>
<p>如果kernel match image very well, 那么它就会给我们一个high response. 被称作Activation Map</p>
<br>

<h3 id="relu函数"><a href="#relu函数" class="headerlink" title="relu函数"></a>relu函数</h3><p>relu是非线性函数</p>
<p><img src="/Blog/Blog/intro/com_graphic/relu.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>$$<br>relu(x) &#x3D; max(0, x)<br>$$<br>其实也相当于一个过滤器, 过滤掉小于0的值</p>
<p>它认为小于0的值没有存储我们想要的一些信息. 而大于0的值存储了某些我们想要的信息</p>
<br>

<h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h3><p>池化的本质就是采样</p>
<p>max pool 就是在一个范围内, 将最大值作为代表进行输出</p>
<p><img src="/Blog/Blog/intro/com_graphic/max_pool.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>Pooling 对于输入的 Feature Map，选择某种方式对其进行降维压缩，以加快运算速度</p>
<br>

<p>之后我们就会得到一个结果</p>
<p><img src="/Blog/Blog/intro/com_graphic/deep_l1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Un-pooling"><a href="#Un-pooling" class="headerlink" title="Un-pooling"></a>Un-pooling</h3><p>un-pooling 做的事情和 pooling 相反, 将一个较小的image转化为较大的image</p>
<br>

<h3 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h3><p>Deconvolution is <strong>not</strong> the opposite of the convolution.</p>
<p>它是卷积的一种, 可以让image变得大一点</p>
<br>

<br>

<p><img src="/Blog/Blog/intro/com_graphic/deep_l2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>得到的小的图像会肯定会丢失一些信息</p>
<p>然而有一种技术可以解决这个问题, skip connections</p>
<p><img src="/Blog/Blog/intro/com_graphic/skip_connections.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>就是说每一层不仅接收前一层的输出, 还接收更前一层(多层)的某些输出</p>
<p>数据跳过了很多layer, 所以叫skip connection.</p>
<p>这个技术可以使得output生成的图像更具有细节</p>
<br>

<p>由于是监督学习, 可以对比和期望的不同. 来用梯度下降进行反向传播</p>
<p>可以参考: <a href="https://daolinzhou.github.io/Blog/2020/05/30/MLP/">多层感知器MLP</a></p>
<br>

<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>通常我们没有特别多的数据, 然而我们可以通过处理图像得到新的图形</p>
<p>例如: 如果我们期望的输出和图片的颜色无关, 那么我们可以将原图进行几次变色生成新的图, 而新的图的期望输出应该和原图一样. 这样机器经过一些训练之后就应该知道颜色与结果无关</p>
<p>或者做一些旋转, 平移. 等等操作.</p>
<br>

<h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><p>思路就是与其只有一个network. 我们可以创造两个network.</p>
<p>一个是generator, 一个是discriminator.</p>
<p>generator 和普通的神经网络一样. 负责一个任务. 例如区分猫狗, 找到alpha map.</p>
<p>discriminator 则是接收generator的output. 然后猜测这个结果是generator生成的还是一个我们期望的真实的结果图</p>
<br>

<br>

<h2 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h2><p>傅里叶级数 的思想是将任何一个<strong>周期性函数</strong>, 用三角函数表达(sin cos)(逼近)</p>
<p>具体内容日后在写, 先列出几个我学习的资源</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1A4411Y7vj">李永乐的傅里叶变换的基础讲解</a></p>
<ul>
<li>注意 15.04 时, 说虚部表示相位的说法是错的, 而是幅角表示相位. 复数有两个表示方法, 一个是用虚部加实部, 一个是用长度和角度. 这里的角度才表示的是相位, 长度才是频率</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41455378">傅里叶级数推导</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41875010">傅里叶变换的推导</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75521342">离散傅里叶变换 (DFT)</a></p>
<ul>
<li>最后的矩阵乘法写反了</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pW411J7s8">形象展示傅里叶变换</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1vt411N7Ti">傅立叶级数</a></p>
<ul>
<li>15:54 解释了傅里叶变换后得到的那组复数是什么意义</li>
<li>18:17 寻找傅里叶级数. 傅里叶变换</li>
<li>21:19 求cn的一种方法, 或者说是进行傅里叶变换的一种方法</li>
<li>21:41 应用傅里叶变换</li>
<li>23:44 其他一些学习傅里叶变换的资源</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1G4411D7kZ">欧拉公式 e^it 为什么代表圆周运动</a></p>
</blockquote>
<br>

<p>为什么用离散傅里叶变换?</p>
<ol>
<li><p>然而实际中我们通常得不到原函数 f(t), 而是只能得到一些样本.</p>
</li>
<li><p>即使有原函数, 计算机也不容易进行积分.</p>
</li>
</ol>
<br>

<p>离散傅里叶变换<br>$$<br>F(n)&#x3D;\sum_{t&#x3D;0}^Nf(t)e^{-i\frac {2\pi n}Nt}<br>$$</p>
<p>离散傅里叶逆变换<br>$$<br>f(t)&#x3D;\sum_{n&#x3D;0}^N[\frac 1N\cdot F(n)\cdot e^{i\frac {2\pi n}N t}]<br>$$<br><br></p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_0.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我把傅里叶变换的实现写在下面了</p>
<br>

<br>

<h2 id="信号-Signals-与图片"><a href="#信号-Signals-与图片" class="headerlink" title="信号(Signals)与图片"></a>信号(Signals)与图片</h2><p>frequency domain 频谱</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_1.gif" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>2d傅里叶变换</p>
<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/2dfft.mp4" type="video/mp4">
</video>

<p>x轴和y轴一同工作形成这个图像</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>分别读取两张图, 一个高频图, 一个低频图</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-selector-tag">img</span> = cv2<span class="hljs-selector-class">.imread</span>(<span class="hljs-string">&quot;../resources/sfu3.jpg&quot;</span>)<br>    <span class="hljs-selector-tag">img</span> = cv2<span class="hljs-selector-class">.cvtColor</span>(<span class="hljs-selector-tag">img</span>, cv2.COLOR_BGR2GRAY)<br>    <span class="hljs-selector-tag">img</span> = <span class="hljs-built_in">im2double</span>(img)<br><br>    f = np<span class="hljs-selector-class">.fft</span><span class="hljs-selector-class">.fft2</span>(img)<br>    fou = np<span class="hljs-selector-class">.fft</span><span class="hljs-selector-class">.fftshift</span>(np<span class="hljs-selector-class">.abs</span>(f))<br>    fou = fou<br><br>    img2 = cv2<span class="hljs-selector-class">.imread</span>(<span class="hljs-string">&quot;../resources/sfu4.jpg&quot;</span>)<br>    img2 = cv2<span class="hljs-selector-class">.resize</span>(img2, <span class="hljs-selector-tag">img</span><span class="hljs-selector-class">.shape</span><span class="hljs-selector-attr">[::-1]</span>)<br>    img2 = cv2<span class="hljs-selector-class">.cvtColor</span>(img2, cv2.COLOR_BGR2GRAY)<br>    img2 = <span class="hljs-built_in">im2double</span>(img2)<br><br>    f2 = np<span class="hljs-selector-class">.fft</span><span class="hljs-selector-class">.fft2</span>(img2)<br>    fou2 = np<span class="hljs-selector-class">.fft</span><span class="hljs-selector-class">.fftshift</span>(np<span class="hljs-selector-class">.abs</span>(f2))<br>    fou2 = fou2<br><br>    <span class="hljs-built_in">imwrite</span>(<span class="hljs-string">&quot;sfu.png&quot;</span>, <span class="hljs-selector-attr">[[img, fou]</span>,  <span class="hljs-selector-attr">[img2, fou2]</span>])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/sfu.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>上面的那张是高频图, 水波是图片中高频率的部分</p>
<p>下面那张是低频图</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/fft_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>分别是原图, 频谱(振幅)图, 相位(偏移)图</p>
<p>Phase image 对人来说难以理解. 但是 Amplitude image 不一样</p>
<p>如果有很多edges. 就说明是 high frequency. </p>
<p>如果只有平面, 或者constant color, 就说明是 low frequency</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/fft_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Convolution-Theorem"><a href="#Convolution-Theorem" class="headerlink" title="Convolution Theorem"></a>Convolution Theorem</h3><p>The Fourier transform of the convolution of two functions is the product of their Fourier transforms.</p>
<p>对两个函数卷积的傅里叶变换等于先对两个函数进行傅里叶变换, 再相乘<br>$$<br>F[g\star h] &#x3D; F[g]F[h]<br>$$<br>The inverse Fourier transform of the product of two Fourier transform is the convolution of the two inverse Fourier transform</p>
<p>对两个函数的乘积进行傅里叶逆变换等于对两个函数分别进行傅里叶逆变换后再卷积<br>$$<br>F^{-1}[gh]&#x3D;F^{-1}[g]\star F^{-1}[h]<br>$$<br><strong>Convolution</strong> in spatial domain is equivalent to <strong>Multiplication</strong> in frequency domain</p>
<p><strong>Multiplication</strong>  in spatial domain is equivalent to <strong>Convolution</strong> in frequency domain</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/fft_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>再 spatial domain 对图像进行卷积意味着filtering(过滤).</p>
<p>意味着我们可以对kernel和图像进行傅里叶变换, 再相乘, 逆变换. 我们依然可以得到filtering 的结果</p>
<p>这也是我们如何区分各种类别的filter</p>
<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    gaus = cv2.getGaussianKernel(<span class="hljs-number">10</span>, <span class="hljs-number">1</span>)<br>    gaus = gaus.dot(gaus.T)<br><br>    box = np.ones((<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>    box = box / np.<span class="hljs-built_in">sum</span>(box)<br><br>    <span class="hljs-comment"># 指定结果图的大小为(622, 622)</span><br>    fft_gaus = np.fft.fftshift(np.<span class="hljs-built_in">abs</span>(np.fft.fft2(gaus, (<span class="hljs-number">622</span>, <span class="hljs-number">622</span>))))<br>    fft_box = np.fft.fftshift(np.<span class="hljs-built_in">abs</span>(np.fft.fft2(box, (<span class="hljs-number">622</span>, <span class="hljs-number">622</span>))))<br><br>    imshow(<span class="hljs-string">&quot;kernel&quot;</span>, [fft_gaus*<span class="hljs-number">20</span>, fft_box*<span class="hljs-number">20</span>])<br>    imwrite(<span class="hljs-string">&quot;fft_kernel.png&quot;</span>, [fft_gaus*<span class="hljs-number">5</span>, fft_box*<span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/fft_kernel.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>可以看到 gauss kernel 经过傅里叶变换后是另一个 gauss kernel. 而box filter经过傅里叶变换后是 sinc 函数</p>
<p>这意味这当用高斯核进行filtering时, 只会保留 low frequency. 而使用box kernel进行filtering时, 则会依然保留一些 high frequency details</p>
<br>

<br>

<p>测试卷积和傅里叶变换的关系. 由于输出的图片都是一样的, 所以</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br><br>    gaus = cv2.getGaussianKernel(<span class="hljs-number">10</span>, <span class="hljs-number">1</span>)<br>    gaus = gaus.dot(gaus.T)<br><br>    <span class="hljs-comment"># 傅里叶变换后再相乘</span><br>    fft_gaus = np.fft.fft2(gaus, img.shape)<br>    fft_img = np.fft.fft2(img)<br>    res = np.real(np.fft.ifft2(fft_img * fft_gaus)).astype(np.uint8)<br><br>    hf_gaus = cv2.filter2D(img, -<span class="hljs-number">1</span>, gaus).astype(np.uint8)<br><br>    imshow(<span class="hljs-string">&quot;img.png&quot;</span>, [res, hf_gaus])<br></code></pre></td></tr></table></figure>

<br>

<p>这个叫做low pass filter. 我们也可以通过用一个不改变原图的kernel减去gauss filter得到high pass filter</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/fft_5.png" srcset="/Blog/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    gaus = cv2.getGaussianKernel(<span class="hljs-number">11</span>, <span class="hljs-number">1</span>)<br>    gaus = gaus.dot(gaus.T)<br><br>    onefilt = np.zeros(gaus.shape)<br>    onefilt[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>] = <span class="hljs-number">1</span><br><br>    high_pass_filter = onefilt - gaus<br><br>    <span class="hljs-comment"># 傅里叶变换后再相乘</span><br>    fft_low = np.fft.fft2(gaus, (<span class="hljs-number">622</span>, <span class="hljs-number">622</span>))<br>    fft_high = np.fft.fft2(high_pass_filter, (<span class="hljs-number">622</span>, <span class="hljs-number">622</span>))<br><br>    fft_low = np.fft.fftshift(np.<span class="hljs-built_in">abs</span>(fft_low))<br>    fft_high = np.fft.fftshift(np.<span class="hljs-built_in">abs</span>(fft_high))<br><br>    imwrite(<span class="hljs-string">&quot;img.png&quot;</span>, [fft_high, fft_low])<br></code></pre></td></tr></table></figure>

<p>当进行per pixel multiplication 时</p>
<p>左边的 kernel 只会 pass high frequency component (high pass filter)</p>
<p>右边的 kernel 只会 pass low frequency component (low pass filter)</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/fft_6.png" srcset="/Blog/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/sfu3.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br><br>    <span class="hljs-comment"># low pass filter</span><br>    gaus = cv2.getGaussianKernel(<span class="hljs-number">11</span>, <span class="hljs-number">1</span>)<br>    gaus = gaus.dot(gaus.T)<br><br>    onefilt = np.zeros(gaus.shape)<br>    onefilt[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>] = <span class="hljs-number">1</span><br><br>    high_pass_filter = onefilt - gaus<br><br>    imwrite(<span class="hljs-string">&quot;img.png&quot;</span>, [cv2.filter2D(img, -<span class="hljs-number">1</span>, high_pass_filter)*<span class="hljs-number">5</span>, cv2.filter2D(img, -<span class="hljs-number">1</span>, gaus)])<br></code></pre></td></tr></table></figure>

<blockquote>
<p>high pass filter 有点 edge detection 的感觉了</p>
</blockquote>
<br>

<br>

<h3 id="Sampling-and-Aliasing"><a href="#Sampling-and-Aliasing" class="headerlink" title="Sampling and Aliasing"></a>Sampling and Aliasing</h3><p>下面分别是一个<strong>low frequency</strong> function在spatial domain 和Frequency domain的图</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>然而当我们将function 转化为数字信号. 我们需要做sampling</p>
<p>例如: 照相就是无限的光中做sampling. 函数(光)是连续的, 无限的. 然而相机只能pixel per pixel 显示. 所以只能做sampling</p>
<br>

<p>我们通过脉冲函数(Impulse function)进行Sampling process</p>
<p>Impulse function 经过傅里叶变换后也是一个inpulse </p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<blockquote>
<p>脉冲函数的[a,b]小区间的integral的值是1</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
</blockquote>
<br>

<p>将原img和脉冲函数相乘, 得到的就是sample img</p>
<p>而在 Spatial domain 的乘法等同于在 Frequency domain 的卷积</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>when we sample the signal, we are actually replicating it over and over again, in the frequency domain.</p>
<br>

<br>

<p>然而当我们对high frequency image 进行 sample signals 就有麻烦了</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们会损失很多细节, 同时frequency domain 表示会彼此碰撞(这就叫做aliasing)</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<blockquote>
<p>当我们尝试 sampling a signal with less number of samples than it’s requires. 就会发生aliasing</p>
</blockquote>
<br>

<br>

<p>一种解决变法是减小 spatial domain 中 impulse responses 之间的距离</p>
<p>spatial domain 中 impulse response的距离如果为t的话, frequency domain中的距离就是 1&#x2F;t. 通过减小spatial中的 t, 就可以增大frequency domain 中的 impulse response的距离, 使得在Frequency domain中更不容易重合</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>通过taking more samples, 就可以解决aliasing</p>
<br>

<h3 id="Anti-Aliasing"><a href="#Anti-Aliasing" class="headerlink" title="Anti-Aliasing"></a>Anti-Aliasing</h3><p>然而这不是一个完美的结果.</p>
<p>more samples 意味着 more data, more file size</p>
<p>如果想要一个small image, 我们希望通过某些方法去掉high frequency with low pass filter</p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这种方法叫做 anti-aliasing</p>
<br>

<p>Anti-aliasing 就是在sampling&#x2F;re-sampling a signal之前用一个low pass filter</p>
<p>Minimum sampling rate 叫做 Nyquist Sampling rate</p>
<p>2 * maximum frequency that you have in the image</p>
<p>这就叫做 Nyquist-Shannon Sampling Theorem</p>
<br>

<h3 id="Nyquist-Shannon-Sampling-Theorem"><a href="#Nyquist-Shannon-Sampling-Theorem" class="headerlink" title="Nyquist-Shannon Sampling Theorem"></a>Nyquist-Shannon Sampling Theorem</h3><p>如果我们sample点太少的话, 就容易出现下图的情况. </p>
<p><img src="/Blog/Blog/intro/com_graphic/fft_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">frequency_representation</span>(<span class="hljs-params">img: np.ndarray, path: <span class="hljs-built_in">str</span>, multiplier=<span class="hljs-number">3</span></span>):<br>    res = np.fft.fftshift(np.<span class="hljs-built_in">abs</span>(np.fft.fft2(img)))<br>    cv2.imwrite(path, res*multiplier)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">anti_aliasing</span>(<span class="hljs-params">hp: np.ndarray, lp: np.ndarray, std_2, size_2, std_4, size_4</span>):<br>    hp_subsample_2 = hp[::<span class="hljs-number">2</span>, ::<span class="hljs-number">2</span>]<br>    lp_subsample_2 = lp[::<span class="hljs-number">2</span>, ::<span class="hljs-number">2</span>]<br>    cv2.imwrite(<span class="hljs-string">&quot;Report_files/HP-sub2.png&quot;</span>, hp_subsample_2 * <span class="hljs-number">255</span>)<br>    cv2.imwrite(<span class="hljs-string">&quot;Report_files/LP-sub2.png&quot;</span>, lp_subsample_2 * <span class="hljs-number">255</span>)<br>    frequency_representation(hp_subsample_2, <span class="hljs-string">&quot;Report_files/HP-sub2-freq.png&quot;</span>)<br>    frequency_representation(lp_subsample_2, <span class="hljs-string">&quot;Report_files/LP-sub2-freq.png&quot;</span>)<br><br>    hp_subsample_4 = hp[::<span class="hljs-number">4</span>, ::<span class="hljs-number">4</span>]<br>    lp_subsample_4 = lp[::<span class="hljs-number">4</span>, ::<span class="hljs-number">4</span>]<br>    cv2.imwrite(<span class="hljs-string">&quot;Report_files/HP-sub4.png&quot;</span>, hp_subsample_4 * <span class="hljs-number">255</span>)<br>    cv2.imwrite(<span class="hljs-string">&quot;Report_files/LP-sub4.png&quot;</span>, lp_subsample_4 * <span class="hljs-number">255</span>)<br>    frequency_representation(hp_subsample_4, <span class="hljs-string">&quot;Report_files/HP-sub4-freq.png&quot;</span>)<br>    frequency_representation(lp_subsample_4, <span class="hljs-string">&quot;Report_files/LP-sub4-freq.png&quot;</span>)<br><br>    gauss_ker_2 = cv2.getGaussianKernel(size_2, std_2)<br>    hp_filt_2 = cv2.filter2D(hp, -<span class="hljs-number">1</span>, gauss_ker_2)<br>    hp_filt_2 = cv2.filter2D(hp_filt_2, -<span class="hljs-number">1</span>, gauss_ker_2.T)<br>    hp_subsample_2_aa = hp_filt_2[::<span class="hljs-number">2</span>, ::<span class="hljs-number">2</span>]<br>    cv2.imwrite(<span class="hljs-string">&quot;Report_files/HP-sub2-aa.png&quot;</span>, hp_subsample_2_aa * <span class="hljs-number">255</span>)<br>    frequency_representation(hp_subsample_2_aa, <span class="hljs-string">&quot;Report_files/HP-sub2-aa-freq.png&quot;</span>)<br><br>    gauss_ker_4 = cv2.getGaussianKernel(size_4, std_4)<br>    hp_filt_4 = cv2.filter2D(hp, -<span class="hljs-number">1</span>, gauss_ker_4)<br>    hp_filt_4 = cv2.filter2D(hp_filt_4, -<span class="hljs-number">1</span>, gauss_ker_4.T)<br>    hp_subsample_4_aa = hp_filt_4[::<span class="hljs-number">4</span>, ::<span class="hljs-number">4</span>]<br>    cv2.imwrite(<span class="hljs-string">&quot;Report_files/HP-sub4-aa.png&quot;</span>, hp_subsample_4_aa * <span class="hljs-number">255</span>)<br>    frequency_representation(hp_subsample_4_aa, <span class="hljs-string">&quot;Report_files/HP-sub4-aa-freq.png&quot;</span>)<br><br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    hp = cv2.cvtColor(cv2.imread(<span class="hljs-string">&quot;./resources/hp.jpg&quot;</span>), cv2.COLOR_BGR2GRAY)[-<span class="hljs-number">500</span>:, <span class="hljs-number">200</span>:<span class="hljs-number">700</span>]<br>    lp = cv2.cvtColor(cv2.imread(<span class="hljs-string">&quot;./resources/lp.jpg&quot;</span>), cv2.COLOR_BGR2GRAY)[<span class="hljs-number">200</span>:<span class="hljs-number">700</span>, <span class="hljs-number">900</span>:<span class="hljs-number">1400</span>]<br>    <br>    hp = im2double(hp)<br>    lp = im2double(lp)<br>    <br>    anti_aliasing(hp, lp, std_2=<span class="hljs-number">0.7</span>, size_2=<span class="hljs-number">3</span>, std_4=<span class="hljs-number">1.5</span>, size_4=<span class="hljs-number">7</span>)<br></code></pre></td></tr></table></figure>

<br>

<p>这两张图分别是high pass image 和 low pass image</p>
<p><img src="/Blog/Blog/intro/com_graphic/HP.png" srcset="/Blog/img/loading.gif" lazyload><img src="/Blog/Blog/intro/com_graphic/LP.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>如果缩小高通图得到的结果(步长为2和步长为4), 可以看到台阶出现锯齿</p>
<p><img src="/Blog/Blog/intro/com_graphic/HP-sub2.png" srcset="/Blog/img/loading.gif" lazyload><img src="/Blog/Blog/intro/com_graphic/HP-sub4.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这两张图的频谱图</p>
<p><img src="/Blog/Blog/intro/com_graphic/HP-sub2-freq.png" srcset="/Blog/img/loading.gif" lazyload><img src="/Blog/Blog/intro/com_graphic/HP-sub4-freq.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p> (可以对比下面anti-aliasing后的频谱图, 在中间的那条线旁边有一些光斑. 这些就是频率重叠后出现的, 而且上面这些频谱图整体较亮, 在一些地方都有光点)</p>
<br>

<p>因此需要用高斯核进行anti-aliasing. 就是在剪裁&#x2F;resize之前, 先用高斯核进行一次过滤. (注意要在不过渡模糊图片的情况下进行anti-aliasing)</p>
<p><img src="/Blog/Blog/intro/com_graphic/HP-sub2-aa.png" srcset="/Blog/img/loading.gif" lazyload><img src="/Blog/Blog/intro/com_graphic/HP-sub4-aa.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这两张图的频谱图</p>
<p><img src="/Blog/Blog/intro/com_graphic/HP-sub2-aa-freq.png" srcset="/Blog/img/loading.gif" lazyload><img src="/Blog/Blog/intro/com_graphic/HP-sub4-aa-freq.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>然而 Low Frequency image 却看起来不错. 因为 aliasing还没有开始</p>
<p><img src="/Blog/Blog/intro/com_graphic/LP-sub2.png" srcset="/Blog/img/loading.gif" lazyload><img src="/Blog/Blog/intro/com_graphic/LP-sub4.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/LP-sub2-freq.png" srcset="/Blog/img/loading.gif" lazyload><img src="/Blog/Blog/intro/com_graphic/LP-sub4-freq.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<blockquote>
<p>对比:</p>
<p><img src="/Blog/Blog/intro/com_graphic/HP-sub2-freq.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/HP-sub2-aa-freq.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/HP-sub4-freq.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/HP-sub4-aa-freq.png" srcset="/Blog/img/loading.gif" lazyload></p>
</blockquote>
<br>

<br>

<h2 id="神经网络的Anti-aliasing"><a href="#神经网络的Anti-aliasing" class="headerlink" title="神经网络的Anti-aliasing"></a>神经网络的Anti-aliasing</h2><p><a target="_blank" rel="noopener" href="https://richzhang.github.io/antialiased-cnns/">来自Richard’ paper</a></p>
<p>这个技术实在是太酷了</p>
<p>而在神经网络中, 进行pooling时也会downsizing &#x2F; downsampling</p>
<p>因此神经网络中也是需要Anti-aliasing的</p>
<br>

<p>这两张图可以正确分类</p>
<p><img src="/Blog/Blog/intro/com_graphic/classify.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>但这两张图却不能</p>
<p><img src="/Blog/Blog/intro/com_graphic/classify_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>而这两张图片的唯一不同就是第二张图是第一张图的一个平移</p>
<ul>
<li><strong>Convolutions</strong> are shift-equivariant</li>
<li><strong>Pooling</strong> builds up shift-invariance</li>
</ul>
<p>这是因为 <strong>striding</strong> (跨步) ignores Nyquist sampling theorem and <strong>aliases</strong></p>
<p>striding 可以在 Convolution layer 也可以在 Pooling layer</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/sampling.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>那么如何解决这个问题?</p>
<p>解决方法就是在subsample之前进行一次blur</p>
<p><img src="/Blog/Blog/intro/com_graphic/sampling_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/aliasing.mp4" type="video/mp4">
</video>

<br>

<p>为什么会出现这种情况?</p>
<p><img src="/Blog/Blog/intro/com_graphic/max_pooling.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果我们没两个一组进行max_pooling</p>
<br>

<p>如果进行一个单位的偏移, 结果就会完全不一样</p>
<p><img src="/Blog/Blog/intro/com_graphic/max_pooling_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/max_pooling_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<blockquote>
<p>Max-pooling breaks shift-equivariance</p>
</blockquote>
<br>

<p>这是因为 Max pooling 其实可以拆分为两个步骤</p>
<p><img src="/Blog/Blog/intro/com_graphic/max_pooling_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<ol>
<li>先对范围内像素取最大值</li>
<li>subsampling</li>
</ol>
<p>正是subsampling这步操作使得shift equivariance失效的地方.</p>
<br>

<p>既然知道是什么引起的问题, 我们就可以对它进行改进</p>
<p>在这两部操作中间添加一个blur操作</p>
<p><img src="/Blog/Blog/intro/com_graphic/max_pooling_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/max_pooling_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<blockquote>
<p>讲一个复合操作, 替换为一系列简单的操作, 从中找出问题所在. 这个思想很有趣</p>
</blockquote>
<br>

<p>其他down sampling层的一些替换手段</p>
<p>将原本的 striding 步长减小, 之后再blur pooling</p>
<p><img src="/Blog/Blog/intro/com_graphic/max_pooling_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这样就可以anti-alias any off-the-shelf network</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/max_pooling_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>使用这个技术之后, 自不变性(self-invariance) 和 准确率(accuracy) 都提高了</p>
<br>

<br>

<h2 id="特征检测-Feature-Detection"><a href="#特征检测-Feature-Detection" class="headerlink" title="特征检测(Feature Detection)"></a>特征检测(Feature Detection)</h2><p>进行特征检测的主要目的是match the features between two images</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们的目标是在这种特别困难的情况下进行match</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<h3 id="Harris-Corner-Detection"><a href="#Harris-Corner-Detection" class="headerlink" title="Harris Corner Detection"></a>Harris Corner Detection</h3><p>一个非常重要的事情是, 我们应该能pin point their location</p>
<p>在两张图都找到对应特征的位置, 之后尝试找到 mapping between those locations</p>
<br>

<h4 id="Type-of-Feature-we-can-locate"><a href="#Type-of-Feature-we-can-locate" class="headerlink" title="Type of Feature we can locate"></a>Type of Feature we can locate</h4><p>如果我们看正方形中间的pixel</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们知道这个点有用, 但是我们无法定位它的位置</p>
<p>如果我们将正方形窗口左移或右移, it will create a change. 因此我们可以明白水平方向这条边的位置</p>
<p>但如果上移或下移, feature not change. 所以我们无法确定竖直方向的位置.</p>
<br>

<p>为了确定点的位置, 我们可以看边角(Corner)</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这个Corner 我们能确定位置, 如果移动这个window, we will create lots of differences in all directions.</p>
<br>

<blockquote>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>第一张图: 在中间区域, 非边非角, 移动window将不会产生太大变化</p>
<p>第二张图: 存在某一方向的一条边, 当移动时, 水平方向会产生不同</p>
<p>第三张图: 在边角处, 如果移动, 在所有方向都会产生不同.</p>
</blockquote>
<p>我们要找的就是这样的点</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>(shift image - original image)^2</p>
<p>u 是 x 轴的变化$\Delta x$, v 是 y 轴的变化$\Delta y$</p>
<br>

<p>如果 E(u, v) 的值非常大(For both u and v directions), 我们就明白它是一个descriptive point</p>
<p>如果我们想这么计算的话, 那将花费非常多的时间, 因为要对每个像素都找到一个window, 计算一个这样的值</p>
<p>然而我们希望在短时间内快速找到非常多的 features</p>
<br>

<p>首先我们要用 small motion assumption, 我们将只看small window</p>
<p>之后用泰勒展开(泰勒展开用于estimate a function locally)</p>
<p><a href="https://daolinzhou.github.io/Blog/2020/05/31/calculus/#Tangent-Planes-and-Linear-Approximations">Tangent-Planes-and-Linear-Approximations</a></p>
<br>


<p>$$<br>I(x+u, y+v) \approx I(x, y)+I_x(x, y)((x+u)-x)+I_y(x,y)((y+v)-y)<br>$$</p>
<p>$$<br>\sum_{x,y} (I(x+u, y+v)-I(x, y))^2 \approx \sum_{x, y}(I_x(u)+I_y(v))^2<br>$$</p>
<br>

<p>如果写成矩阵的形式就是</p>
<p><img src="/Blog/Blog/intro/com_graphic/mf.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>由于sum之和x, y有关, 所以我们可以把sum写入, 把只含有u, v 的矩阵提取出来.</p>
<br>

<p>定义H</p>
<p><img src="/Blog/Blog/intro/com_graphic/mf_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>之后</p>
<p><img src="/Blog/Blog/intro/com_graphic/mf_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>如果竖直方向(y轴)gradient有值, 水平方向(x轴)gradient的值为0</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>如果水平方向(x轴)gradient有值, 竖直方向(y轴)gradient的值为0</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们可以将H看作是一个椭圆, 两个半径分别由H的两个特征值来描述</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们希望椭圆尽可能小. 换句话说edge尽可能strong</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/feature_d11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="判断一个点是否是Corner"><a href="#判断一个点是否是Corner" class="headerlink" title="判断一个点是否是Corner"></a>判断一个点是否是Corner</h4><p>下面我们就需要确定某一个像素是处于上图的哪个位置</p>
<p>方法1: 使用调和平均值. 只有两个数都大的时候调和平均值才大</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>就是H的行列式除以H对角线的和, 用这个值去判断当前像素处于哪片区域</p>
<br>

<p>方法2</p>
<p>然而对于Harries Corner Detection还有 另一种逼近(测量)方法, 这种方法更常使用<br>$$<br>R&#x3D;\lambda_1 \lambda_2 - k(\lambda_1+\lambda_2)^2<br>$$<br>k是一个非常小的数(通常在 [0.04, 0.06] 之间)<br>$$<br>&#x3D;det(H)-k\cdot tr(H)^2<br>$$</p>
<blockquote>
<p>通过 R 来判断其属于那个区间</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_d_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
</blockquote>
<br>

<p>当得到R(或f)的值, 我们就可以判断一个点是否是Corner了.</p>
<p>和 Canny Edge Detection 类似, 我们要先 threshold it (注意两种方法选取的threshold是不同的), 之后用 Non Maximum Suppression(NMS 非极大值抑制算法) 来找到 corner 的point</p>
<p>我们可以直接使用H, 但当我们看它的neighborhood, 我们知道the effect of the neighbors should be less and less. 如果我们不添加权值, 这就像是用box filter, 但我们也可以用高斯核作为权值</p>
<p><img src="/Blog/Blog/intro/com_graphic/mf_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.cvtColor(cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>), cv2.COLOR_BGR2GRAY)<br>    img = im2double(img)<br>    sobel = np.array([[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                      [-<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],<br>                      [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])<br>    gaus = cv2.getGaussianKernel(<span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br>    gaus = gaus.dot(gaus.T)<br>    dog = convolve2d(gaus, sobel)<br><br>    <span class="hljs-comment"># gradient in x direction, I_x, I_y</span><br>    ix = cv2.filter2D(img, -<span class="hljs-number">1</span>, dog)<br>    iy = cv2.filter2D(img, -<span class="hljs-number">1</span>, dog.T)<br><br>    <span class="hljs-comment"># I_x^2, I_y^2, I_xy</span><br>    <span class="hljs-comment"># 用高斯核进行权重分配</span><br>    ix2g = cv2.filter2D(ix**<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>, gaus)<br>    iy2g = cv2.filter2D(iy**<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>, gaus)<br>    ixiyg = cv2.filter2D(ix*iy, -<span class="hljs-number">1</span>, gaus)<br><br>    <span class="hljs-comment"># 有两中方法判断</span><br>    <span class="hljs-comment"># f = det(H) / tr(H)</span><br>    <span class="hljs-comment"># harcor = (ix2g * iy2g - ixiyg * ixiyg) / (ix2g + iy2g)</span><br>    <br>    <span class="hljs-comment"># R = det(H) - k*tr(H), k = 0.05</span><br>    harcor = ix2g * iy2g - ixiyg * ixiyg - <span class="hljs-number">0.05</span> * ((ix2g + iy2g)**<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># 大于 threshold 的点我们认为可能是 Corner</span><br>    corners = harcor &gt; <span class="hljs-number">0.001</span><br><br>    <span class="hljs-comment"># NMS</span><br>    <span class="hljs-comment"># 膨胀操作 dilate 选择区域中最大值, localmax 中的点就是**周围有最大值**的点</span><br>    localmax = cv2.dilate(harcor, np.ones((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)))<br><br>    img_feature = img.copy()<br>    <span class="hljs-comment"># 如果这个点和原本的点一样, 我们就认为它是local max.(被改变的点周围有比它大的点)</span><br>    <span class="hljs-comment"># 再通过 threshold 过滤</span><br>    <span class="hljs-comment"># img_feature[(harcor == localmax) * corners] = 1</span><br>    <span class="hljs-comment"># 使用numpy的按位与, 应该比使用乘法快</span><br>    img_feature[np.bitwise_and((harcor == localmax), corners)] = <span class="hljs-number">1</span><br>   <br>    <span class="hljs-comment"># imwrite(&quot;corner_detection.PNG&quot;, [img*255, img_feature*255])</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>这里巧妙的使用了膨胀的特性进行非最大值抑制(Canny 不能这么做, 因为Canny需要沿梯度方向进行抑制, 而此处只要是周围的像素就进行抑制) (一个是指向性法术, 一个是范围性法术)</p>
</blockquote>
<p><img src="/Blog/Blog/intro/com_graphic/corner_detection.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>需要仔细看才能从右边的图中看到细小的白点, 这就是Corner, 也可以说是特征.</p>
<br>

<p>总结一下:</p>
<ol>
<li>定义gauss核 和 dog核</li>
<li>使用他们求出图的梯度(x轴gradient, y轴gradient)</li>
<li>使用梯度求 H 矩阵的元素 $I_x^2, I_y^2, I_xI_y$</li>
<li>使用这些元素计算 R 或 f</li>
<li>使用R(或f) 判断一个点是否可能是Corner</li>
<li>对R(或f) 使用NMS找到一个范围内的 local max</li>
<li>结合 5, 6 的结果判断一个点是否是Corner</li>
</ol>
<br>

<br>

<h2 id="特征不变性-特征描述-特征匹配Feature-Invariance-Description-and-Matching"><a href="#特征不变性-特征描述-特征匹配Feature-Invariance-Description-and-Matching" class="headerlink" title="特征不变性, 特征描述, 特征匹配Feature Invariance, Description, and Matching"></a>特征不变性, 特征描述, 特征匹配Feature Invariance, Description, and Matching</h2><p>我们首先要找到feature point, 之后我们要create a description of these points. (我们有一个好的表示方式来表示一块区域的特征). 之后匹配它们.</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Image-transformations"><a href="#Image-transformations" class="headerlink" title="Image transformations"></a>Image transformations</h3><p>我们可能遇到的变换: 几何变换 和 光度变换</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Invariance-and-equivariance"><a href="#Invariance-and-equivariance" class="headerlink" title="Invariance and equivariance"></a>Invariance and equivariance</h3><p>We want corner locations to be <em>invariant</em> to photometric transformations and <em>equivariant</em> to geometric transformations</p>
<p>我们希望corner的位置在光度变换时是invariant, 在几何变换时是equivariant</p>
<ul>
<li><strong>Invariance:</strong> image is transformed and corner locations do not change</li>
<li><strong>Equivariance:</strong> if we have two transformed versions of the same image, features should be detected in corresponding locations(等方差?)</li>
</ul>
<br>

<p>来看看Harris Corner Detection</p>
<h4 id="image-translation"><a href="#image-translation" class="headerlink" title="image translation"></a>image translation</h4><p><img src="/Blog/Blog/intro/com_graphic/feature_des_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>当我们改变了Corner的位置, Harris Corner Detection能够和以前一样找到Corner的位置</p>
<p><strong>Corner location is equivariant with respect to translation</strong></p>
<br>

<br>

<h4 id="image-rotation"><a href="#image-rotation" class="headerlink" title="image rotation"></a>image rotation</h4><p><img src="/Blog/Blog/intro/com_graphic/feature_des_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>旋转后椭圆(Corner Edge Detection H matrix)的shape不应该百变</p>
<p><strong>Corner location is equivariant with respect to image rotation</strong></p>
<br>

<br>

<h4 id="Affine-intensity-change"><a href="#Affine-intensity-change" class="headerlink" title="Affine intensity change"></a>Affine intensity change</h4><p><img src="/Blog/Blog/intro/com_graphic/feature_des_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>对于仿射变换, Harris Corner Detection 出问题了. 因为我们设置了threshold去探测Corner</p>
<p>而这个Threshold should change from one brightness to another brightness.</p>
<p><strong>因此Harris Corner Detector is not really invariant to  affine intensity change.</strong></p>
<p>它能处理一些仿射变换, 但如果点和threshold的很接近, 那么很可能会fail.</p>
<br>

<h4 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h4><p><img src="/Blog/Blog/intro/com_graphic/feature_des_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>当进行把图片放大时, 原本的 Corner 会被检测成 Edge</p>
<p><strong>Neither invariant nor equivariant to scaling</strong></p>
<br>

<h3 id="Scaling-invariant-detection"><a href="#Scaling-invariant-detection" class="headerlink" title="Scaling invariant detection"></a>Scaling invariant detection</h3><p>假设我们在检测corner, 如果离得太近就会看起来像Edge. 但当我们increase neighborhood size, 它就看起来像corner</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>但如果增大太多, 那就有特别多的 feature 在同一个 window 中</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>Key idea: find scale that gives local maximum of <em>f</em></p>
<ul>
<li><p>in both position and scale</p>
</li>
<li><p>One definition of <em>f</em>: the Harris operator</p>
</li>
</ul>
<br>

<h3 id="Automatic-Scale-Selection"><a href="#Automatic-Scale-Selection" class="headerlink" title="Automatic Scale Selection"></a>Automatic Scale Selection</h3><p>那么如何进行Scale selection automatically?</p>
<p>首先对图中的所有像素进行corner detection at multiple scales 计算 Cornerness. 并将它们放入一张图中.</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>之后我们可以选择其中Cornerness值最大的一个的scale</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>当我们对不同大小的图片进行了这个操作</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这样我们就可以决定具体选择什么scale, 即使两张图的scale不同.</p>
<p>之后我们把它缩放成统一size的图, 之后匹配它们</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>如果我们用非常大的kernel对图片进行filtering会非常耗时, 所以我们要使用一个叫高斯金字塔的东西</p>
<br>

<h3 id="高斯金字塔-Gaussian-pyramid"><a href="#高斯金字塔-Gaussian-pyramid" class="headerlink" title="高斯金字塔(Gaussian pyramid)"></a>高斯金字塔(Gaussian pyramid)</h3><p><img src="/Blog/Blog/intro/com_graphic/feature_des_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>we create different version of the same image.</p>
<br>

<p>我们将用同样的 kernel detector 对不同大小的图片进行filtering</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>之后我们就会得到different point at different scales.</p>
<br>

<h3 id="Laplacian-of-Gaussian"><a href="#Laplacian-of-Gaussian" class="headerlink" title="Laplacian of Gaussian"></a>Laplacian of Gaussian</h3><p>除了用Harris Corner measure(). 我们还可以用 Laplacian of Gaussian kernel(LoG)</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>LoG 可以当作是一个 “Blob” detector</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们同样需要找到scale, 而scale等于我们找到的Blob的半径</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>而我们可以对所有的 图像(包括缩放后的) 做这个scale space selection 找到它们的 corner response</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/feature_des_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>当我们进行NMS, 不再仅仅看周围的8个neighbor, 而是看这一个正方体内的neighbor</p>
<p>一个像素除了周围8个像素, 上下层也有neighbor</p>
<p>如果要被标记为一个点, 它要比它所有neighbor大</p>
<br>

<p>之后就能找到best scale for the feature and the best location for the feature.</p>
<br>

<p>对于Scale Invariant Detection, 除了使用拉普拉斯, 我们还可以用Difference of Gaussians</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>而我们已经有很多 scaled 图像 with gaussian filtered already(当我们downsize a image we should do gauss blur first)</p>
<p>we can just take the difference between these scales, it will give us something very similar to the laplacian kernel.</p>
<br>

<h3 id="Describe"><a href="#Describe" class="headerlink" title="Describe"></a>Describe</h3><p>在我们找到 features 之后, 下一个问题就是 matching 它们.</p>
<p> <img src="/Blog/Blog/intro/com_graphic/feature_des_30.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如何 matching 他们? 答案就是对每个点使用一个 descriptor 来找到两张图中类似的descriptors </p>
<br>

<p>我们希望descriptor 是 invariance (not change from one image to another).</p>
<p><strong>Invariance vs. discriminability</strong></p>
<ul>
<li><p>Invariance:</p>
<ul>
<li>Descriptor shouldn’t change even if image is transformed</li>
</ul>
</li>
<li><p>Discriminability:</p>
<ul>
<li>Descriptor should be highly unique for each point</li>
</ul>
</li>
</ul>
<br>

<h4 id="Rotation-invariance"><a href="#Rotation-invariance" class="headerlink" title="Rotation invariance"></a>Rotation invariance</h4><p>只要找到dominant orientation of the image patch</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>dominant orientation 可以用</p>
<ul>
<li>given by $x_{max}$, the eigenvector of <strong>H</strong> corresponding to $\lambda_{max}$ (the <em>larger</em> eigenvalue)</li>
<li>the orientation of the (smoothed) gradient(canny edge detector中使用.)</li>
</ul>
<p>当我们有了dominant orientation, 我们可以直接旋转window. </p>
<p>Then it will invariance to Rotation.</p>
<br>

<h4 id="Multiscale-Oriented-PatcheS-descriptor-MOPS"><a href="#Multiscale-Oriented-PatcheS-descriptor-MOPS" class="headerlink" title="Multiscale Oriented PatcheS descriptor(MOPS)"></a>Multiscale Oriented PatcheS descriptor(MOPS)</h4><p>假设我们看40x40的window</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_26.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<ol>
<li>Down scale to 8x8</li>
<li>Rotate it</li>
<li>Normalize it</li>
</ol>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_27.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="Scale-Invariant-Feature-Transform-SIFT"><a href="#Scale-Invariant-Feature-Transform-SIFT" class="headerlink" title="Scale Invariant Feature Transform (SIFT)"></a>Scale Invariant Feature Transform (SIFT)</h4><p><img src="/Blog/Blog/intro/com_graphic/feature_des_28.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>和之前一样</p>
<p>SIFT find the features by looking at the difference of the Gaussian at scaled space. And finding local maximum in the scale space.</p>
<br>

<p>之后尝试descript the features.</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_29.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>核心思想就是看gradient directions, in the local window around the pixel.</p>
<ol>
<li>对于每个pixel, 我们计算它的 gradient 的强度和方向</li>
<li>去掉强度小的 gradient(可能来自噪声)</li>
<li>创建直方图, 表述每个gradient的方向</li>
</ol>
<p>这个直方图就是SIFT descriptor.</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/feature_des_31.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>在真实的实现中, 通常把window 分成2x2的4个区域</p>
<p>之后为每个像素计算直方图, 计算出16x16&#x3D;128个descriptor</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_32.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>优点:</p>
<ul>
<li><p>可以处理最多60度的视角不同.</p>
</li>
<li><p>可以处理明显的亮度变化</p>
</li>
<li><p>算法快速, 可以事实运行</p>
</li>
<li><p>有许多开源代码</p>
</li>
</ul>
<br>

<h3 id="Feature-matching"><a href="#Feature-matching" class="headerlink" title="Feature matching"></a>Feature matching</h3><p>Given a feature in $I_1$, how to find best match in $I_2$?</p>
<ul>
<li>我们要定义一个distance function 来比较两个descriptor</li>
<li>对$I_2$的所有feature进行测试, 找到distance最小的一个</li>
</ul>
<br>

<p>然而有时候这并不可靠. 因为我们可能有similar patches in image.</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_33.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们可以用ratio distance</p>
<ul>
<li>Better approach: ratio distance &#x3D; $\frac {||f_1 - f_2 ||} {|| f_1 - f_2’ ||}$ <ul>
<li>$f_2$ is best SSD match to $f_1$ in $I_2$</li>
<li>$f_2’$ is 2nd best SSD match to $f_1$ in $I_2$</li>
<li>gives large values for ambiguous matches</li>
</ul>
</li>
</ul>
<p>如果$f_2$ 和 $f’_2$相似, 我们不会认为是good match</p>
<p>如果一个是good match, 一个是low match, 那么我们依然可以得到一个high matching score.</p>
<br>

<p>有的时候使用ratio test, 可以得到一个比较不错的结果.</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_34.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>但是对于有些情况, 如果我们只用simple threshold of ratio, 会有一些不正确的match.</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_35.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们需要evaluate the performance of the feature descriptors by using how many good matches and how many incorrect matches for a given threshold.</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_36.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>求出feature distance之后, 通过 threshold 区分good match 和 bad match.</p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_37.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>之后可以绘制出 <a href="https://daolinzhou.github.io/Blog/2020/05/08/machine-learning-8/#%E7%B2%BE%E5%87%86%E7%8E%87-%E5%8F%AC%E5%9B%9E%E7%8E%87%E6%9B%B2%E7%BA%BF">precision-recall curve</a></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_38.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/feature_des_39.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="Transformations-amp-Image-Alignment"><a href="#Transformations-amp-Image-Alignment" class="headerlink" title="Transformations &amp; Image Alignment"></a>Transformations &amp; Image Alignment</h2><p>让我们尝试让这样两张图进行对齐:</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>然而有更好的对齐方式:</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<ul>
<li><p>First, we need to know what this transformation is.</p>
</li>
<li><p>Second, we need to figure out how to compute it using feature matches</p>
</li>
</ul>
<br>

<h3 id="Image-Warping"><a href="#Image-Warping" class="headerlink" title="Image Warping"></a>Image Warping</h3><p><img src="/Blog/Blog/intro/com_graphic/alignment_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>之前进行的image filtering, 改变的是image的range, 也就是y轴</p>
<p>而image warping, 改变的是image的domain, 也就是x轴</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Parametric-global-warping"><a href="#Parametric-global-warping" class="headerlink" title="Parametric (global) warping"></a>Parametric (global) warping</h3><p><img src="/Blog/Blog/intro/com_graphic/alignment_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>为了实现它, 我们要对第一张图中的所有像素使用warping operator(T)得到它应该在第二张图中的位置</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>T 是一个coordinate changing function<br>$$<br>T(p)&#x3D;p’<br>$$<br>为什么说 T 是 global ?</p>
<ol>
<li>它对p中所有的点都生效</li>
<li>可以被几个参数描述</li>
</ol>
<br>

<h3 id="Linear-Transform"><a href="#Linear-Transform" class="headerlink" title="Linear Transform"></a>Linear Transform</h3><p>如果是线性变换, 则可以用(2, 2)的矩阵描述</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><strong>缩放scaling</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><strong>旋转rotate</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><strong>镜像mirror</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>但是我们无法使用二维矩阵实现translation<br>$$<br>x’&#x3D;x+t_x<br>$$</p>
<p>$$<br>y’&#x3D;y+t_y<br>$$</p>
<p>Translation is not a linear operation on 2D coordinates.</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/alignment_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Homogeneous-coordinates"><a href="#Homogeneous-coordinates" class="headerlink" title="Homogeneous coordinates"></a>Homogeneous coordinates</h3><p>技巧: 增加一个coordinate</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果已经是homogeneous coordinate 了, 但是最后一项不是1, 则可以简单的进行归一</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>如何对这个向量进行Translation?</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="仿射"><a href="#仿射" class="headerlink" title="仿射"></a>仿射</h3><p><img src="/Blog/Blog/intro/com_graphic/alignment_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>只要T的最后一行是(0, 0, 1), 那么我们就称它为仿射变换.</p>
<p>具体表现就是线性变换后接上一个平移</p>
<br>

<h4 id="基本的仿射变换"><a href="#基本的仿射变换" class="headerlink" title="基本的仿射变换"></a>基本的仿射变换</h4><p><img src="/Blog/Blog/intro/com_graphic/alignment_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>仿射变换的性质</p>
<ul>
<li><p>Origin does not necessarily map to origin</p>
</li>
<li><p>Lines map to lines</p>
</li>
<li><p>Parallel lines remain parallel</p>
</li>
<li><p>Ratios are preserved</p>
</li>
<li><p>Closed under composition</p>
</li>
</ul>
<br>

<p>然而这些依然不足以match这两张图</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Homography"><a href="#Homography" class="headerlink" title="Homography"></a>Homography</h3><p>如果我们把这个最后一行填满会怎样?</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这东西叫做 homography</p>
<br>

<p>如果我们在现实中改变拍照的角度, 两张照片就是homography的关系</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>但是如果最后一个元素是0怎么办? $gx+hy+1&#x3D;0$</p>
<p>那么此时 x 和 y 就变成Infinity</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/alignment_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>这也就是解决这个问题的办法</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们要找到一个能用于描述这两张图关系的homography.</p>
<br>

<h4 id="Homographies-的性质"><a href="#Homographies-的性质" class="headerlink" title="Homographies 的性质"></a>Homographies 的性质</h4><p><img src="/Blog/Blog/intro/com_graphic/alignment_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/alignment_26.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h3 id="Forward-Warping"><a href="#Forward-Warping" class="headerlink" title="Forward Warping"></a>Forward Warping</h3><p>然而当我们对图片进行warp时, 某些点转换后的坐标可能是小数(也就是说某个点的坐标可能在两个pixel之间)</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_27.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_28.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>解决的办法就是对一些像素添加”贡献度”(contribution). normalize later(splatting)</p>
<p>But can still result in holes if one pixel doesn’t have any mapping pixels to its immediate neighbors.</p>
<br>

<h3 id="Inverse-Warping"><a href="#Inverse-Warping" class="headerlink" title="Inverse Warping"></a>Inverse Warping</h3><p>为了丢弃这些holes. 我们可以对结果图片进行inverse warping</p>
<p>来找到 warping 后的 image 中某一点对应在原图的位置</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_29.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>然而依然有可能映射回去的像素处于两个像素之间</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_30.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>此时的做法就是<strong>resampling</strong></p>
<ul>
<li><em>resample</em> color value from <em>interpolated</em> (<strong>prefiltered&#x2F; Anti-aliasing</strong>) source image</li>
</ul>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/alignment_31.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="计算Transformation"><a href="#计算Transformation" class="headerlink" title="计算Transformation"></a>计算Transformation</h3><p>假设我们已经知道了A和B之间的match, 如何计算Transform matrix T from A to B</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_32.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>先从一个简单的例子说起</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_33.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_34.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们要计算displacement of match i:</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_35.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>对于多个match, 我们可以简单的去平均值来作为translation.</p>
<br>

<p>或者我们可以把这个问题看作是一个linear equation</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_36.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<ul>
<li>Problem: more equations than unknowns<ul>
<li>“Overdetermined” system of equations</li>
<li>We will find the <em>least squares</em> solution(<a href="https://daolinzhou.github.io/Blog/2020/03/06/play-with-linear-algebra-4/#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95-least-squares-Approximation">最小二乘法</a>)</li>
</ul>
</li>
</ul>
<p>使用最小二乘法找到近似解<br>$$<br>A^TAt &#x3D; A^Tb<br>$$</p>
<p>$$<br>t &#x3D; (A^TA)^{-1}A^Tb<br>$$</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/alignment_37.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>仿射变换中有6个未知数</p>
<p>残差和损失函数</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_38.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们可以将它写作矩阵的形式</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_39.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>Homographies 有 8 degree of freedom.</p>
<p>所以我们<strong>至少</strong>要4个点来定义一个homography(we get 2 equations from each point)</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_40.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_41.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_42.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>多组x, y组合起来就是</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_43.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>也就是说当有image A 和 B</p>
<ol>
<li><p>Compute image features for A and B</p>
</li>
<li><p>Match features between A and B</p>
</li>
<li><p>Compute homography between A and B using least squares on set of matches</p>
</li>
</ol>
<br>

<p>然而有些东西会发生错误: 比如说outlier</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_44.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<blockquote>
<p>其实这样使用最小二乘法就相当于使用一元线性回归法, 从这个角度上讲可以, 把$x_i, y_i$作为输入 $x_i’, y_i’$作为输出, 训练一个回归的机器学习模型.</p>
</blockquote>
<br>

<br>

<h2 id="RANSAC"><a href="#RANSAC" class="headerlink" title="RANSAC"></a>RANSAC</h2><p>RANSAC : Random Sample Consensus</p>
<p>如何处理两张图片的outlier match &#x2F; incorrect match</p>
<p><img src="/Blog/Blog/intro/com_graphic/alignment_44.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>先从简单的线性回归看起</p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>回归线收到outlier影响严重.</p>
<br>

<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>为了解决这个问题, RANSAC的思路是: 尝试许多hypothesized line</p>
<ul>
<li>对于每一个hypothesized line</li>
<li>计算number of points that “agree” with the line<ul>
<li>“Agree” &#x3D; with a small distance of the line</li>
<li>例如: <strong>inliers</strong> to that line</li>
</ul>
</li>
<li>对于所有的hypothesized line, 选择那个agree points 最多的线</li>
</ul>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/ransac_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>举个例子:</p>
<p>蓝色是correct match, 红色是incorrect match</p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>但是目前我们还不知道</p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>因此我们随机选择一个match, 计算inlier</p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>再选择另一个match, 计算inlier</p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>结果就是inlier最多的那个match</p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>RANSAC可以处理outlier &lt; $50%$的情况(容错率高)</p>
<blockquote>
<p>核心思想: </p>
<p>“All good matches are alike; every bad match is bad in its own way.”  – Tolstoy via Alyosha Efros</p>
</blockquote>
<br>

<h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>RANSAC需要一些超参数:</p>
<ul>
<li><strong>Inlier threshold</strong> 决定了我们希望 inliers 能容纳多少噪音<ul>
<li>often model noise as Gaussian with some standard deviation(eg. 3 pixels)</li>
</ul>
</li>
<li><strong>Number of rounds</strong>(iterations) related to the percentage of outliers we expect, and the probability of success we’d like to guarantee.<ul>
<li>假设有$20%$ 的outlier, 我们想要找到 correct answer with $99%$ probability.  How many of rounds do we need?</li>
</ul>
</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>如何生成hypothesis line?</p>
<p>随便选择两个点, 连接成一条线</p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>RANSAC:</p>
<ol>
<li>随机选择 s 个 samples<ul>
<li>s &#x3D; minimum sample size that lets you fit a model</li>
</ul>
</li>
<li>Fit a model(eg, line) to those samples.</li>
<li>计算 inlier 的数量</li>
<li>重复 N 次</li>
<li>选择最好的模型(最大inlier数量的模型)</li>
</ol>
<br>

<p>迭代几次? round &#x3D; N</p>
<p>如果我们每次选择 <em>s</em> samples, outlier 的比例为 <em>e</em>, 我们希望正确结果的可能性为 <em>p</em></p>
<p> <img src="/Blog/Blog/intro/com_graphic/ransac_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>优势与劣势:</p>
<p>优势</p>
<ul>
<li>算法简单, 适用性高</li>
<li>可以解决各种各样的问题</li>
<li>练习时通常工作很好</li>
</ul>
<p>劣势:</p>
<ul>
<li><p>有很多参数需要调参</p>
</li>
<li><p>有时需要迭代多次</p>
</li>
<li><p>inlier 比例低时会失败</p>
</li>
<li><p>We can often do better than brute-force sampling</p>
</li>
</ul>
<br>

<p>通过RANSAC找到inlier之后就可以用最小二乘法得到transformation</p>
<br>

<p>这样我们就能创建全景图了</p>
<p><img src="/Blog/Blog/intro/com_graphic/ransac_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.mathworks.com/help/vision/ug/feature-based-panoramic-image-stitching.html">panoramic matlab</a></p>
</blockquote>
<br>

<br>

<h2 id="图像分割-Segmentation"><a href="#图像分割-Segmentation" class="headerlink" title="图像分割 Segmentation"></a>图像分割 Segmentation</h2><p>我们谈论图像分割, 我们是希望将图区分为不同的区域. 这片区域可能是因为颜色被分割, 可能是因为它的语义(semantic meaning)</p>
<p>semantic segmentation 是目前 Computer vision 中一个热门的topic</p>
<br>

<h3 id="Toy-Example"><a href="#Toy-Example" class="headerlink" title="Toy Example"></a>Toy Example</h3><br>

<p><img src="/Blog/Blog/intro/com_graphic/segmentation.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们要尝试根据黑色, 白色, 灰色分割图片.</p>
<p>只要放入直方图中, 横轴代表颜色(灰度)纵轴代表像素个数就能简单区分</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>然而实际只要有一些噪音, 我们的结果就会大变样</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>此时我们需要进行 <strong>clustering</strong>. 就是找到一种方式, 把同样类别的像素放到一起</p>
<p> <img src="/Blog/Blog/intro/com_graphic/segmentation_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这只是一个1D example, 实际上我们可能要处理RGB. </p>
<p>  <img src="/Blog/Blog/intro/com_graphic/segmentation_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="K-Means-的应用"><a href="#K-Means-的应用" class="headerlink" title="K-Means 的应用"></a>K-Means 的应用</h3><p>进行聚类的方法通常使用<a href="https://daolinzhou.github.io/Blog/2020/05/20/k-means/">K-Means</a>, 然而K-Means也有很多缺点</p>
<br>

<p>这就是使用K-Means进行图形分割的结果.</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>由于我们在这里只是按照颜色进行分类, 而有些噪音的颜色像手的颜色, 所以被错误地分类为手.</p>
<br>

<br>

<p>然而不是所有图像都可以通过颜色进行分割.</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>为什么得到这样的结果?</p>
<p>因为我们只关注<strong>颜色</strong>, 而不关注<strong>距离</strong>, 即使两个颜色距离很远, 但只要它们的颜色类似, 我们就会把它们分为同一类.</p>
<p>但在大部分情况下, 两个像素间的距离是一个非常重要的信息. 通常同一个物品的像素是聚集在一起的.</p>
<ul>
<li>近距离的两个像素更有可能属于同一个物品</li>
<li>远距离的两个像素更有可能属于不同物品</li>
</ul>
<p>因此我们可以把一个<strong>像素的位置也作为分类的特征</strong>. 原本是同构(R, G, B) 三个特征进行分类, 现在是根据(R, G, B, X, Y)进行分类. (X, Y 代表坐标)</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<blockquote>
<p>这也不是完美的解决方案. 这个问题根本没有完美的解决方案</p>
</blockquote>
<br>

<p>除了K-Means本身的缺点.</p>
<p>使用K-Means进行图像分割还有一些其他的问题</p>
<p>虽然k-Means 可以把类似的像素分成一类, 但是</p>
<p>Captures pixel similarity but</p>
<ol>
<li>Doesn’t capture continuity of contours</li>
<li>Captures near&#x2F;far relationships only weakly</li>
<li>Can merge far away objects together</li>
<li>Can’t deal with texture(质地)</li>
</ol>
<br>

<h3 id="Texture-质地"><a href="#Texture-质地" class="headerlink" title="Texture 质地"></a>Texture 质地</h3><p>Texture 这个概念很难定义, 它基于人们的知觉(perception)</p>
<ul>
<li><p>Some sort of pattern consisting of repeating elements</p>
</li>
<li><p>That we perceive as a pattern rather than individual elements</p>
</li>
</ul>
<p>有些texture内有edge, 但它们依然应该作为一个物体</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>在现实中也可能这种情况</p>
<p>我们可能会把一群鸟看作一个整体</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>有的时候即使没有边缘(boundary), 我们也可以通过Texture区分边缘</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>为了能区分这个 boundary, we need to somehow parameterize what a texture is.</p>
<br>

<br>

<h3 id="Julesz’s-texton-theory"><a href="#Julesz’s-texton-theory" class="headerlink" title="Julesz’s texton theory"></a>Julesz’s texton theory</h3><p>人类的视觉以两种不同方式运行.</p>
<ul>
<li><strong>Pre-attentive vision</strong> - parallel, instantaneous</li>
<li><strong>Attentive vision</strong> - serial search by focusing on individual things</li>
</ul>
<p>而纹理鉴别在<strong>pre-attentive mode</strong>时进行</p>
<ul>
<li>We don’t look at individual patterns but at statistics of the region</li>
</ul>
<p>What kind of statistics?</p>
<ul>
<li><p>Not just average color</p>
</li>
<li><p>But density of certain elements – “textons”</p>
</li>
</ul>
<p>我们可以用一系列filters作为textons</p>
<p>Textons 可能是:</p>
<ul>
<li><p>Elongated blobs - e.g. rectangles, ellipses, line segments with specific orientations, widths and lengths</p>
</li>
<li><p>Terminators - ends of line segments</p>
</li>
<li><p>Crossings of line segments</p>
</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="2D-Texton"><a href="#2D-Texton" class="headerlink" title="2D Texton"></a>2D Texton</h4><p><img src="/Blog/Blog/intro/com_graphic/segmentation_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Mean-Shift的应用"><a href="#Mean-Shift的应用" class="headerlink" title="Mean Shift的应用"></a>Mean Shift的应用</h3><p>K-Means有诸多坏处, 因此另一个比较好的方法就是使用均值漂移<a href="https://daolinzhou.github.io/Blog/2020/05/21/mean-shift/">Mean-Shift</a></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>但这依然只是在进行比较低级的分类</p>
<br>

<ul>
<li><p>Pros:</p>
<ul>
<li>Does not assume shape on clusters</li>
<li>One parameter choice (window size)</li>
<li>Generic technique</li>
<li>Find multiple modes</li>
</ul>
</li>
<li><p>Cons:</p>
<ul>
<li>Selection of window size</li>
<li>Does not scale well with dimension of feature space</li>
</ul>
</li>
</ul>
<br>

<h3 id="Superpixels-x2F-x2F-TODO-实现超像素算法"><a href="#Superpixels-x2F-x2F-TODO-实现超像素算法" class="headerlink" title="Superpixels &#x2F;&#x2F; TODO 实现超像素算法"></a>Superpixels &#x2F;&#x2F; TODO 实现超像素算法</h3><p><img src="/Blog/Blog/intro/com_graphic/segmentation_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>当我们处理superpixels. we are not interesting in define objects</p>
<p>we are interesting in define small coheirs regions</p>
<br>

<p>When we look at them at Segmentation point of view or meaning point of view, or a color point of view. They are similar to each other.</p>
<p>所以有时我们不关注单一pixel, 而是关注这些coheirs regions, 把他们当作一个大pixel</p>
<p>可以降低数据的维度, 处理大的superpixel而不是小的pixel</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><a target="_blank" rel="noopener" href="https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html">OpenCV GrabCut</a></p>
<br>

<br>

<h2 id="Optical-Flow"><a href="#Optical-Flow" class="headerlink" title="Optical Flow"></a>Optical Flow</h2><p>Motion is a powerful perceptual cue </p>
<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/move.mp4" type="video/mp4">
</video>

<p>even “impoverished” motion data can evoke a strong percept</p>
<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/move2.mp4" type="video/mp4">
</video>

<br>

<p>Motion 在计算机视觉中的应用:</p>
<ul>
<li><p>3D shape reconstruction</p>
</li>
<li><p>Object segmentation</p>
</li>
<li><p>Learning and tracking of dynamical models</p>
</li>
<li><p>Event and activity recognition</p>
</li>
<li><p>Self-supervised and predictive learning</p>
</li>
</ul>
<br>

<h3 id="Motion-field"><a href="#Motion-field" class="headerlink" title="Motion field"></a>Motion field</h3><ul>
<li>The motion field is the projection of the 3D scene motion into the image</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>左边两张图有一个小转动, 但是motion translate with different motion vectors</p>
<br>

<h3 id="Optical-Flow-1"><a href="#Optical-Flow-1" class="headerlink" title="Optical Flow"></a>Optical Flow</h3><p>定义: optical flow is the <strong>apparent(明显的)</strong> motion of brightness patterns in the image.</p>
<p>理想情况, optical flow 应该和 motion field 一样.</p>
<p>但是一定要注意: apparent motion 可能是因为光源变化产生的, 而不是由真实的 motion 产生的. (尽管不经常发生)</p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/segmentation_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>假设:</p>
<ol>
<li>亮度一样, 同一个点的投影是一样的</li>
<li>小量移动, 点不会移动太远</li>
<li>空间连贯, 点和它neighbors的运动类似</li>
</ol>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/segmentation_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>t 表示的是 time.</p>
<br>
$$
I_xu+I_yv+I_t=0
$$
这个式子有两个未知量u, v. 
$$
\nabla I(u, v)+I_t=0
$$
当我们没有足够多的提示之前, 我们无法得到真正的motion

<p><img src="/Blog/Blog/intro/com_graphic/segmentation_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>举个例子:</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_26.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_27.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果单看上面两张图, 我们可能猜测line是这样移动的.</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_28.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>但当我们看整个 line 是, line可能是这样运动的</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_29.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>可以看到 actual motion 可能和 perceived motion 不一样</p>
<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/move3.mp4" type="video/mp4">
</video>

<p>例如理发店门口的这个, 看起来像是从上往下移动, 然而实际只是进行旋转</p>
<br>

<p>为了解决这个aperture problem, 我们需要更多的 equations.</p>
<p>根据我们对空间连续的假设, 这个点周围的 pixel 的移动方式应该和这个点类似. 因此我们可以用周围点的数据扩大 equations 的数量</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_30.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>之后这又变成了用最小二乘法解决的问题</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_31.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>回忆在Harris Corner Detection中的部分知识. </p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_32.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>当窗口移动时, 我们对得到一些”变化”. 我们可以说一个corner在x, y轴怎么移动的. 或者一条线在某一方向如何移动的</p>
<br>

<p>bad case: <strong>single straight edge</strong>. 只有一条直边, 我们无法准确找到它的motion</p>
<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/move4.mp4" type="video/mp4">
</video>

<p>good case: 有 corner, 我们能找到 actual motion</p>
<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/move5.mp4" type="video/mp4">
</video>

<br>

<h3 id="Lucas-Kanade-flow-example-x2F-x2F-TODO-实现这个算法"><a href="#Lucas-Kanade-flow-example-x2F-x2F-TODO-实现这个算法" class="headerlink" title="Lucas-Kanade flow example &#x2F;&#x2F; TODO 实现这个算法"></a>Lucas-Kanade flow example &#x2F;&#x2F; TODO 实现这个算法</h3><p><img src="/Blog/Blog/intro/com_graphic/segmentation_33.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>但是在某些情况下Lucas-Kanade也有error</p>
<ol>
<li>The motion is large (larger than a pixel)</li>
<li>A point does not move like its neighbors</li>
<li>Brightness constancy does not hold</li>
</ol>
<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/move6.mp4" type="video/mp4">
</video>

<p>例如这个移动, 离镜头越近motion越大, 离镜头越远, motion越小.</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_34.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>因此Lucas-Kanade无法处理近距离的大motion, 但是远距离小motion可以处理</p>
<br>

<h3 id="Multi-resolution-estimation"><a href="#Multi-resolution-estimation" class="headerlink" title="Multi-resolution estimation"></a>Multi-resolution estimation</h3><video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/move7.mp4" type="video/mp4">
</video>

<p>和SIFT类似 (<strong>高斯金字塔就是在模拟镜头的远近</strong>), 通过subsampling. 原本一个进行大motion移动的像素就会变成小motion移动的像素. 相当于拉远了距离.</p>
<p>因此我们可以catch the large motion in the smaller version image. 就像SIFT catch corner in the smaller version image 一样.</p>
<p> 这样大motion通过subsampling + Lucas-Kanade 得到, 小motion可以直接通过Lucas-Kande得到. 直到计算出图片中所有的motion</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_35.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Fixing-the-error-in-Lucas-Kanade"><a href="#Fixing-the-error-in-Lucas-Kanade" class="headerlink" title="Fixing the error in Lucas-Kanade"></a>Fixing the error in Lucas-Kanade</h3><p>我们首先estimate large motion in small image.</p>
<p>keep iterative it and refining it</p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_36.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_37.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<blockquote>
<p>Iterative refinement:</p>
<ul>
<li><p>Estimate velocity at each pixel using one iteration of Lucas and Kanade estimation</p>
</li>
<li><p>Warp one image toward the other using the estimated flow field</p>
</li>
<li><p>Refine estimate by repeating the process</p>
</li>
</ul>
</blockquote>
<br>

<h3 id="Feature-Tracking"><a href="#Feature-Tracking" class="headerlink" title="Feature Tracking"></a>Feature Tracking</h3><p><img src="/Blog/Blog/intro/com_graphic/segmentation_38.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>当我们有2张以上的图, 我们就能track a feature from one frame to the next by following the optical flow</p>
<p>然而难点是:</p>
<ol>
<li>找到好的 feature to track</li>
<li>添加或删除track. 因为有时一个特征运行到一定程度后会不动或消失</li>
</ol>
<h4 id="Shi-Tomasi-feature-tracker"><a href="#Shi-Tomasi-feature-tracker" class="headerlink" title="Shi-Tomasi feature tracker"></a>Shi-Tomasi feature tracker</h4><ul>
<li><p>Find good features using eigenvalues of second-moment matrix</p>
<ul>
<li>Key idea: “good” features to track are the ones whose motion can be estimated reliably</li>
</ul>
</li>
<li><p>From frame to frame, track with Lucas-Kanade</p>
<ul>
<li>This amounts to assuming a translation model for frame-to-frame feature movement</li>
</ul>
</li>
<li><p>Check consistency of tracks by <em>affine</em> registration to the first observed instance of the feature</p>
<ul>
<li>Affine model is more accurate for larger displacements</li>
<li>Comparing to the first frame helps to minimize drift</li>
</ul>
</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/segmentation_39.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><a target="_blank" rel="noopener" href="https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html">OpenCV: Optical Flow</a></p>
<br>

<br>

<h2 id="Camera"><a href="#Camera" class="headerlink" title="Camera"></a>Camera</h2><p>假设有一个传感器, 还有一个要拍照的物体</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>此时如果拍照的话会是什么样?</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>树的每一个点都会有光射向传感器的所有区域, 此时的图像是非常模糊的</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>如何解决这个问题, 让图片看起来更清晰?</p>
<p>这就是小孔成像的原理, 过滤光线. 孔的大小叫做adapter</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Focal-length"><a href="#Focal-length" class="headerlink" title="Focal length"></a>Focal length</h3><p>adapter 到 传感器的距离是 focal length</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>显然这个 focal length 是可变的. 如果缩短focal length</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>图像会变小.</p>
<br>

<h3 id="Aperture"><a href="#Aperture" class="headerlink" title="Aperture"></a>Aperture</h3><p>如果增大Aperture, 就相当于允许更多的光通过小孔.</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>图像会变得模糊, 如果没有小孔, 我们可以看作时一个无限大的小孔.</p>
<p>而孔越小进入的光线就越小, 图片就越暗.</p>
<br>

<h3 id="Lens"><a href="#Lens" class="headerlink" title="Lens"></a>Lens</h3><p>如果我们想要sharp image, 我们就要缩小pin hole, 但如果要让更多光进入camera, (less noisy, more brightness) 那么我们就需要方法pin hole. 这就是一个取舍问题</p>
<p>有没有方法可以得到sharp image, 同时保持亮度?</p>
<p>解决的办法就是使用镜片(Lens)</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>然而使用lens也有一些缺点. 例如这个例子, 红色的光很好的聚焦了, 但蓝色的光没有. 这个叫做 circle of confusion</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>使用lens只能对一个范围进行聚焦, 太近或太远都会变得模糊</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>现实中我们使用的不是pin hole, 而是aperture</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这个可以自动调整小孔的大小.</p>
<br>

<h3 id="Eclipse-pinholw-cameras"><a href="#Eclipse-pinholw-cameras" class="headerlink" title="Eclipse pinholw cameras"></a>Eclipse pinholw cameras</h3><p><img src="/Blog/Blog/intro/com_graphic/camera_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果靠近树洞会看到什么</p>
<h4 id="Field-of-View-Zoom"><a href="#Field-of-View-Zoom" class="headerlink" title="Field of View(Zoom)"></a>Field of View(Zoom)</h4><p><img src="/Blog/Blog/intro/com_graphic/camera_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>重新布置一下小孔相机</p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/camera_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/camera_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Shutters-快门"><a href="#Shutters-快门" class="headerlink" title="Shutters(快门)"></a>Shutters(快门)</h3><p>amount of time that allowed the light to hit the camera.</p>
<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/camera1.mp4" type="video/mp4">
</video>

<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/camera2.mp4" type="video/mp4">
</video>

<video id="video" controls="" preload="none" width="100%">
<source id="mp4" src="/Blog/intro/com_graphic/camera3.mp4" type="video/mp4">
</video>
<br>

<br>

<h2 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h2><p><img src="/Blog/Blog/intro/com_graphic/projection.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>错觉</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Modeling-projection"><a href="#Modeling-projection" class="headerlink" title="Modeling projection"></a>Modeling projection</h3><p><img src="/Blog/Blog/intro/com_graphic/projection_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>就是俩相似三角形</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>COP 和 plane的距离 d 代表 camera 的 focal length.</p>
<p>然而投影后的结果不需要z轴, z轴代表空间. 投影后的结果就应该舍去z</p>
<br>

<p>这是否是linear transformation?</p>
<p>不是, 因为我们要除以 z, 不是linear</p>
<br>

<p>当我们使用homogeneous coordinates on the image plane for two dimensional operations. we put third dimension 1, which is a dummy dimension.</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们也可以随时转换回来, 只要重新将最后一个元素<strong>归一再舍去</strong>就行.  </p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="透视投影-Perspective-projection"><a href="#透视投影-Perspective-projection" class="headerlink" title="透视投影 Perspective projection"></a>透视投影 Perspective projection</h4><p><img src="/Blog/Blog/intro/com_graphic/projection_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这就叫做<strong>perspective projection</strong>(透视投影)</p>
<p>矩阵就叫做投影矩阵</p>
<p>我们可以自由地scaling矩阵, 而结果不变</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="正射投影-Orthographic-projection"><a href="#正射投影-Orthographic-projection" class="headerlink" title="正射投影 Orthographic projection"></a>正射投影 Orthographic projection</h4><p><img src="/Blog/Blog/intro/com_graphic/projection_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>假设 image plane 离 COP 的距离是infinity. </p>
<p>此时投影和原本物体的大小是一样的此时不再depend on depth.</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h5 id="正射投影的变体"><a href="#正射投影的变体" class="headerlink" title="正射投影的变体"></a>正射投影的变体</h5><ul>
<li><strong>Scaled orthographic</strong> (也叫做 weak perspective)</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/projection_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<ul>
<li><strong>Affine projection</strong> (也叫做 paraperspective)</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/projection_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="projection-和-降维"><a href="#projection-和-降维" class="headerlink" title="projection 和 降维"></a>projection 和 降维</h4><p>人眼看世界就是将3D的物体降维成2维平面</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>然而降维是会丢失信息的</p>
<p>我们丢失了 real angles between objects(now depends our viewing) and distances(lengths)</p>
<br>

<h3 id="投影的性质"><a href="#投影的性质" class="headerlink" title="投影的性质"></a>投影的性质</h3><ol>
<li>Many-to-one: 同一条射线上的所有点map to图片上的同一个点(例如我们只能看见最近的物体, 隐藏在这个物体后面的物体我们看不到)</li>
<li>Points map to points</li>
<li>Lines map to lines. linear line 投影后也是 linear line<ul>
<li>但是通过focal point 的line会投影成一个点(Many-to-one)</li>
</ul>
</li>
<li>Planes map to planes (or half plane)<ul>
<li>但是通过focal point 的plane会投影成一条线</li>
</ul>
</li>
</ol>
<br>

<p>one thing we call Perspective (透视) is vanishing line</p>
<p>现实世界的两条平行线汇聚在一个点(vanishing point)</p>
<ul>
<li><p>Each direction in space has its own vanishing point</p>
</li>
<li><p>But parallels parallel to the image plane remain parallel</p>
</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/projection_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们希望找到camera coordinate 和 real world coordinate之间的关系, 这样就可以通过数学来建模projection</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="Camera-parameters"><a href="#Camera-parameters" class="headerlink" title="Camera parameters"></a>Camera parameters</h3><p>为了将现实世界中(x, y, z) 坐标投影到camera. 我们要首先将(x, y, z) transform 到 camera coordinate 中.</p>
<p>因此我们要知道:</p>
<ol>
<li>Camera position (in world coordinates)</li>
<li>Camera orientation (in world coordinates)</li>
</ol>
<p>之后就可以project into the image plane to get a pixel coordinate</p>
<ol start="3">
<li>需要知道 camera intrinsics</li>
</ol>
<br>

<p>相机可以由几个参数描述:</p>
<ul>
<li>Translation <strong>T</strong> of the optical center from the origin of world coords</li>
<li>Rotation <strong>R</strong> of the image plane</li>
<li>focal length <strong>f</strong>, principal point $(x’_c,y’_c)$, pixel size $(s_x, s_y)$</li>
</ul>
<blockquote>
<p>T, R 叫做 extrinsics parameters, because it relate the camera with the world outside the camera</p>
<p>f, $(x’_c,y’_c)$, $(s_x, s_y)$ 叫做 intrinsics parameters</p>
</blockquote>
<p>projection 依然可以用矩阵乘法表示</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_18.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>我们有world coordinate, 以及一个world coordinate中的点q</p>
<p>我们把这个点和Center of Projection 用一条线连接起来, 然后看这条线在哪里和image plane交叉</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_19.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>In order to get rid of the extrinsic part of the projection equation that we just saw, we can create a “canonical form” of the camera.</p>
<p>we will shift the world coordinate to make sure the origin is the center of projection. and xyz axis is defined the way we want them to define it.</p>
<br>

<p> 如何得到 canonical form? </p>
<p>step1, 平移一下让两个原点重合</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_20.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_21.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>step2, rotate by <strong>R</strong></p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_22.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>intrinsics matrix</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_23.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>在现实世界中的camera, $\alpha&#x3D;1$. 但是在一些特殊情况, 例如使用fancy cinema lenses, $\alpha$会有些不同</p>
<p>同样, 通常pixel是不会skew的</p>
<p>principal point is where z-axis across image plane.</p>
<br>

<p>Projection Matrix 就是这一系列参数 (矩阵) 的乘积</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_24.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>后三个矩阵进行坐标转换, 第一个K矩阵把转换后的坐标放入2D image coordinate.</p>
<p>有时后3个矩阵会这样表示, 因为我们只关注parameter, 其余值都是固定值</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_25.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h3 id="透视变形-Perspective-distortion"><a href="#透视变形-Perspective-distortion" class="headerlink" title="透视变形 Perspective distortion"></a>透视变形 Perspective distortion</h3><p><img src="/Blog/Blog/intro/com_graphic/projection_26.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_27.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>解决办法: 让相机可以shift lens</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_28.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>image 还可能因为lens flows 而变形</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_29.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>手机会自动修正, 但如果用真正的相机就要手动修正</p>
<br>

<h4 id="Pincushion-Barrel"><a href="#Pincushion-Barrel" class="headerlink" title="Pincushion, Barrel"></a>Pincushion, Barrel</h4><p><img src="/Blog/Blog/intro/com_graphic/projection_30.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>左边图, 两个竖直的平行线实际上bending toward each other a little bit, 这个叫做pincushion</p>
<p>而右边图则是轻微有些向外的弧度. 这个叫做Barrel</p>
<br>

<p>这两个问题同样可以用数学的方式解决</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_31.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>这两个问题的发生是因为 aperture 和 lens 之间有空隙</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_32.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>如果pinhole 如果离object近, 离image远, 则会有barrel Distortion</p>
<p>如果pinhole 如果离image近, 离object远, 则会有pin cushion Distortion</p>
<br>

<p>我们可以用一些high order parameter处理这些变形, 因为大部分distortion depends on where you are looking inside the image plane</p>
<p><img src="/Blog/Blog/intro/com_graphic/projection_33.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>estimating the (intrinsic&#x2F;extrinsic) parameter 叫做 camera calibration</p>
<br>

<br>

<h2 id="Stereo-立体"><a href="#Stereo-立体" class="headerlink" title="Stereo 立体"></a>Stereo 立体</h2><p>这是一个stereo image</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>以前看3D电影用蓝色和红色的眼镜</p>
<br>

<p>我们有两张图, 分别取自两个不同的观看角度</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<ul>
<li>How can we compute the depth of each point in the image?</li>
</ul>
<p>我们知道, 当移动camera时, 离camera近的点移动的距离大, 离camera远的点移动的距离小. 我们可以利用这方面的信息计算每一个点的深度.</p>
<br>

<h3 id="Epipolar-geometry"><a href="#Epipolar-geometry" class="headerlink" title="Epipolar geometry"></a>Epipolar geometry</h3><p>2 cameras only separate by a translation.</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><strong>Match Pixels in Conjugate</strong> <strong>Epipolar</strong> <strong>Lines</strong></p>
<p>同样我们要假设 brightness 是不变的</p>
<p>然而即使如此, 这也是一个困难的问题</p>
<br>

<p>基本算法:</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>由于这是一维的搜索, 所以我们可以对这条线上的每个点计算SSD(sum of square distance) loss function.</p>
<p>之后就可以绘制每个点的ssd来找到best metch</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>通过更改window size, 算法的效果也发生改变</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<ul>
<li><p>smaller window: more detail, more noise</p>
</li>
<li><p>bigger window: less noise, less detail</p>
</li>
</ul>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>What defines a good stereo correspondence?</p>
<ol>
<li><p>Match quality</p>
<p>Want each pixel to find a good match in the other image</p>
</li>
<li><p>Smoothness</p>
<p>If two pixels are adjacent, they should (usually) move about the same amount</p>
</li>
</ol>
<p>大部分情况, 如果两个像素贴在一起, 那么他们的depth应该类似</p>
<br>

<p>我们要最小化损失函数</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_10.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_11.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>除了match cost 我们还可以加上 smooth cost</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_12.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_13.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>而smooth cost 有很多种定义的方法</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_14.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>2D 问题有很多局部最小值, 所以梯度下降效果不佳</p>
<p>而且搜索空间大, (n, m)大小的图像 k disparities 有 $k^{nm}$种解决方案</p>
<p>然而依然有办法找到这个NP难问题的近似解</p>
<br>

<p>首先定义match cost 和 smoothness cost</p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_15.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>我们要最小化E<br>$$<br>E&#x3D;matchCost+smoothnessCost<br>$$<br>This is a special type of energy function known as an<br> MRF (Markov Random Field)</p>
<ul>
<li>Effective and fast algorithms have been developed:<ul>
<li>Graph cuts, belief propagation….</li>
<li>for more details (and code): <a target="_blank" rel="noopener" href="http://vision.middlebury.edu/MRF/">http://vision.middlebury.edu/MRF/</a> </li>
<li>Great <a target="_blank" rel="noopener" href="http://www.cs.washington.edu/education/courses/577/04sp/">tutorials</a> available online (including video of talks)</li>
</ul>
</li>
</ul>
<br>

<p><img src="/Blog/Blog/intro/com_graphic/stereo_16.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/com_graphic/stereo_17.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>














<h2 id="拓展-实现conv2"><a href="#拓展-实现conv2" class="headerlink" title="拓展: 实现conv2"></a>拓展: 实现conv2</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 对一个点作为中心进行卷积</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convolution</span>(<span class="hljs-params">img, x, y, kernel</span>):<br>    max_h, max_w = img.shape<br>    height, width = kernel.shape<br>    width = (width - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>    height = (height - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>    res = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(-width, width + <span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(-height, height + <span class="hljs-number">1</span>):<br>            new_x, new_y = x - i, y - j<br>            p = <span class="hljs-number">0</span>    <span class="hljs-comment"># 如果超出边界, 默认为 0</span><br>            <span class="hljs-comment"># check range</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-number">0</span> &lt;= new_x &lt; max_w <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= new_y &lt; max_h:<br>                p = img[new_y][new_x]<br>            res += p * kernel[j + height][i + width]<br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv2</span>(<span class="hljs-params">img, kernel, mode=<span class="hljs-string">&quot;full&quot;</span></span>):<br>    height, width = img.shape<br><br>    offset_y, offset_x = kernel.shape<br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&quot;full&quot;</span>:<br>        offset_x = (offset_x - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>        offset_y = (offset_y - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>    <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&quot;same&quot;</span>:<br>        offset_x, offset_y = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># mode == &quot;valid&quot;</span><br>        offset_x = -(offset_x - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>        offset_y = -(offset_y - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br><br>    res = np.empty((height+offset_y*<span class="hljs-number">2</span>, width+offset_x*<span class="hljs-number">2</span>))<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(-offset_x, height+offset_x):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(-offset_y, width+offset_y):<br>            res[i+offset_x][j+offset_y] = convolution(img, j, i, kernel)<br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<p>然而由于卷积可以看作时向量的点乘. 因此使用向量化运算的速度会更快. 改进代码如下</p>
<p>思想就是把两个矩阵reshape成向量进行点乘. (由于是卷积, 所以其中一个向量取逆序), 然而远远不及convolve2d快</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv2</span>(<span class="hljs-params">img, kernel, mode=<span class="hljs-string">&quot;full&quot;</span></span>):<br>    height, width = img.shape<br><br>    k_y, k_x = kernel.shape<br>    kernel1d = kernel.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)[::-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&quot;full&quot;</span>:<br>        offset_x = k_x - <span class="hljs-number">1</span><br>        offset_y = k_y - <span class="hljs-number">1</span><br>        img_exp = np.zeros((height + offset_y*<span class="hljs-number">2</span>, width + offset_x * <span class="hljs-number">2</span>))<br>        img_exp[offset_y:height+offset_y, offset_x: width+offset_x] = img<br>        res = np.empty((height+offset_y, width+offset_x))<br>    <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&quot;same&quot;</span>:<br>        offset_x = (k_x - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>        offset_y = (k_y - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>        img_exp = np.zeros((height + offset_y * <span class="hljs-number">2</span>, width + offset_x * <span class="hljs-number">2</span>))<br>        img_exp[offset_y:height + offset_y, offset_x: width + offset_x] = img<br>        res = np.empty(img.shape)<br>    <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># mode == &quot;valid&quot;</span><br>        offset_x = -(k_x - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>        offset_y = -(k_y - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>        img_exp = img.copy()<br>        res = np.empty((height+<span class="hljs-number">2</span>*offset_y, width+<span class="hljs-number">2</span>*offset_x))<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, res.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, res.shape[<span class="hljs-number">1</span>]):<br>            res[i][j] = img_exp[i:i+k_y, j:j+k_x].reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>).dot(kernel1d)<br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<br>

<h2 id="拓展-测试卷积"><a href="#拓展-测试卷积" class="headerlink" title="拓展: 测试卷积"></a>拓展: 测试卷积</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    lapkern = np.array([[<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>                        [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, -<span class="hljs-number">1</span>],<br>                        [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])<br>    gauss = cv2.getGaussianKernel(<span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br><br>    test_kernel_2 = conv2(lapkern, gauss.dot(gauss.T))<br><br>    <span class="hljs-comment"># 3 使用一维高斯核两次</span><br>    test_kernel_3 = convolve2d(lapkern, gauss)<br>    test_kernel_3 = convolve2d(test_kernel_3, gauss.T)<br><br>    <span class="hljs-comment"># 4和2卷积的顺序颠倒. </span><br>    test_kernel_4 = conv2(gauss.dot(gauss.T), lapkern)<br>    <br>    <span class="hljs-built_in">print</span>(np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">abs</span>(test_kernel_2 - test_kernel_3) &gt; <span class="hljs-number">1e-7</span>))	<span class="hljs-comment"># output = 0</span><br>    <span class="hljs-built_in">print</span>(np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">abs</span>(test_kernel_2 - test_kernel_4) &gt; <span class="hljs-number">1e-7</span>)) <span class="hljs-comment"># output = 0</span><br></code></pre></td></tr></table></figure>

<p>输出为0, 说明每个元素的差异在1e-7一下, 可以判断为相等.</p>
<br>

<h2 id="拓展-实现Canny-Edge-Detection"><a href="#拓展-实现Canny-Edge-Detection" class="headerlink" title="拓展: 实现Canny Edge Detection"></a>拓展: 实现Canny Edge Detection</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CannyEdgeDetection</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, img: np.ndarray, threshold1, threshold2, gauss_size=<span class="hljs-number">5</span>, gauss_std=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(img.shape) == <span class="hljs-number">2</span> <span class="hljs-keyword">or</span> img.shape[<span class="hljs-number">2</span>] == <span class="hljs-number">1</span><br>        <span class="hljs-keyword">assert</span> threshold2 &gt;= threshold1, <span class="hljs-string">&quot;threshold2 should greater than threshold1&quot;</span><br>        <br>        self.d = [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]<br>        self.x, self.y = img.shape<br><br>        <span class="hljs-keyword">if</span> threshold2 &lt;= <span class="hljs-number">1</span>:<br>            self.<span class="hljs-built_in">max</span> = <span class="hljs-number">1</span><br>            self.<span class="hljs-built_in">min</span> = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>:<br>            self.<span class="hljs-built_in">max</span> = <span class="hljs-number">255</span><br>            self.<span class="hljs-built_in">min</span> = <span class="hljs-number">0</span><br><br>        img = img.astype(<span class="hljs-built_in">float</span>)<br><br>        <span class="hljs-comment"># 高斯核和索伯核</span><br>        sob = np.array([[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                        [-<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],<br>                        [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])<br>        gauss_kernel = cv2.getGaussianKernel(gauss_size, gauss_std)<br><br>        <span class="hljs-comment"># 计算检测x轴方向上gradient的kernel</span><br>        kernel_x = convolve2d(sob.T, gauss_kernel)<br>        kernel_x = convolve2d(kernel_x, gauss_kernel.T)<br><br>        <span class="hljs-comment"># 计算检测y轴方向上gradient的kernel</span><br>        kernel_y = convolve2d(sob, gauss_kernel)<br>        kernel_y = convolve2d(kernel_y, gauss_kernel.T)<br><br>        <span class="hljs-comment"># 获取所有点的 x, y 方向的 gradient</span><br>        gradient_x = cv2.filter2D(img, -<span class="hljs-number">1</span>, kernel_x)<br>        gradient_y = cv2.filter2D(img, -<span class="hljs-number">1</span>, kernel_y)<br><br>        <span class="hljs-comment"># 计算近似 gradient, 和对应的方向</span><br>        gradient = np.sqrt(gradient_x**<span class="hljs-number">2</span> + gradient_y**<span class="hljs-number">2</span>)<br>        direction = np.arctan(gradient_y / gradient_x)<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        取整之后得到 [-2 ,2] 之间的一个整数数 代表 pi/4, 如果是2则代表左侧较亮, -2代表右侧较亮</span><br><span class="hljs-string">        所以要进行 +2 的偏移</span><br><span class="hljs-string">           左侧较亮</span><br><span class="hljs-string">        |     2   1 |                      | 3   2   1 |</span><br><span class="hljs-string">        |     p   0 |    --&gt;     左侧较亮   | 4   p   0 |    右侧较亮</span><br><span class="hljs-string">        |    -2  -1 |                      |           |</span><br><span class="hljs-string">           右侧较亮</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        direction = (direction // (np.pi/<span class="hljs-number">4</span>)).astype(<span class="hljs-built_in">int</span>)<br>        direction += <span class="hljs-number">2</span><br><br>        <span class="hljs-comment"># NMS</span><br>        res = self.nms(gradient, direction)<br><br>        <span class="hljs-comment"># 双阈值</span><br>        res[threshold2 &lt;= res] = self.<span class="hljs-built_in">max</span><br>        res[threshold1 &gt; res] = self.<span class="hljs-built_in">min</span><br><br>        <span class="hljs-comment"># 连接边</span><br>        self.link_edges(res)<br><br>        <span class="hljs-comment"># 保存</span><br>        self.edge_img = res<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_result</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.edge_img<br><br>    <span class="hljs-comment"># 非极大值抑制, 返回抑制后的结果</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">nms</span>(<span class="hljs-params">self, gradient, direction</span>):<br>        res = np.zeros(gradient.shape)<br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, res.shape[<span class="hljs-number">0</span>]):<br>            <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, res.shape[<span class="hljs-number">1</span>]):<br>                i, j = self.d[direction[x, y]]<br>                <span class="hljs-comment"># 如果 x, y 小于梯度方向的两个像素的gradient, 则抑制</span><br>                <span class="hljs-keyword">if</span> (self.in_area(x+i, y+j) <span class="hljs-keyword">and</span> self.is_less_than(gradient[x, y], gradient[x+i, y+j])) <span class="hljs-keyword">or</span>\<br>                        (self.in_area(x-i, y-j) <span class="hljs-keyword">and</span> self.is_less_than(gradient[x, y], gradient[x-i, y-j])):<br>                    res[x, y] = self.<span class="hljs-built_in">min</span><br>                <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># 保留为原有的梯度值</span><br>                    res[x, y] = gradient[x, y]<br>        <span class="hljs-keyword">return</span> res<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">link_edges</span>(<span class="hljs-params">self, img</span>):<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, img.shape[<span class="hljs-number">0</span>]):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, img.shape[<span class="hljs-number">1</span>]):<br>                <span class="hljs-keyword">if</span> img[i, j] != self.<span class="hljs-built_in">min</span> <span class="hljs-keyword">and</span> img[i, j] != self.<span class="hljs-built_in">max</span>:<br>                    img[i, j] = self.<span class="hljs-built_in">max</span> <span class="hljs-keyword">if</span> self.next_to_edge(img, i, j) <span class="hljs-keyword">else</span> self.<span class="hljs-built_in">min</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">in_area</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span> &lt;= x &lt; self.x <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= y &lt; self.y<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">next_to_edge</span>(<span class="hljs-params">self, img: np.ndarray, x, y</span>):<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> self.d:<br>            newx, newy = x + i, y + j<br>            <span class="hljs-keyword">if</span> self.in_area(newx, newy) <span class="hljs-keyword">and</span> self.is_equals(img[newx, newy], self.<span class="hljs-built_in">max</span>):<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br><br>    <span class="hljs-comment"># return a == b</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">is_equals</span>(<span class="hljs-params">self, a, b</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">abs</span>(a - b) &lt; <span class="hljs-number">1e-7</span><br><br>    <span class="hljs-comment"># return a &lt; b</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">is_less_than</span>(<span class="hljs-params">self, a, b</span>):<br>        <span class="hljs-keyword">return</span> (a - b) &lt; -<span class="hljs-number">1e-7</span><br>    <br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    img = cv2.imread(<span class="hljs-string">&quot;../resources/cards.jpg&quot;</span>)<br>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br>    canny = CannyEdgeDetection(img, <span class="hljs-number">0.15</span>*<span class="hljs-number">255</span>, <span class="hljs-number">0.3</span>*<span class="hljs-number">255</span>)<br>    edge_img = canny.get_result()<br>    cv2.imshow(<span class="hljs-string">&quot;test&quot;</span>, edge_img)<br>    imwrite(<span class="hljs-string">&quot;canny_test.png&quot;</span>, [img, edge_img])<br>    cv2.waitKey(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/com_graphic/canny_test.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h2 id="拓展-实现傅里叶变换"><a href="#拓展-实现傅里叶变换" class="headerlink" title="拓展: 实现傅里叶变换"></a>拓展: 实现傅里叶变换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 离散傅里叶变换</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fft</span>(<span class="hljs-params">f</span>):<br>    N = f.shape[<span class="hljs-number">0</span>]<br>    t = np.arange(<span class="hljs-number">0</span>, N)<br>    res = np.array([f.dot(np.exp((-<span class="hljs-number">1j</span> * <span class="hljs-number">2</span> * np.pi * n * t)/N)) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N)])<br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-comment"># 离散傅里叶逆变换</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ifft</span>(<span class="hljs-params">F</span>):<br>    N = F.shape[<span class="hljs-number">0</span>]<br>    n = np.arange(<span class="hljs-number">0</span>, N)<br>    res = np.array([F.dot(np.exp(<span class="hljs-number">1j</span> * <span class="hljs-number">2</span> * np.pi * n * t/N))/N <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N)])<br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-comment"># 二维离散傅里叶变换</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fft2</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> fft(fft(x).T).T<br><br><br><span class="hljs-comment"># 对比</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    x = np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x = &quot;</span>, x)<br><br>    f = np.sin(x) + np.sin(<span class="hljs-number">2</span> * x) + np.sin(<span class="hljs-number">3</span> * x)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;sin(x)+sin(2x)+sin(3x) = &quot;</span>, f)<br><br>    <span class="hljs-comment"># 自己写的傅里叶变换</span><br>    fft_res = fft(f)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;fft_res = &quot;</span>, fft_res)<br><br>    ifft_res = ifft(fft_res)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ifft_res = &quot;</span>, np.real(ifft_res))<br><br>    <span class="hljs-comment"># np 中的傅里叶变换</span><br>    np_fft_res = np.fft.fft(f)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;np_fft_res = &quot;</span>, np_fft_res)<br><br>    np_ifft_res = np.fft.ifft(np_fft_res)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;np_ifft_res = &quot;</span>, np_ifft_res)<br></code></pre></td></tr></table></figure>

<p>实际效果是一致的(浮点误差不算)</p>
<p>通常使用傅里叶变换时还配套使用 fftshift 把零频点移到频谱的中间</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">fftshift1</span>(<span class="hljs-params">F:np.ndarray</span>):<br>    index = (F.shape[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> np.concatenate([F[index:], F[:index]], axis=<span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ifftshift</span>(<span class="hljs-params">F:np.ndarray</span>):<br>    index = F.shape[<span class="hljs-number">0</span>] // <span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> np.concatenate([F[index:], F[:index]], axis=<span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fftshift</span>(<span class="hljs-params">F:np.ndarray</span>):<br>    cf = F.copy()	<span class="hljs-comment"># 因为要递归修改</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(cf.shape) != <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(F.shape[<span class="hljs-number">0</span>]):<br>            cf[i] = fftshift(F[i])<br>    index = (cf.shape[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> np.concatenate([cf[index:], cf[:index]], axis=<span class="hljs-number">0</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ifftshift</span>(<span class="hljs-params">F:np.ndarray</span>):<br>    cf = F.copy()<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(cf.shape) != <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(F.shape[<span class="hljs-number">0</span>]):<br>            cf[i] = fftshift(F[i])<br>    index = cf.shape[<span class="hljs-number">0</span>] // <span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> np.concatenate([cf[index:], cf[:index]], axis=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<br>

<p>差不多是这个感觉, 当然频谱不长这样. 这里只是为了形象.</p>
<p><img src="/Blog/Blog/intro/com_graphic/ifftshift.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>因为可能会是多维的, 所以递归实现. </p>
<p>为了方便理解, 我把一维的实现也放上去了, 理解一维是怎么运作的之后, 多维自然就理解了</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/Blog/categories/%E5%9B%BE%E5%83%8F%E5%AD%A6/">图像学</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/Blog/tags/python3/">python3</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/opencv/">opencv</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/%E5%9B%BE%E5%83%8F%E5%AD%A6/">图像学</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/WebGl/">WebGl</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/javascript/">javascript</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/Blog/2020/09/20/opencv-trap/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">opencv</span>
                        <span class="visible-mobile">前の記事</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/Blog/2020/09/08/Computing-System/">
                        <span class="hidden-mobile">计算机底层</span>
                        <span class="visible-mobile">次の記事</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;ディレクトリ</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">検索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">キーワード</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/Blog/js/events.js" ></script>
<script  src="/Blog/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/Blog/js/local-search.js" ></script>



  
    <script  src="/Blog/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/Blog/js/boot.js" ></script>


</body>
</html>
